<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>分类: LLMs - Hello, NilEra :-)</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hello, NilEra :-)"><meta name="msapplication-TileImage" content="/img/StarLogo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hello, NilEra :-)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="努力做自己喜欢的事"><meta property="og:type" content="blog"><meta property="og:title" content="Hello, NilEra :-)"><meta property="og:url" content="https://hello-nilera.com/"><meta property="og:site_name" content="Hello, NilEra :-)"><meta property="og:description" content="努力做自己喜欢的事"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hello-nilera.com/img/og_image.png"><meta property="article:author" content="NilEra"><meta property="article:tag" content="Hello NilEra"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://hello-nilera.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hello-nilera.com"},"headline":"Hello, NilEra :-)","image":["https://hello-nilera.com/img/og_image.png"],"author":{"@type":"Person","name":"NilEra"},"publisher":{"@type":"Organization","name":"Hello, NilEra :-)","logo":{"@type":"ImageObject","url":"https://hello-nilera.com/img/StarLogo.svg"}},"description":"努力做自己喜欢的事"}</script><link rel="icon" href="/img/StarLogo.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?249654dcf9a3bf70708fdfc6e2b1ec2b";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><meta name="msvalidate.01" content="F6BD78C6BD0096D2218CF88334111125"><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.loli.net/ajax/libs/pace/1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/StarLogo.svg" alt="Hello, NilEra :-)" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">目录</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a><a class="navbar-item" href="/me">我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/NilEra-K"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="is-active"><a href="#" aria-current="page">LLMs</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-10-24T02:01:05.000Z" title="2024/10/24 10:01:05">2024-10-24</time>发表</span><span class="level-item"><time dateTime="2024-10-24T02:02:25.418Z" title="2024/10/24 10:02:25">2024-10-24</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">几秒读完 (大约0个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/24/LLM-FINETUNE/">LLM-FINETUNE</a></p><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-09-12T02:03:09.000Z" title="2024/9/12 10:03:09">2024-09-12</time>发表</span><span class="level-item"><time dateTime="2024-09-12T10:56:35.065Z" title="2024/9/12 18:56:35">2024-09-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">7 分钟读完 (大约1053个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/09/12/RedHat-Build-LLM/">RedHat-Build-LLM</a></p><div class="content"><h2 id="如何在-Linux-服务器上搭建本地LLMs-RedHat-篇-🤔"><a href="#如何在-Linux-服务器上搭建本地LLMs-RedHat-篇-🤔" class="headerlink" title="如何在 Linux 服务器上搭建本地LLMs (RedHat 篇)🤔"></a>如何在 Linux 服务器上搭建本地LLMs (<code>RedHat</code> 篇)🤔</h2><p>最近因为一些原因，需要在一台 <code>RedHat</code> 红帽机器上配置一个 <code>qwen2-7B</code> 模型来进行离线大模型的使用。过程非常曲折，特此记录本次过程。</p>
<p>拿到服务器账号之后，我先检查了一些环境配置，个人用户缺失很多环境，好消息是 <code>root</code> 用户可以使用 <code>nvcc</code> 命令。那么我们就可以不用费劲去安装 <code>cuda</code> 了。</p>
<p>那么我首先使用个人用户安装了 <code>Anaconda</code>。安装过程如下：</p>
<p>安装完成后，配置环境变量，创建虚拟环境（这里我使用的 Python 版本是我参考的组卡平台的版本，这个版本经过测试不存在什么问题）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n qwen2 python=3.10.12</span><br></pre></td></tr></table></figure>

<p>启动虚拟环境时可能会遇到问题：</p>
<p>① 出现错误：<code>CondaError: Run &#39;conda init&#39; before &#39;conda activate&#39;</code></p>
<ul>
<li>出现这个问题时，我们需要按照他的提示信息，执行 <code>conda init</code> 即可。</li>
<li>如果还是不行，我们可以尝试执行 <code>conda init --system --all</code>。</li>
<li>如果还是不行，可以尝试修改 <code>shell</code> 配置文件（查看这篇博客（我并没有到这一步，是否有用需要自己斟酌）：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46429533/article/details/140855331">CSDN：关于conda环境启动不了 CondaError: Run ‘conda init’ before ‘conda activate’</a>）。</li>
</ul>
<p>② 换源问题</p>
<p>然后我继续向下推进：</p>
<p>我们切换到 <code>qwen2</code> 环境，正常安装相应的 <code>modelscope</code> 软件包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope</span><br></pre></td></tr></table></figure>

<p>目前为止一切顺利。我们需要使用 <code>git</code> 命令将 <code>llama.cpp</code> 拉到本地，因为这个服务器无法访问 <code>GitHub</code>，这里我们可以使用 <code>Gitee</code> 来进行替代。使用 <code>Gitee</code> 可以将 <code>GitHub</code> 上的仓库拉取下来： </p>
<img src="./RedHat_Build_LLM/GiteePullGithub.png" alt="GiteePullGithub" style="zoom: 50%;" />

<img src="./RedHat_Build_LLM/GiteePullGithub-2.png" alt="GiteePullGithub" style="zoom: 50%;" />

<img src="./RedHat_Build_LLM/GiteePullGithub-3.png" alt="GiteePullGithub" style="zoom: 50%;" />

<p>然后对于 <code>Gitee</code> 中的文件，我们使用 <code>wget</code> 命令下载即可。</p>
<p><strong>更新 GCC</strong></p>
<p>⭐⭐<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41054313/article/details/119453611">Linux升级gcc到最新版本gcc-11.2.0_更新gcc-CSDN博客</a></p>
<p>⭐⭐⭐<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/943011">Linux gcc升级全过程，过程超详细-阿里云开发者社区 (aliyun.com)</a></p>
<p>安装 <code>Ccache</code></p>
<ol>
<li><p>从官方网站下载 <code>Ccache</code>，<a target="_blank" rel="noopener" href="https://ccache.dev/">Ccache — Compiler cache</a>，我们也可以从 <code>GitHub</code> 的 <a target="_blank" rel="noopener" href="https://github.com/ccache/ccache/releases/"><code>Releases</code></a> 页面上找到较旧的版本。</p>
</li>
<li><p>使用 <code>tar -zxvf ccache-&lt;VERSION&gt;-linux-x86_64.tar.xz</code>  命令，解压 <code>Ccache</code> 到文件夹</p>
</li>
<li><p>较早的版本需要执行三条语句进行安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>较新的版本提供了一键安装方式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般为 /usr/local/ccache-your-version_num</span></span><br><span class="line">make install prefix=/usr/local/ccache-4.9</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置环境变量</p>
</li>
<li><p>检查 <code>Ccache</code> 的版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ccache -V</span><br></pre></td></tr></table></figure></li>
</ol>
<p>安装 <code>MPFR</code> 和 <code>GMP</code></p>
<p>安装 <code>MPC</code></p>
<ol>
<li>官网：<a target="_blank" rel="noopener" href="https://www.multiprecision.org/mpc/download.html">Download — multiprecision.org</a></li>
</ol>
<p>问题：</p>
<p><code>error while loading shared libraries: libmpfr.so.6: cannot open shared object file: No such file or directory</code> 安装 <code>libmpfr6-4.1.0-alt1.x86_64.rpm</code> 并且将缺少的库放到 <code>/usr/lib64</code> 中。</p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://altlinux.pkgs.org/p10/classic-x86_64/libmpfr6-4.1.0-alt1.x86_64.rpm.html">libmpfr6-4.1.0-alt1.x86_64.rpm ALT Linux P10 Download (pkgs.org)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost gcc_bag]<span class="comment"># wget https://altlinux.pkgs.org/sisyphus/classic-x86_64/libmpfr6-4.1.0-alt2.x86_64.rpm.html</span></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># ls</span></span><br><span class="line">libmpfr6-4.1.0-alt2.x86_64.rpm</span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># rpm2cpio libmpfr6-4.1.0-alt2.x86_64.rpm | cpio -div</span></span><br><span class="line"></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># rpm2cpio libmpfr6-4.1.0-alt2.x86_64.rpm | cpio -div</span></span><br><span class="line">./usr/lib64/libmpfr.so.6</span><br><span class="line">./usr/lib64/libmpfr.so.6.1.0</span><br><span class="line">./usr/share/doc/mpfr-4.1.0</span><br><span class="line">./usr/share/doc/mpfr-4.1.0/AUTHORS</span><br><span class="line">./usr/share/doc/mpfr-4.1.0/BUGS</span><br><span class="line">./usr/share/doc/mpfr-4.1.0/NEWS</span><br><span class="line">5494 blocks</span><br><span class="line"></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># ls</span></span><br><span class="line">libmpfr6-4.1.0-alt2.x86_64.rpm usr</span><br><span class="line"></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># mv ./usr/lib64/libmpfr.so.6 /usr/lib64/</span></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># mv ./usr/lib64/libmpfr.so.6.1.0 /usr/lib64/</span></span><br></pre></td></tr></table></figure>



<p>更新 <code>binutils</code>，包括 <code>ld</code>、<code>as</code></p>
<p><code>binutils</code> 镜像：<a target="_blank" rel="noopener" href="https://ftp.gnu.org/gnu/binutils/">Index of &#x2F;gnu&#x2F;binutils</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost Share]<span class="comment"># wget http://ftp.gnu.org/gnu/binutils/binutils-2.25.1.tar.bz2  </span></span><br><span class="line">//wget如果下载不下来就 直接浏览器访问网址下载后再解压</span><br><span class="line">[root@localhost Share]<span class="comment"># tar -xjf binutils-2.25.1.tar.bz2   </span></span><br><span class="line">[root@localhost Share]<span class="comment"># cd binutils-2.25.1  </span></span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># ./configure  --prefix=/usr  </span></span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># make  </span></span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># make install  </span></span><br><span class="line">//安装完毕验证结果</span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># as --version </span></span><br><span class="line">GNU assembler (GNU Binutils) 2.25.1  </span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># objdump -v  </span></span><br><span class="line">GNU objdump (GNU Binutils) 2.25.1  </span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># ld -v  </span></span><br><span class="line">GNU ld (GNU Binutils) 2.25.1 </span><br></pre></td></tr></table></figure>



<p>安装 <code>nvcc</code></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bigbaojian/article/details/129642388">https://blog.csdn.net/bigbaojian/article/details/129642388</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79059379">Linux 下的 CUDA 安装和使用指南 - 知乎 (zhihu.com)</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-13T12:19:01.000Z" title="2024/8/13 20:19:01">2024-08-13</time>发表</span><span class="level-item"><time dateTime="2024-10-18T15:38:30.830Z" title="2024/10/18 23:38:30">2024-10-18</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">6 分钟读完 (大约903个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/13/Deploy-LLMs-ON-Linux/">Deploy_LLMs_ON_Linux</a></p><div class="content"><h2 id="如何在-Linux-服务器上搭建本地LLMs-🤔"><a href="#如何在-Linux-服务器上搭建本地LLMs-🤔" class="headerlink" title="如何在 Linux 服务器上搭建本地LLMs 🤔"></a>如何在 Linux 服务器上搭建本地LLMs 🤔</h2><p>如何在 <code>Linux</code> 服务器上部署大语言模型，以 <code>qwen1_5-32b-chat-q8_k_0</code> 为例。服务器使用显卡 <code>A4000</code>，预算：$5950$ 元。</p>
<h3 id="搭建-qwen1-5-32b-chat-q8-k-0"><a href="#搭建-qwen1-5-32b-chat-q8-k-0" class="headerlink" title="搭建 qwen1_5-32b-chat-q8_k_0"></a>搭建 <code>qwen1_5-32b-chat-q8_k_0</code></h3><ol>
<li><p>下载 🤗<code>Hugging Face</code> 库，这个库主要是用于下载模型使用。当然为了保证速度，我们可以使用 <code>wget</code> 命令替代他。如果你决定使用 <code>wget</code> 命令，你可以选择跳过这一步，具体的使用方式在第五步呈现。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ pip install huggingface_hub</span><br></pre></td></tr></table></figure>

<p>或者是直接下载 <code>modelscope</code> 库，使用 <code>modelscope</code> 下载模型（⭐推荐）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ pip install modelscope</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个 <code>LocalGit</code> 文件夹，并进入该文件夹</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ <span class="built_in">mkdir</span> LocalGit</span><br><span class="line">(base) ➜  ~ <span class="built_in">cd</span> LocalGit</span><br><span class="line">(base) ➜  LocalGit </span><br></pre></td></tr></table></figure>
</li>
<li><p>克隆 <code>llama.cpp</code> 的仓库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  LocalGit git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp</span><br><span class="line">(base) ➜  LocalGit <span class="built_in">cd</span> llama.cpp</span><br><span class="line">(base) ➜  llama.cpp git:(master)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在有 <code>GPU</code> 的环境下编译 <code>llama.cpp</code></p>
<p><strong>前置条件：</strong>安装 <code>nvcc</code> + <code>cmake</code></p>
<p>执行代码进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc</span><br></pre></td></tr></table></figure>

<p>如果出现错误：<code>(base) ➜  llama.cpp git:(master) make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc Makefile:76: *** LLAMA_CUBLAS is removed. Use GGML_CUDA instead..  Stop.</code></p>
<p>修改代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) make GGML_CUDA=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc</span><br></pre></td></tr></table></figure>

<p>为了加快编译速度，我们可以尝试以下命令添加参数 <code>j</code>，<code>j</code> 后面的数字表示同时编译的线程数（可根据 <code>CPU</code> 核数决定），实测能缩短约 $1&#x2F;3$ 的时间：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) make -j6 GGML_CUDA=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载相应的模型</p>
<p>① 使用 <code>Hugging Face</code> 下载相应模型，实测服务器网速在 <code>3M~6M</code> 左右，具体方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ huggingface-cli download Qwen/Qwen1.5-32B-Chat-GGUF qwen1_5-32b-chat-q8_0.gguf --local-dir . --local-dir-use-symlinks False</span><br></pre></td></tr></table></figure>

<p>② 使用 <code>wget</code> 下载 <code>modelscope</code> 的模型文件，实测网速在 <code>10M~22M</code> 左右，这需要你先获取到模型的下载链接，具体方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ wget https://www.modelscope.cn/models/qwen/Qwen1.5-32B-Chat-GGUF/resolve/master/qwen1_5-32b-chat-q8_0.gguf</span><br></pre></td></tr></table></figure>

<p>③ 直接使用 <code>modelscope</code> 库下载模型，实测网速在 <code>18M~65M</code> 左右，具体方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ <span class="built_in">cd</span> LocalGit </span><br><span class="line">(base) ➜  LocalGit <span class="built_in">mkdir</span> models</span><br><span class="line">(base) ➜  LocalGit <span class="built_in">cd</span> models</span><br><span class="line">(base) ➜  models modelscope download --model=qwen/Qwen2-7B-Instruct-GGUF --local_dir . qwen2-7b-instruct-q8_0.gguf</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 <code>llama.cpp</code> 的相关命令进行操作</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) ./main -m ../models/qwen1_5-32b-chat-q8_0.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) ./llama-server -m ../models/qwen1_5-32b-chat-q8_0.gguf -ngl 80 -fa</span><br></pre></td></tr></table></figure>

<p>如果是 <code>Qwen2-7B-Instruct-GGUF</code>，可以参考官方文档：<a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/qwen/qwen2-7b-instruct-gguf">Qwen2-7B-Instruct-GGUF · 模型库 — Qwen2-7B-Instruct-GGUF · 模型库 (modelscope.cn)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) ./llama-server -m ../models/qwen2-7b-instruct-q8_0.gguf -ngl 29 -fa</span><br></pre></td></tr></table></figure>
</li>
<li><p>兼容 <code>OpenAI API</code>，使用 <code>Python</code> 代码测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import openai</span><br><span class="line"></span><br><span class="line">client = openai.OpenAI(</span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8080/v1&quot;</span>, <span class="comment"># &quot;http://&lt;Your api-server IP&gt;:port&quot;</span></span><br><span class="line">    api_key = <span class="string">&quot;sk-no-key-required&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">    model=<span class="string">&quot;qwen&quot;</span>,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;tell me something about michael jordan&quot;</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(completion.choices[0].message.content)</span><br></pre></td></tr></table></figure>
</li>
<li><p>命令启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./llama-cli -m qwen2-7b-instruct-q5_k_m.gguf \</span><br><span class="line">  -n 512 -co -i -<span class="keyword">if</span> -f prompts/chat-with-qwen.txt \</span><br><span class="line">  --in-prefix <span class="string">&quot;&lt;|im_start|&gt;user\n&quot;</span> \</span><br><span class="line">  --in-suffix <span class="string">&quot;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span> \</span><br><span class="line">  -ngl 24 -fa</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="拓展补充"><a href="#拓展补充" class="headerlink" title="拓展补充"></a>拓展补充</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shebao3333/article/details/139428868">Llama.cpp大模型量化简明手册_llamacpp量化-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/oandy0/article/details/135712598">【Llama2 windows部署详细教程】第二节：llama.cpp成功在windows上编译的秘诀_llama cpp 编译-CSDN博客</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-12T15:10:23.000Z" title="2024/8/12 23:10:23">2024-08-12</time>发表</span><span class="level-item"><time dateTime="2024-08-13T09:55:49.971Z" title="2024/8/13 17:55:49">2024-08-13</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">2 分钟读完 (大约226个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/12/Windows-Build-llama-cpp/">Windows Build llama.cpp</a></p><div class="content"><h2 id="Windows-平台下构建-llama-cpp"><a href="#Windows-平台下构建-llama-cpp" class="headerlink" title="Windows 平台下构建 llama.cpp"></a>Windows 平台下构建 <code>llama.cpp</code></h2><p>在使用 <code>LM-Studio</code> 时，对于一些参数量不是很大的模型来说，大多数不需要进行模型的合并，如 <code>qwen2-7b</code> 等。这些模型往往只需要下载后加载到 <code>LM-Studio</code> 中即可。</p>
<p>但是对于参数量很大的模型，如 <code>qwen2-72b-instruct</code> 等，因为模型文件较大不利于传输，因此模型开发者可能会使用 <code>llama.cpp</code> 对 <code>GGUF</code> 模型进行拆分，所以这个时候我们在下载模型时就需要进行模型的合并。</p>
<p><code>qwen2-72b-instruct</code> 在 <code>q8</code> 量化给出了两个模型文件，分别是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qwen2-72b-instruct-q8_k_m-00001-of-00002.gguf</span><br><span class="line">qwen2-72b-instruct-q8_k_m-00002-of-00002.gguf</span><br></pre></td></tr></table></figure>

<p>为了使用这些分割后的 <code>GGUF</code> 文件，我们可以使用 <code>llama-gguf-split</code> 合并他们</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llama-gguf-spilt --merge input.gguf output.gguf</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-05T02:37:16.000Z" title="2024/8/5 10:37:16">2024-08-05</time>发表</span><span class="level-item"><time dateTime="2024-10-23T06:28:46.689Z" title="2024/10/23 14:28:46">2024-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">9 分钟读完 (大约1370个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/05/Deploy-LLMs-ON-PC/">Deploy_LLMs_ON_PC</a></p><div class="content"><h2 id="如何搭建运行在本地的-LLMs-🤔"><a href="#如何搭建运行在本地的-LLMs-🤔" class="headerlink" title="如何搭建运行在本地的 LLMs 🤔"></a>如何搭建运行在本地的 LLMs 🤔</h2><p>[TOC]</p>
<h3 id="🤗-1-基于-LM-Studio"><a href="#🤗-1-基于-LM-Studio" class="headerlink" title="🤗 1. 基于 LM-Studio"></a>🤗 1. 基于 <code>LM-Studio</code></h3><ol>
<li><p>访问 <code>LM-Studio</code>，网址：<a target="_blank" rel="noopener" href="https://lmstudio.ai/">LM Studio - Discover, download, and run local LLMs</a></p>
<p>下载对应系统的安装包，然后双击运行即可。</p>
</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Website.png">

<ol start="2">
<li>访问 <code>ModelScope</code><a target="_blank" rel="noopener" href="https://community.modelscope.cn/">魔搭社区</a> 或者 🤗<code>Hugging Face</code><a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a>，这里以 <code>ModelScope</code> 为例，进入模型库，下载相应模型。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/ModelScope.png">

<center>魔搭社区官网</center>

<img src="./Deploy_LLMs_ON_PC/ModelScopeDownloadModels.png">

<center>找到需要的模型并下载</center>

<ol start="3">
<li>下载好响应的模型后，将模型组织好，放到相应的文件夹中，这里按照 <code>models/Publisher/Repository/*.gguf</code> 的路径组织模型路径，然后选择 <code>Change</code> 更改模型的位置。如果不按照该路径组织，则会出现 <code>You have 1 uncategorized model files.</code> 错误，如下图所示：</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/Error_With_Wrong_Project_Structure.png">

<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_ModelFile.png">

<ol start="4">
<li>但是那种方式是不太推荐的，我们组织 <code>USER/MODEL_NAME/*.gguf</code> 的结构，这种结构会比较明了：</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_ModelFile0002.png">

<ol start="5">
<li>完成模型文件的下载和组织后，我们可以进入聊天页面，选择模型进行加载。这里为了节约空间，我删除了 <code>nilera/Qwen1.5-7B-Chat-Q4-GGUF</code> 目录下的文件。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/SelectAModel2Load.png">

<ol start="6">
<li>选择模型加载，等待加载完成即可像平时使用其他大模型的时候一样使用这些模型。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_Test_qwen1_5.png">

<ol start="7">
<li>但是如果我们想在代码中使用我们的大模型应该怎么做呢？我们可以选择 <code>LM-Studio</code> 的 <code>Local Server</code> 菜单项，选择 <code>Start Server</code> 即可部署到一个本地指定的端口（默认是 <code>1234</code>）。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_Start_Local_Server0001.png">

<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_Start_Local_Server_0002.png">

<ol start="8">
<li>右侧有许多样例，我们可以选择一段样例，如：<code>chat(python)</code>，这里对这段代码进行简单的解释。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example: reuse your existing OpenAI setup</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># Point to the local server</span></span><br><span class="line">client = OpenAI(base_url=<span class="string">&quot;http://localhost:1234/v1&quot;</span>, api_key=<span class="string">&quot;lm-studio&quot;</span>)</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">&quot;Publisher/Repository&quot;</span>,									<span class="comment"># 可以理解为模型路径, 这里以启动在这个端口的模型为准</span></span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Always answer in Chinese.&quot;</span>&#125;,	<span class="comment"># 系统设置: 每次都用中文回答</span></span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Introduce yourself.&quot;</span>&#125;			<span class="comment"># 对话设置: 这里希望 AI 介绍一下他自己</span></span><br><span class="line">  ],</span><br><span class="line">  temperature=<span class="number">0.7</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(completion.choices[<span class="number">0</span>].message)							<span class="comment"># 获取模型的回复</span></span><br></pre></td></tr></table></figure>

<ol start="9">
<li>然后我们就可以愉快的使用 <code>Python</code> 调用我们的本地大模型了。</li>
</ol>
<h3 id="⛵-2-使用-PowerInfer-框架"><a href="#⛵-2-使用-PowerInfer-框架" class="headerlink" title="⛵ 2. 使用 PowerInfer 框架"></a>⛵ 2. 使用 <code>PowerInfer</code> 框架</h3><p><code>PowerInfer</code> 框架 <em>GitHub</em> 链接：<a target="_blank" rel="noopener" href="https://github.com/SJTU-IPADS/PowerInfer">SJTU-IPADS&#x2F;PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs (github.com)</a></p>
<p>$2024$ 年发布论文 <code>PowerInfer-2</code>：<font color=Blue>[</font><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.06282">2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone (arxiv.org)</a></p>
<p><code>Anaconda</code> 命令使用：<a target="_blank" rel="noopener" href="https://blog.csdn.net/miracleoa/article/details/106115730">【anaconda】conda创建、查看、删除虚拟环境（anaconda命令集）_conda 创建环境-CSDN博客</a></p>
<p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42232045/article/details/135113112">大模型笔记之-3090显卡推理70B参数模型|基于PowerInfer 一个 CPU&#x2F;GPU LLM 推理引擎-CSDN博客</a></p>
<ol>
<li>使用 <code>Conda</code> 创建环境，这里 Python 版本需要大于 <code>3.8</code>：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n powerinfer1 python=3.8</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>激活 <code>Conda</code> 环境：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate powerinfer1</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>克隆 <code>PowerInfer</code> 框架代码：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:SJTU-IPADS/PowerInfer.git</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>安装所需依赖：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>使用 <code>CMake</code> 进行编译（<code>CMake</code> 版本需要大于：<code>3.17+</code>）</p>
<p>这里很大概率可能会出现编译器版本与 CUDA 版本不一致的情况，解决方案：<a target="_blank" rel="noopener" href="https://blog.csdn.net/lishiyu93/article/details/114599859">fatal error C1189: #error: – unsupported Microsoft Visual Studio version! - CSDN博客</a></p>
<p>这里我有三个 <code>CUDA</code> 版本，貌似修改其中任意一个就可以，这里我修改的是 <code>CUDA v11.6</code> 版本。</p>
<p>① 如果是 <code>NVIDIA GPUs</code>，需要使用如下方式进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake -S . -B build -DLLAMA_CUBLAS=ON</span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<p>② 如果是 <code>AMD GPUs</code>，需要使用下面的方式进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace &#x27;1100&#x27; to your card architecture name, you can get it by rocminfo</span></span><br><span class="line">CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++ cmake -S . -B build -</span><br><span class="line">DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1100</span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<p>③ 如果是 <code>CPU ONLY</code>，需要使用下面的方式进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake -S . -B build</span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<p>这里我有一块 <code>Nvidia 1050ti</code> 所以我使用<strong>方式 ①</strong>进行编译。</p>
</li>
<li><p>对于我们下载的模型，可以使用提供的方式进行转化，转化为 PowerInfer 可以使用的类型：</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make sure that you have done `pip install -r requirements.txt`</span></span><br><span class="line">python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR</span><br><span class="line"><span class="comment"># python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py --outfile D:/LMStudio/models/Publisher/Repository/qwen1_5-7b-chat-q4_0.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>或者将要 原始模型转化为 GGUF 模型</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert-dense.py --outfile /PATH/TO/DENSE/GGUF/REPO/MODELNAME.gguf /PATH/TO/ORIGINAL/MODEL</span><br></pre></td></tr></table></figure>

<ol start="8">
<li>运行模型</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/Release/main.exe -m C:/Users/NilEra/Downloads/llama-7b-relu.powerinfer.gguf -n 128 -t 2 -p <span class="string">&quot;Once upon a time&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中/home/user/data/ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.q4.powerinfer.gguf为GPTQ量化过的模型文件</span></span><br></pre></td></tr></table></figure>

<ol start="9">
<li>一些问题：</li>
</ol>
<p><img src="/./Deploy_LLMs_ON_PC/Issues22.png" alt="Issus22"></p>
<h3 id="🔧-3-在-Windows-上搭建-llama-cpp"><a href="#🔧-3-在-Windows-上搭建-llama-cpp" class="headerlink" title="🔧 3. 在 Windows 上搭建 llama.cpp"></a>🔧 3. 在 Windows 上搭建 <code>llama.cpp</code></h3><ol>
<li><p>在 Windows 上搭建 <code>llama.cpp</code> 是需要安装很多工具，且安装完成后也存在无法正常成功编译的情况（存在依赖、库等各种问题），因此这里我们可以使用 <code>w64devkit</code> 工具，使用他可以方便我们进行 <code>llama.cpp</code> 的编译。首先我们先下载 <code>w64devkit</code></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_49186516/article/details/142653813">参考网址01</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/oandy0/article/details/135712598">参考网址02</a></p>
</li>
<li><p>然后再 <code>make</code> 可以了</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-01T07:29:16.000Z" title="2024/8/1 15:29:16">2024-08-01</time>发表</span><span class="level-item"><time dateTime="2024-08-13T12:18:19.674Z" title="2024/8/13 20:18:19">2024-08-13</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">2 分钟读完 (大约289个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/01/LLM-General-Education/">LLM_General_Education</a></p><div class="content"><h2 id="🤗-大语言模型通识"><a href="#🤗-大语言模型通识" class="headerlink" title="🤗 大语言模型通识"></a>🤗 大语言模型通识</h2><h3 id="大语言模型的配置需求"><a href="#大语言模型的配置需求" class="headerlink" title="大语言模型的配置需求"></a>大语言模型的配置需求</h3><p>首先要搞清楚，本地可以部署什么大模型，取决于个人电脑的硬件配置，尤其需要关注 <code>GPU</code> 的显存。一般来说，只要本地机器 <code>GPU</code> 的显存能够满足大模型的要求，那基本上都可以本地部署。</p>
<p>那么大模型类别这么多，有 $7B$、$13B$、$70B$ 等等，<code>GPU</code> 显存如何准备呢？</p>
<p>在没有考虑任何模型量化技术的前提下，有公式如下：<br>$$<br>GB &#x3D; B × 2<br>$$</p>
<p>其中为 $GB$ 模型显存占用，$B$ 为大模型参数量。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv33371756/">AI大模型本地化部署Q&#x2F;A硬件篇</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35054222/article/details/139123564">如何找到最新的大模型、如何判断本地硬件是否满足大模型需求、如何快速部署大模型</a></p>
<p><a target="_blank" rel="noopener" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-leaderboard">大模型综合评测对比 | 当前主流大模型在各评测数据集上的表现总榜单 | 数据学习 (DataLearner)</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-01T07:29:16.000Z" title="2024/8/1 15:29:16">2024-08-01</time>发表</span><span class="level-item"><time dateTime="2024-10-23T12:38:43.409Z" title="2024/10/23 20:38:43">2024-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">几秒读完 (大约9个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/01/LLM-Knowledge-Base/">LLM_General_Education</a></p><div class="content"><h2 id="📦-大模型知识库"><a href="#📦-大模型知识库" class="headerlink" title="📦 大模型知识库"></a>📦 大模型知识库</h2><p>[TOC]</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatarMyself.jpg" alt="NilEra"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">NilEra</p><p class="is-size-6 is-block">C/C++ Developer!</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Jinan Shandong</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">48</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">41</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">47</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/NilEra-K" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/NilEra-K"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/NilEra-K" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">GitHub</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="https://nano.chemtian.top/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Thymol Blue</span></span><span class="level-right"><span class="level-item tag">nano.chemtian.top</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/CppDev/"><span class="level-start"><span class="level-item">CppDev</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/CppDev/Gaming/"><span class="level-start"><span class="level-item">Gaming</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/QT6/"><span class="level-start"><span class="level-item">QT6</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"><span class="level-start"><span class="level-item">多线程编程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"><span class="level-start"><span class="level-item">网络编程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/"><span class="level-start"><span class="level-item">语法点</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/bind/"><span class="level-start"><span class="level-item">bind</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/enum-class/"><span class="level-start"><span class="level-item">enum class</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/%E8%99%9A%E5%87%BD%E6%95%B0/"><span class="level-start"><span class="level-item">虚函数</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/JavaDev/"><span class="level-start"><span class="level-item">JavaDev</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/JavaDev/Project/"><span class="level-start"><span class="level-item">Project</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/JavaDev/SpringBoot/"><span class="level-start"><span class="level-item">SpringBoot</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/%E8%BD%AF%E4%BB%B6%E6%BA%90/"><span class="level-start"><span class="level-item">软件源</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"><span class="level-start"><span class="level-item">大数据技术</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/ECharts/"><span class="level-start"><span class="level-item">ECharts</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/HBase/"><span class="level-start"><span class="level-item">HBase</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"><span class="level-start"><span class="level-item">数据可视化</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/"><span class="level-start"><span class="level-item">项目</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="level-start"><span class="level-item">开发工具</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Axure-RP-9/"><span class="level-start"><span class="level-item">Axure RP 9</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/SVN/"><span class="level-start"><span class="level-item">SVN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/VSCode/"><span class="level-start"><span class="level-item">VSCode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构/算法</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E5%93%88%E5%B8%8C-Hash/"><span class="level-start"><span class="level-item">哈希(Hash)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%88-Stack/"><span class="level-start"><span class="level-item">栈(Stack)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%91-Tree/"><span class="level-start"><span class="level-item">树(Tree)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E8%A1%A8-List/"><span class="level-start"><span class="level-item">线性表(List)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/"><span class="level-start"><span class="level-item">LLMs</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Stable-Diffusion/"><span class="level-start"><span class="level-item">Stable_Diffusion</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"><span class="level-start"><span class="level-item">网络架构</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Kolmogorov-Arnold-Networks/"><span class="level-start"><span class="level-item">Kolmogorov-Arnold Networks</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/%E9%97%B2%E8%81%8A/"><span class="level-start"><span class="level-item">闲聊</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E9%97%B2%E8%81%8A/%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">计划</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-24T02:01:05.000Z">2024-10-24</time></p><p class="title"><a href="/2024/10/24/LLM-FINETUNE/">LLM-FINETUNE</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-17T00:46:50.000Z">2024-10-17</time></p><p class="title"><a href="/2024/10/17/Run-Others-Java-Projects/">Run-Others-Java-Projects</a></p><p class="categories"><a href="/categories/JavaDev/">JavaDev</a> / <a href="/categories/JavaDev/Project/">Project</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-20T02:07:10.000Z">2024-09-20</time></p><p class="title"><a href="/2024/09/20/List/">List</a></p><p class="categories"><a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/">数据结构/算法</a> / <a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E8%A1%A8-List/">线性表(List)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-12T02:03:09.000Z">2024-09-12</time></p><p class="title"><a href="/2024/09/12/RedHat-Build-LLM/">RedHat-Build-LLM</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-11T01:06:12.000Z">2024-09-11</time></p><p class="title"><a href="/2024/09/11/SVN-Quick-IN/">SVN-Quick-IN</a></p><p class="categories"><a href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a> / <a href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/SVN/">SVN</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">十月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">八月 2024</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">七月 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">五月 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">四月 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">三月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Annotation/"><span class="tag">Annotation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Axure-RP-9/"><span class="tag">Axure RP 9</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-11-%E6%96%B0%E6%A0%87%E5%87%86/"><span class="tag">C++ 11 新标准</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CppDev/"><span class="tag">CppDev</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ECharts/"><span class="tag">ECharts</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EasyX/"><span class="tag">EasyX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Fine-Tune-%E5%BE%AE%E8%B0%83/"><span class="tag">Fine-Tune(微调)</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GameDev/"><span class="tag">GameDev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HBase/"><span class="tag">HBase</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hadoop/"><span class="tag">Hadoop</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaDev/"><span class="tag">JavaDev</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KAN/"><span class="tag">KAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kaggle/"><span class="tag">Kaggle</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLMs/"><span class="tag">LLMs</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LM-Studio/"><span class="tag">LM-Studio</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Maven/"><span class="tag">Maven</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MyBatis-Plus/"><span class="tag">MyBatis-Plus</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PowerInfer/"><span class="tag">PowerInfer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QT6/"><span class="tag">QT6</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/R/"><span class="tag">R</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RedHat/"><span class="tag">RedHat</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SVN/"><span class="tag">SVN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scala/"><span class="tag">Scala</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spark/"><span class="tag">Spark</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SpringBoot/"><span class="tag">SpringBoot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stable-Diffusion/"><span class="tag">Stable Diffusion</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tomcat/"><span class="tag">Tomcat</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VSCode/"><span class="tag">VSCode</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ZooKeeper/"><span class="tag">ZooKeeper</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"><span class="tag">多线程编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"><span class="tag">大数据技术</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">大模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="tag">开发工具</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">数据结构</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"><span class="tag">网络编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E6%B3%95%E7%82%B9/"><span class="tag">语法点</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BD%AF%E4%BB%B6%E6%BA%90/"><span class="tag">软件源</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%97%B2%E8%81%8A/"><span class="tag">闲聊</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A1%B9%E7%9B%AE/"><span class="tag">项目</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/StarLogo.svg" alt="Hello, NilEra :-)" height="28"></a><p class="is-size-7"><span>&copy; 2024 NilEra</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024 前方⚡高能</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/NilEra-K"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container" id="algolia-input"></div><div id="algolia-poweredby" style="display:flex;margin:0 .5em 0 1em;align-items:center;line-height:0"></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div><div class="searchbox-footer"></div></div></div><script src="https://cdnjs.loli.net/ajax/libs/algoliasearch/4.0.3/algoliasearch-lite.umd.js" crossorigin="anonymous" defer></script><script src="https://cdnjs.loli.net/ajax/libs/instantsearch.js/4.3.1/instantsearch.production.min.js" crossorigin="anonymous" defer></script><script src="/js/algolia.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadAlgolia({"applicationId":"2TB5ZZYPCO","apiKey":"00a43f1d62ca7b24c8b78d5f0223c065","indexName":"dev_nilera_blog"}, {"hint":"想要查找什么...","no_result":"未找到搜索结果","untitled":"(无标题)","empty_preview":"(无内容预览)"});
        });</script></body></html>