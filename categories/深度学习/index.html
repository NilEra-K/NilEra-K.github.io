<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>分类: 深度学习 - Hello, NilEra :-)</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hello, NilEra :-)"><meta name="msapplication-TileImage" content="/img/StarLogo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hello, NilEra :-)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="努力做自己喜欢的事"><meta property="og:type" content="blog"><meta property="og:title" content="Hello, NilEra :-)"><meta property="og:url" content="https://hello-nilera.com/"><meta property="og:site_name" content="Hello, NilEra :-)"><meta property="og:description" content="努力做自己喜欢的事"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hello-nilera.com/img/og_image.png"><meta property="article:author" content="NilEra"><meta property="article:tag" content="Hello NilEra"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://hello-nilera.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hello-nilera.com"},"headline":"Hello, NilEra :-)","image":["https://hello-nilera.com/img/og_image.png"],"author":{"@type":"Person","name":"NilEra"},"publisher":{"@type":"Organization","name":"Hello, NilEra :-)","logo":{"@type":"ImageObject","url":"https://hello-nilera.com/img/StarLogo.svg"}},"description":"努力做自己喜欢的事"}</script><link rel="icon" href="/img/StarLogo.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?249654dcf9a3bf70708fdfc6e2b1ec2b";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><meta name="msvalidate.01" content="F6BD78C6BD0096D2218CF88334111125"><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.loli.net/ajax/libs/pace/1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/StarLogo.svg" alt="Hello, NilEra :-)" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">目录</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a><a class="navbar-item" href="/me">我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/NilEra-K"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">深度学习</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-10-24T02:01:05.000Z" title="2024/10/24 10:01:05">2024-10-24</time>发表</span><span class="level-item"><time dateTime="2024-10-24T02:02:25.418Z" title="2024/10/24 10:02:25">2024-10-24</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">几秒读完 (大约0个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/24/LLM-FINETUNE/">LLM-FINETUNE</a></p><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-09-12T02:03:09.000Z" title="2024/9/12 10:03:09">2024-09-12</time>发表</span><span class="level-item"><time dateTime="2024-09-12T10:56:35.065Z" title="2024/9/12 18:56:35">2024-09-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">7 分钟读完 (大约1053个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/09/12/RedHat-Build-LLM/">RedHat-Build-LLM</a></p><div class="content"><h2 id="如何在-Linux-服务器上搭建本地LLMs-RedHat-篇-🤔"><a href="#如何在-Linux-服务器上搭建本地LLMs-RedHat-篇-🤔" class="headerlink" title="如何在 Linux 服务器上搭建本地LLMs (RedHat 篇)🤔"></a>如何在 Linux 服务器上搭建本地LLMs (<code>RedHat</code> 篇)🤔</h2><p>最近因为一些原因，需要在一台 <code>RedHat</code> 红帽机器上配置一个 <code>qwen2-7B</code> 模型来进行离线大模型的使用。过程非常曲折，特此记录本次过程。</p>
<p>拿到服务器账号之后，我先检查了一些环境配置，个人用户缺失很多环境，好消息是 <code>root</code> 用户可以使用 <code>nvcc</code> 命令。那么我们就可以不用费劲去安装 <code>cuda</code> 了。</p>
<p>那么我首先使用个人用户安装了 <code>Anaconda</code>。安装过程如下：</p>
<p>安装完成后，配置环境变量，创建虚拟环境（这里我使用的 Python 版本是我参考的组卡平台的版本，这个版本经过测试不存在什么问题）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n qwen2 python=3.10.12</span><br></pre></td></tr></table></figure>

<p>启动虚拟环境时可能会遇到问题：</p>
<p>① 出现错误：<code>CondaError: Run &#39;conda init&#39; before &#39;conda activate&#39;</code></p>
<ul>
<li>出现这个问题时，我们需要按照他的提示信息，执行 <code>conda init</code> 即可。</li>
<li>如果还是不行，我们可以尝试执行 <code>conda init --system --all</code>。</li>
<li>如果还是不行，可以尝试修改 <code>shell</code> 配置文件（查看这篇博客（我并没有到这一步，是否有用需要自己斟酌）：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46429533/article/details/140855331">CSDN：关于conda环境启动不了 CondaError: Run ‘conda init’ before ‘conda activate’</a>）。</li>
</ul>
<p>② 换源问题</p>
<p>然后我继续向下推进：</p>
<p>我们切换到 <code>qwen2</code> 环境，正常安装相应的 <code>modelscope</code> 软件包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope</span><br></pre></td></tr></table></figure>

<p>目前为止一切顺利。我们需要使用 <code>git</code> 命令将 <code>llama.cpp</code> 拉到本地，因为这个服务器无法访问 <code>GitHub</code>，这里我们可以使用 <code>Gitee</code> 来进行替代。使用 <code>Gitee</code> 可以将 <code>GitHub</code> 上的仓库拉取下来： </p>
<img src="./RedHat_Build_LLM/GiteePullGithub.png" alt="GiteePullGithub" style="zoom: 50%;" />

<img src="./RedHat_Build_LLM/GiteePullGithub-2.png" alt="GiteePullGithub" style="zoom: 50%;" />

<img src="./RedHat_Build_LLM/GiteePullGithub-3.png" alt="GiteePullGithub" style="zoom: 50%;" />

<p>然后对于 <code>Gitee</code> 中的文件，我们使用 <code>wget</code> 命令下载即可。</p>
<p><strong>更新 GCC</strong></p>
<p>⭐⭐<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41054313/article/details/119453611">Linux升级gcc到最新版本gcc-11.2.0_更新gcc-CSDN博客</a></p>
<p>⭐⭐⭐<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/943011">Linux gcc升级全过程，过程超详细-阿里云开发者社区 (aliyun.com)</a></p>
<p>安装 <code>Ccache</code></p>
<ol>
<li><p>从官方网站下载 <code>Ccache</code>，<a target="_blank" rel="noopener" href="https://ccache.dev/">Ccache — Compiler cache</a>，我们也可以从 <code>GitHub</code> 的 <a target="_blank" rel="noopener" href="https://github.com/ccache/ccache/releases/"><code>Releases</code></a> 页面上找到较旧的版本。</p>
</li>
<li><p>使用 <code>tar -zxvf ccache-&lt;VERSION&gt;-linux-x86_64.tar.xz</code>  命令，解压 <code>Ccache</code> 到文件夹</p>
</li>
<li><p>较早的版本需要执行三条语句进行安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>较新的版本提供了一键安装方式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般为 /usr/local/ccache-your-version_num</span></span><br><span class="line">make install prefix=/usr/local/ccache-4.9</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置环境变量</p>
</li>
<li><p>检查 <code>Ccache</code> 的版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ccache -V</span><br></pre></td></tr></table></figure></li>
</ol>
<p>安装 <code>MPFR</code> 和 <code>GMP</code></p>
<p>安装 <code>MPC</code></p>
<ol>
<li>官网：<a target="_blank" rel="noopener" href="https://www.multiprecision.org/mpc/download.html">Download — multiprecision.org</a></li>
</ol>
<p>问题：</p>
<p><code>error while loading shared libraries: libmpfr.so.6: cannot open shared object file: No such file or directory</code> 安装 <code>libmpfr6-4.1.0-alt1.x86_64.rpm</code> 并且将缺少的库放到 <code>/usr/lib64</code> 中。</p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://altlinux.pkgs.org/p10/classic-x86_64/libmpfr6-4.1.0-alt1.x86_64.rpm.html">libmpfr6-4.1.0-alt1.x86_64.rpm ALT Linux P10 Download (pkgs.org)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost gcc_bag]<span class="comment"># wget https://altlinux.pkgs.org/sisyphus/classic-x86_64/libmpfr6-4.1.0-alt2.x86_64.rpm.html</span></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># ls</span></span><br><span class="line">libmpfr6-4.1.0-alt2.x86_64.rpm</span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># rpm2cpio libmpfr6-4.1.0-alt2.x86_64.rpm | cpio -div</span></span><br><span class="line"></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># rpm2cpio libmpfr6-4.1.0-alt2.x86_64.rpm | cpio -div</span></span><br><span class="line">./usr/lib64/libmpfr.so.6</span><br><span class="line">./usr/lib64/libmpfr.so.6.1.0</span><br><span class="line">./usr/share/doc/mpfr-4.1.0</span><br><span class="line">./usr/share/doc/mpfr-4.1.0/AUTHORS</span><br><span class="line">./usr/share/doc/mpfr-4.1.0/BUGS</span><br><span class="line">./usr/share/doc/mpfr-4.1.0/NEWS</span><br><span class="line">5494 blocks</span><br><span class="line"></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># ls</span></span><br><span class="line">libmpfr6-4.1.0-alt2.x86_64.rpm usr</span><br><span class="line"></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># mv ./usr/lib64/libmpfr.so.6 /usr/lib64/</span></span><br><span class="line">[root@localhost gcc_bag]<span class="comment"># mv ./usr/lib64/libmpfr.so.6.1.0 /usr/lib64/</span></span><br></pre></td></tr></table></figure>



<p>更新 <code>binutils</code>，包括 <code>ld</code>、<code>as</code></p>
<p><code>binutils</code> 镜像：<a target="_blank" rel="noopener" href="https://ftp.gnu.org/gnu/binutils/">Index of &#x2F;gnu&#x2F;binutils</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost Share]<span class="comment"># wget http://ftp.gnu.org/gnu/binutils/binutils-2.25.1.tar.bz2  </span></span><br><span class="line">//wget如果下载不下来就 直接浏览器访问网址下载后再解压</span><br><span class="line">[root@localhost Share]<span class="comment"># tar -xjf binutils-2.25.1.tar.bz2   </span></span><br><span class="line">[root@localhost Share]<span class="comment"># cd binutils-2.25.1  </span></span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># ./configure  --prefix=/usr  </span></span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># make  </span></span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># make install  </span></span><br><span class="line">//安装完毕验证结果</span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># as --version </span></span><br><span class="line">GNU assembler (GNU Binutils) 2.25.1  </span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># objdump -v  </span></span><br><span class="line">GNU objdump (GNU Binutils) 2.25.1  </span><br><span class="line">[root@localhost binutils-2.25.1]<span class="comment"># ld -v  </span></span><br><span class="line">GNU ld (GNU Binutils) 2.25.1 </span><br></pre></td></tr></table></figure>



<p>安装 <code>nvcc</code></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bigbaojian/article/details/129642388">https://blog.csdn.net/bigbaojian/article/details/129642388</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79059379">Linux 下的 CUDA 安装和使用指南 - 知乎 (zhihu.com)</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-09-05T07:07:36.000Z" title="2024/9/5 15:07:36">2024-09-05</time>发表</span><span class="level-item"><time dateTime="2024-09-05T13:13:59.683Z" title="2024/9/5 21:13:59">2024-09-05</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Stable-Diffusion/">Stable_Diffusion</a></span><span class="level-item">几秒读完 (大约55个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/09/05/How2Use-Stable-Diffusion/">How2Use_Stable_Diffusion</a></p><div class="content"><h1 id="🤔-How-To-Use-Stable-Diffusion"><a href="#🤔-How-To-Use-Stable-Diffusion" class="headerlink" title="🤔 How To Use Stable Diffusion"></a>🤔 How To Use Stable Diffusion</h1><p>Extension：</p>
<p>ChatGPT-4V</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> u+x install_linux_mac.sh</span><br><span class="line">./install_linux_mac.sh</span><br></pre></td></tr></table></figure>

<p>出现错误：</p>
<p>ImportError: Using SOCKS proxy, but the ‘socksio’ package is not installed. Make sure to install httpx using <code>pip install httpx[socks]</code>.</p>
<p>unset all_proxy &amp;&amp; unset ALL_PROXY</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-22T09:58:38.000Z" title="2024/8/22 17:58:38">2024-08-22</time>发表</span><span class="level-item"><time dateTime="2024-08-22T12:08:59.048Z" title="2024/8/22 20:08:59">2024-08-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/">网络架构</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Kolmogorov-Arnold-Networks/">Kolmogorov-Arnold Networks</a></span><span class="level-item">2 分钟读完 (大约332个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/22/Kolmogorov-Arnold-Networks/">Kolmogorov-Arnold Networks</a></p><div class="content"><h2 id="Kolmogorov-Arnold-Networks-KAN"><a href="#Kolmogorov-Arnold-Networks-KAN" class="headerlink" title="Kolmogorov-Arnold Networks (KAN)"></a>Kolmogorov-Arnold Networks (KAN)</h2><blockquote>
<p><strong>What <code>KAN</code> I Say ? <code>MLP</code> OUT !</strong></p>
</blockquote>
<p>Kolmogorov-Arnold Networks(KAN)</p>
<p>[TOC]</p>
<p><code>KAN</code> 源码中的核心组件：</p>
<table>
<thead>
<tr>
<th>核心代码</th>
<th>主要内容</th>
</tr>
</thead>
<tbody><tr>
<td><code>KAN.py</code></td>
<td>主要的文件，包含定义 <code>KAN</code> 模型的主要类和函数，定义了 <code>KAN</code> 模型，它由多个 <code>KANLayer</code> 和 <code>Symbolic_KANLayer</code>  组成，包含前向传播、模式设置、符号激活函数的固定和建议、模型训练、剪枝以及可视化等功能。</td>
</tr>
<tr>
<td><code>KANLayer.py</code></td>
<td>定义 <code>KAN</code> 模型中使用的自定义层，包括模型的核心组件，如特殊的激活函数或其他处理层，是构成 <code>KAN</code> 的基础。</td>
</tr>
<tr>
<td><code>LBFGS.py</code></td>
<td>包含使用 <code>KAN</code> 训练的 <code>L-BFGS</code> 优化器的实现，用于在训练过程中对 <code>KAN</code> 的参数进行优化。</td>
</tr>
<tr>
<td><code>Symbolic_KANLayer.py</code></td>
<td>实现了一种特殊的 <code>KAN</code> 层，用于处理符号计算或增强模型解释性，用符号函数（如正弦）来代替传统的数值激活函数。以便深入分析和解释。</td>
</tr>
<tr>
<td><code>spline.py</code></td>
<td>包含实现样条函数的代码，提供一系列函数，用于处理 B 样条曲线。包括：计算 B 样条基函数、从系数生成样条曲线，以及从样条曲线估计系数等。</td>
</tr>
</tbody></table>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-17T01:24:42.000Z" title="2024/8/17 09:24:42">2024-08-17</time>发表</span><span class="level-item"><time dateTime="2024-09-03T14:02:31.676Z" title="2024/9/3 22:02:31">2024-09-03</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Kaggle/">Kaggle</a></span><span class="level-item">13 分钟读完 (大约1953个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/17/RSNA-2024/">RSNA-2024</a></p><div class="content"><h2 id="RSNA-2024-Lumbar-Spine-Degenerative-Classification-腰椎退行性分类"><a href="#RSNA-2024-Lumbar-Spine-Degenerative-Classification-腰椎退行性分类" class="headerlink" title="RSNA 2024 Lumbar Spine Degenerative Classification 腰椎退行性分类"></a>RSNA 2024 Lumbar Spine Degenerative Classification 腰椎退行性分类</h2><p>Classify lumbar spine degenerative conditions：对腰椎退行性疾病进行分类</p>
<p>[TOC]</p>
<h3 id="Overview-概述"><a href="#Overview-概述" class="headerlink" title="Overview 概述"></a>Overview 概述</h3><p><em>The goal of this competition is to create models that can be used to aid in the detection and classification of degenerative spine conditions using lumbar spine MR images. Competitors will develop models that simulate a radiologist’s performance in diagnosing spine conditions.</em></p>
<p>本次竞赛的目标是创建可用于帮助使用腰椎 MR 图像检测和分类脊柱退行性疾病的模型。参赛者将开发模型来模拟放射科医生诊断脊柱疾病的表现。</p>
<h4 id="Description-描述"><a href="#Description-描述" class="headerlink" title="Description 描述"></a>Description 描述</h4><p><em>Low back pain is the leading cause of disability worldwide, according to the World Health Organization, affecting 619 million people in 2020. Most people experience low back pain at some point in their lives, with the frequency increasing with age. Pain and restricted mobility are often symptoms of spondylosis, a set of degenerative spine conditions including degeneration of intervertebral discs and subsequent narrowing of the spinal canal (spinal stenosis), subarticular recesses, or neural foramen with associated compression or irritations of the nerves in the low back.</em></p>
<p>根据世界卫生组织的数据，腰痛是全球范围内导致残疾的主要原因，到 2020 年，腰痛将影响 6.19 亿人。大多数人在一生中的某个阶段都会经历腰痛，且频率随着年龄的增长而增加。疼痛和活动受限通常是脊椎病的症状，这是一组退行性脊柱疾病，包括椎间盘退变和随后的椎管变窄（椎管狭窄）、关节下隐窝或神经孔，并伴有下肢神经受压或刺激。</p>
<p><em>Magnetic resonance imaging (MRI) provides a detailed view of the lumbar spine vertebra, discs and nerves, enabling radiologists to assess the presence and severity of these conditions. Proper diagnosis and grading of these conditions help guide treatment and potential surgery to help alleviate back pain and improve overall health and quality of life for patients.</em></p>
<p>磁共振成像 (MRI) 提供腰椎、椎间盘和神经的详细视图，使放射科医生能够评估这些病症的存在和严重程度。对这些病症的正确诊断和分级有助于指导治疗和潜在的手术，以帮助减轻背痛并改善患者的整体健康和生活质量。</p>
<p><em>RSNA has teamed with the <a target="_blank" rel="noopener" href="https://www.asnr.org/">American Society of Neuroradiology (ASNR)</a> to conduct this competition exploring whether artificial intelligence can be used to aid in the detection and classification of degenerative spine conditions using lumbar spine MR images.</em></p>
<p>RSNA 与<a target="_blank" rel="noopener" href="https://www.asnr.org/">美国神经放射学会 (ASNR)</a>合作举办了本次竞赛，探讨人工智能是否可以利用腰椎 MR 图像来帮助检测和分类脊柱退行性疾病。</p>
<p><em>The challenge will focus on the classification of five lumbar spine degenerative conditions: Left Neural Foraminal Narrowing, Right Neural Foraminal Narrowing, Left Subarticular Stenosis, Right Subarticular Stenosis, and Spinal Canal Stenosis. For each imaging study in the dataset, we’ve provided severity scores (Normal&#x2F;Mild, Moderate, or Severe) for each of the five conditions across the intervertebral disc levels L1&#x2F;L2, L2&#x2F;L3, L3&#x2F;L4, L4&#x2F;L5, and L5&#x2F;S1.</em></p>
<p>挑战将集中于五种腰椎退行性疾病的分类：左神经椎间孔狭窄、右神经椎间孔狭窄、左关节下狭窄、右关节下狭窄和椎管狭窄。对于数据集中的每项影像学研究，我们为椎间盘 L1&#x2F;L2、L2&#x2F;L3、L3&#x2F;L4、L4&#x2F;L5 级别的五种情况中的每一种提供了严重性评分（正常&#x2F;轻度、中度或严重）和 L5&#x2F;S1。</p>
<p><em>To create the ground truth dataset, the RSNA challenge planning task force collected imaging data sourced from eight sites on five continents. This multi-institutional, expertly curated dataset promises to improve standardized classification of degenerative lumbar spine conditions and enable development of tools to automate accurate and rapid disease classification.</em></p>
<p>为了创建实况数据集，RSNA 挑战计划工作组收集了来自五大洲八个站点的成像数据。这个多机构、专业策划的数据集有望改善退行性腰椎疾病的标准化分类，并支持开发自动化准确、快速的疾病分类工具。</p>
<p><em>Challenge winners will be recognized at an event during the RSNA 2024 annual meeting. For more information on the challenge, contact RSNA Informatics staff at <a href="mailto:informatics@rsna.org">informatics@rsna.org</a>.</em></p>
<p>挑战赛获胜者将在 RSNA 2024 年年会期间的活动中获得表彰。有关挑战的更多信息，请联系 RSNA 信息学工作人员： <a href="mailto:informatics@rsna.org">informatics@rsna.org</a> 。</p>
<h4 id="Evaluation-评估"><a href="#Evaluation-评估" class="headerlink" title="Evaluation 评估"></a>Evaluation 评估</h4><p><em>Submissions are evaluated using the average of sample weighted log losses and an <code>any_severe_spinal</code> prediction generated by the metric. The <a target="_blank" rel="noopener" href="https://www.kaggle.com/code/metric/rsna-lumbar-metric-71549">metric notebook can be found here</a>.</em></p>
<p>使用样本加权对数损失的平均值和由该指标生成的<code>any_severe_spinal</code>预测来评估提交的结果。<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/metric/rsna-lumbar-metric-71549">公制笔记本可以在这里找到</a>。</p>
<p><em>The sample weights are as follows:</em></p>
<p>样本权重如下：</p>
<ul>
<li>1 for normal&#x2F;mild. 1 为正常&#x2F;轻度。</li>
<li>2 for moderate. 2为中等。</li>
<li>4 for severe. 4为严重。</li>
</ul>
<p><em>For each row ID in the test set, you must predict a probability for each of the different severity levels. The file should contain a header and have the following format:</em></p>
<p>对于测试集中的每个行 ID，您必须预测每个不同严重性级别的概率。该文件应包含标头并具有以下格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">row_id,normal_mild,moderate,severe</span><br><span class="line">123456_left_neural_foraminal_narrowing_l1_l2,0.333,0.333,0.333</span><br><span class="line">123456_left_neural_foraminal_narrowing_l2_l3,0.333,0.333,0.333</span><br><span class="line">123456_left_neural_foraminal_narrowing_l3_l4,0.333,0.333,0.333</span><br><span class="line">etc.</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>row_id</th>
<th>normal_mild</th>
<th>moderate</th>
<th>severe</th>
</tr>
</thead>
<tbody><tr>
<td>123456_left_neural_foraminal_narrowing_l1_l2</td>
<td>0.333</td>
<td>0.333</td>
<td>0.333</td>
</tr>
<tr>
<td>123456_left_neural_foraminal_narrowing_l2_l3</td>
<td>0.333</td>
<td>0.333</td>
<td>0.333</td>
</tr>
<tr>
<td>123456_left_neural_foraminal_narrowing_l3_l4</td>
<td>0.333</td>
<td>0.333</td>
<td>0.333</td>
</tr>
<tr>
<td>etc.</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><em>In rare cases the lowest vertebrae aren’t visible in the imagery. You still need to make predictions (nulls will cause errors), but those rows will not be scored.</em></p>
<p>在极少数情况下，图像中看不到最低的椎骨。您仍然需要进行预测（空值会导致错误），但这些行不会被评分。</p>
<p><em>For this competition, the</em> <code>any_severe_scalar</code> <em>has been set to</em> <code>1.0</code>.</p>
<p>对于本次比赛，<code>any_severe_scalar</code>已设置为<code>1.0</code> 。</p>
<h3 id="Dataset-Description-数据集描述"><a href="#Dataset-Description-数据集描述" class="headerlink" title="Dataset Description 数据集描述"></a>Dataset Description 数据集描述</h3><p>The goal of this competition is to identify medical conditions affecting the lumbar spine in MRI scans.<br>本次比赛的目标是通过 MRI 扫描识别影响腰椎的医疗状况。</p>
<p>This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a full length sample submission) will be made available to your notebook.<br>本次比赛采用隐藏测试方式。当您提交的笔记本电脑被评分时，实际的测试数据（包括完整长度的样本提交）将提供给您的笔记本电脑。</p>
<h4 id="Files-文件"><a href="#Files-文件" class="headerlink" title="Files 文件"></a>Files 文件</h4><p><strong>train.csv</strong> <em>Labels for the train set</em>. 训练集的标签。</p>
<ul>
<li><code>study_id</code> - <em>The study ID. Each study may include multiple series of images.</em><br><code>study_id</code> - 研究 ID，每个研究可能包括多个系列的图像。</li>
<li><code>[condition]_[level]</code> - <em>The target labels, such as</em> <code>spinal_canal_stenosis_l1_l2</code> <em>, with the severity levels of</em> <code>Normal/Mild</code><em>,</em> <code>Moderate</code> <em>, or</em> <code>Severe</code> <em>. Some entries have incomplete labels.</em><br><code>[condition]_[level]</code> - 目标标签，例如<code>spinal_canal_stenosis_l1_l2</code> ，严重程度级别为 <code>Normal/Mild</code> 、 <code>Moderate</code> 或 <code>Severe</code>。有些条目的标签不完整。</li>
</ul>
<p><strong>train_label_coordinates.csv<br>训练标签坐标.csv</strong></p>
<ul>
<li><code>study_id</code></li>
<li><code>series_id</code> - The imagery series ID.<br><code>series_id</code> - 图像系列 ID。</li>
<li><code>instance_number</code> - The image’s order number within the 3D stack.<br><code>instance_number</code> - 图像在 3D 堆栈中的顺序号。</li>
<li><code>condition</code> - There are three core conditions: spinal canal stenosis, neural_foraminal_narrowing, and subarticular_stenosis. The latter two are considered for each side of the spine.<br><code>condition</code> - 共有三种核心病症：椎管狭窄、神经椎间孔狭窄和关节下狭窄。脊柱的每一侧都考虑后两者。</li>
<li><code>level</code> - The relevant vertebrae, such as <code>l3_l4</code><br><code>level</code> - 相关椎骨，例如<code>l3_l4</code></li>
<li><code>[x/y]</code> - The x&#x2F;y coordinates for the center of the area that defined the label.<br><code>[x/y]</code> - 定义标签的区域中心的 x&#x2F;y 坐标。</li>
</ul>
<p><strong>sample_submission.csv 样本提交.csv</strong></p>
<ul>
<li><code>row_id</code> - A slug of the study ID, condition, and level such as <code>12345_spinal_canal_stenosis_l3_l4</code>.<br><code>row_id</code> - 研究 ID、条件和级别的 slug，例如<code>12345_spinal_canal_stenosis_l3_l4</code> 。</li>
<li><code>[normal_mild/moderate/severe]</code> - The three prediction columns.<br><code>[normal_mild/moderate/severe]</code> - 三个预测列。</li>
</ul>
<p><strong>[train&#x2F;test]_images&#x2F;[study_id]&#x2F;[series_id]&#x2F;[instance_number].dcm</strong> The imagery data.图像数据。</p>
<p><strong>[train&#x2F;test]_series_descriptions.csv</strong></p>
<ul>
<li><p><code>study_id</code></p>
</li>
<li><p><code>series_id</code></p>
</li>
<li><p><code>series_description</code> The scan’s orientation.<br><code>series_description</code> 扫描方向。</p>
</li>
</ul>
<h3 id="方案说明"><a href="#方案说明" class="headerlink" title="方案说明"></a>方案说明</h3><p>Transformer + KAN 6.09 结果很不好</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-13T12:19:01.000Z" title="2024/8/13 20:19:01">2024-08-13</time>发表</span><span class="level-item"><time dateTime="2024-10-18T15:38:30.830Z" title="2024/10/18 23:38:30">2024-10-18</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">6 分钟读完 (大约903个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/13/Deploy-LLMs-ON-Linux/">Deploy_LLMs_ON_Linux</a></p><div class="content"><h2 id="如何在-Linux-服务器上搭建本地LLMs-🤔"><a href="#如何在-Linux-服务器上搭建本地LLMs-🤔" class="headerlink" title="如何在 Linux 服务器上搭建本地LLMs 🤔"></a>如何在 Linux 服务器上搭建本地LLMs 🤔</h2><p>如何在 <code>Linux</code> 服务器上部署大语言模型，以 <code>qwen1_5-32b-chat-q8_k_0</code> 为例。服务器使用显卡 <code>A4000</code>，预算：$5950$ 元。</p>
<h3 id="搭建-qwen1-5-32b-chat-q8-k-0"><a href="#搭建-qwen1-5-32b-chat-q8-k-0" class="headerlink" title="搭建 qwen1_5-32b-chat-q8_k_0"></a>搭建 <code>qwen1_5-32b-chat-q8_k_0</code></h3><ol>
<li><p>下载 🤗<code>Hugging Face</code> 库，这个库主要是用于下载模型使用。当然为了保证速度，我们可以使用 <code>wget</code> 命令替代他。如果你决定使用 <code>wget</code> 命令，你可以选择跳过这一步，具体的使用方式在第五步呈现。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ pip install huggingface_hub</span><br></pre></td></tr></table></figure>

<p>或者是直接下载 <code>modelscope</code> 库，使用 <code>modelscope</code> 下载模型（⭐推荐）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ pip install modelscope</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个 <code>LocalGit</code> 文件夹，并进入该文件夹</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ <span class="built_in">mkdir</span> LocalGit</span><br><span class="line">(base) ➜  ~ <span class="built_in">cd</span> LocalGit</span><br><span class="line">(base) ➜  LocalGit </span><br></pre></td></tr></table></figure>
</li>
<li><p>克隆 <code>llama.cpp</code> 的仓库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  LocalGit git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp</span><br><span class="line">(base) ➜  LocalGit <span class="built_in">cd</span> llama.cpp</span><br><span class="line">(base) ➜  llama.cpp git:(master)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在有 <code>GPU</code> 的环境下编译 <code>llama.cpp</code></p>
<p><strong>前置条件：</strong>安装 <code>nvcc</code> + <code>cmake</code></p>
<p>执行代码进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc</span><br></pre></td></tr></table></figure>

<p>如果出现错误：<code>(base) ➜  llama.cpp git:(master) make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc Makefile:76: *** LLAMA_CUBLAS is removed. Use GGML_CUDA instead..  Stop.</code></p>
<p>修改代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) make GGML_CUDA=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc</span><br></pre></td></tr></table></figure>

<p>为了加快编译速度，我们可以尝试以下命令添加参数 <code>j</code>，<code>j</code> 后面的数字表示同时编译的线程数（可根据 <code>CPU</code> 核数决定），实测能缩短约 $1&#x2F;3$ 的时间：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) make -j6 GGML_CUDA=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载相应的模型</p>
<p>① 使用 <code>Hugging Face</code> 下载相应模型，实测服务器网速在 <code>3M~6M</code> 左右，具体方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ huggingface-cli download Qwen/Qwen1.5-32B-Chat-GGUF qwen1_5-32b-chat-q8_0.gguf --local-dir . --local-dir-use-symlinks False</span><br></pre></td></tr></table></figure>

<p>② 使用 <code>wget</code> 下载 <code>modelscope</code> 的模型文件，实测网速在 <code>10M~22M</code> 左右，这需要你先获取到模型的下载链接，具体方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ wget https://www.modelscope.cn/models/qwen/Qwen1.5-32B-Chat-GGUF/resolve/master/qwen1_5-32b-chat-q8_0.gguf</span><br></pre></td></tr></table></figure>

<p>③ 直接使用 <code>modelscope</code> 库下载模型，实测网速在 <code>18M~65M</code> 左右，具体方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ <span class="built_in">cd</span> LocalGit </span><br><span class="line">(base) ➜  LocalGit <span class="built_in">mkdir</span> models</span><br><span class="line">(base) ➜  LocalGit <span class="built_in">cd</span> models</span><br><span class="line">(base) ➜  models modelscope download --model=qwen/Qwen2-7B-Instruct-GGUF --local_dir . qwen2-7b-instruct-q8_0.gguf</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 <code>llama.cpp</code> 的相关命令进行操作</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) ./main -m ../models/qwen1_5-32b-chat-q8_0.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) ./llama-server -m ../models/qwen1_5-32b-chat-q8_0.gguf -ngl 80 -fa</span><br></pre></td></tr></table></figure>

<p>如果是 <code>Qwen2-7B-Instruct-GGUF</code>，可以参考官方文档：<a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/qwen/qwen2-7b-instruct-gguf">Qwen2-7B-Instruct-GGUF · 模型库 — Qwen2-7B-Instruct-GGUF · 模型库 (modelscope.cn)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  llama.cpp git:(master) ./llama-server -m ../models/qwen2-7b-instruct-q8_0.gguf -ngl 29 -fa</span><br></pre></td></tr></table></figure>
</li>
<li><p>兼容 <code>OpenAI API</code>，使用 <code>Python</code> 代码测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import openai</span><br><span class="line"></span><br><span class="line">client = openai.OpenAI(</span><br><span class="line">    base_url=<span class="string">&quot;http://localhost:8080/v1&quot;</span>, <span class="comment"># &quot;http://&lt;Your api-server IP&gt;:port&quot;</span></span><br><span class="line">    api_key = <span class="string">&quot;sk-no-key-required&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">    model=<span class="string">&quot;qwen&quot;</span>,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;tell me something about michael jordan&quot;</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(completion.choices[0].message.content)</span><br></pre></td></tr></table></figure>
</li>
<li><p>命令启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./llama-cli -m qwen2-7b-instruct-q5_k_m.gguf \</span><br><span class="line">  -n 512 -co -i -<span class="keyword">if</span> -f prompts/chat-with-qwen.txt \</span><br><span class="line">  --in-prefix <span class="string">&quot;&lt;|im_start|&gt;user\n&quot;</span> \</span><br><span class="line">  --in-suffix <span class="string">&quot;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span> \</span><br><span class="line">  -ngl 24 -fa</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="拓展补充"><a href="#拓展补充" class="headerlink" title="拓展补充"></a>拓展补充</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shebao3333/article/details/139428868">Llama.cpp大模型量化简明手册_llamacpp量化-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/oandy0/article/details/135712598">【Llama2 windows部署详细教程】第二节：llama.cpp成功在windows上编译的秘诀_llama cpp 编译-CSDN博客</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-12T15:10:23.000Z" title="2024/8/12 23:10:23">2024-08-12</time>发表</span><span class="level-item"><time dateTime="2024-08-13T09:55:49.971Z" title="2024/8/13 17:55:49">2024-08-13</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">2 分钟读完 (大约226个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/12/Windows-Build-llama-cpp/">Windows Build llama.cpp</a></p><div class="content"><h2 id="Windows-平台下构建-llama-cpp"><a href="#Windows-平台下构建-llama-cpp" class="headerlink" title="Windows 平台下构建 llama.cpp"></a>Windows 平台下构建 <code>llama.cpp</code></h2><p>在使用 <code>LM-Studio</code> 时，对于一些参数量不是很大的模型来说，大多数不需要进行模型的合并，如 <code>qwen2-7b</code> 等。这些模型往往只需要下载后加载到 <code>LM-Studio</code> 中即可。</p>
<p>但是对于参数量很大的模型，如 <code>qwen2-72b-instruct</code> 等，因为模型文件较大不利于传输，因此模型开发者可能会使用 <code>llama.cpp</code> 对 <code>GGUF</code> 模型进行拆分，所以这个时候我们在下载模型时就需要进行模型的合并。</p>
<p><code>qwen2-72b-instruct</code> 在 <code>q8</code> 量化给出了两个模型文件，分别是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qwen2-72b-instruct-q8_k_m-00001-of-00002.gguf</span><br><span class="line">qwen2-72b-instruct-q8_k_m-00002-of-00002.gguf</span><br></pre></td></tr></table></figure>

<p>为了使用这些分割后的 <code>GGUF</code> 文件，我们可以使用 <code>llama-gguf-split</code> 合并他们</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llama-gguf-spilt --merge input.gguf output.gguf</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-05T02:37:16.000Z" title="2024/8/5 10:37:16">2024-08-05</time>发表</span><span class="level-item"><time dateTime="2024-10-23T06:28:46.689Z" title="2024/10/23 14:28:46">2024-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">9 分钟读完 (大约1370个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/05/Deploy-LLMs-ON-PC/">Deploy_LLMs_ON_PC</a></p><div class="content"><h2 id="如何搭建运行在本地的-LLMs-🤔"><a href="#如何搭建运行在本地的-LLMs-🤔" class="headerlink" title="如何搭建运行在本地的 LLMs 🤔"></a>如何搭建运行在本地的 LLMs 🤔</h2><p>[TOC]</p>
<h3 id="🤗-1-基于-LM-Studio"><a href="#🤗-1-基于-LM-Studio" class="headerlink" title="🤗 1. 基于 LM-Studio"></a>🤗 1. 基于 <code>LM-Studio</code></h3><ol>
<li><p>访问 <code>LM-Studio</code>，网址：<a target="_blank" rel="noopener" href="https://lmstudio.ai/">LM Studio - Discover, download, and run local LLMs</a></p>
<p>下载对应系统的安装包，然后双击运行即可。</p>
</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Website.png">

<ol start="2">
<li>访问 <code>ModelScope</code><a target="_blank" rel="noopener" href="https://community.modelscope.cn/">魔搭社区</a> 或者 🤗<code>Hugging Face</code><a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a>，这里以 <code>ModelScope</code> 为例，进入模型库，下载相应模型。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/ModelScope.png">

<center>魔搭社区官网</center>

<img src="./Deploy_LLMs_ON_PC/ModelScopeDownloadModels.png">

<center>找到需要的模型并下载</center>

<ol start="3">
<li>下载好响应的模型后，将模型组织好，放到相应的文件夹中，这里按照 <code>models/Publisher/Repository/*.gguf</code> 的路径组织模型路径，然后选择 <code>Change</code> 更改模型的位置。如果不按照该路径组织，则会出现 <code>You have 1 uncategorized model files.</code> 错误，如下图所示：</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/Error_With_Wrong_Project_Structure.png">

<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_ModelFile.png">

<ol start="4">
<li>但是那种方式是不太推荐的，我们组织 <code>USER/MODEL_NAME/*.gguf</code> 的结构，这种结构会比较明了：</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_ModelFile0002.png">

<ol start="5">
<li>完成模型文件的下载和组织后，我们可以进入聊天页面，选择模型进行加载。这里为了节约空间，我删除了 <code>nilera/Qwen1.5-7B-Chat-Q4-GGUF</code> 目录下的文件。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/SelectAModel2Load.png">

<ol start="6">
<li>选择模型加载，等待加载完成即可像平时使用其他大模型的时候一样使用这些模型。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_Test_qwen1_5.png">

<ol start="7">
<li>但是如果我们想在代码中使用我们的大模型应该怎么做呢？我们可以选择 <code>LM-Studio</code> 的 <code>Local Server</code> 菜单项，选择 <code>Start Server</code> 即可部署到一个本地指定的端口（默认是 <code>1234</code>）。</li>
</ol>
<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_Start_Local_Server0001.png">

<img src="./Deploy_LLMs_ON_PC/LM_Studio_Change_Start_Local_Server_0002.png">

<ol start="8">
<li>右侧有许多样例，我们可以选择一段样例，如：<code>chat(python)</code>，这里对这段代码进行简单的解释。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example: reuse your existing OpenAI setup</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># Point to the local server</span></span><br><span class="line">client = OpenAI(base_url=<span class="string">&quot;http://localhost:1234/v1&quot;</span>, api_key=<span class="string">&quot;lm-studio&quot;</span>)</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">&quot;Publisher/Repository&quot;</span>,									<span class="comment"># 可以理解为模型路径, 这里以启动在这个端口的模型为准</span></span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Always answer in Chinese.&quot;</span>&#125;,	<span class="comment"># 系统设置: 每次都用中文回答</span></span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Introduce yourself.&quot;</span>&#125;			<span class="comment"># 对话设置: 这里希望 AI 介绍一下他自己</span></span><br><span class="line">  ],</span><br><span class="line">  temperature=<span class="number">0.7</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(completion.choices[<span class="number">0</span>].message)							<span class="comment"># 获取模型的回复</span></span><br></pre></td></tr></table></figure>

<ol start="9">
<li>然后我们就可以愉快的使用 <code>Python</code> 调用我们的本地大模型了。</li>
</ol>
<h3 id="⛵-2-使用-PowerInfer-框架"><a href="#⛵-2-使用-PowerInfer-框架" class="headerlink" title="⛵ 2. 使用 PowerInfer 框架"></a>⛵ 2. 使用 <code>PowerInfer</code> 框架</h3><p><code>PowerInfer</code> 框架 <em>GitHub</em> 链接：<a target="_blank" rel="noopener" href="https://github.com/SJTU-IPADS/PowerInfer">SJTU-IPADS&#x2F;PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs (github.com)</a></p>
<p>$2024$ 年发布论文 <code>PowerInfer-2</code>：<font color=Blue>[</font><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.06282">2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone (arxiv.org)</a></p>
<p><code>Anaconda</code> 命令使用：<a target="_blank" rel="noopener" href="https://blog.csdn.net/miracleoa/article/details/106115730">【anaconda】conda创建、查看、删除虚拟环境（anaconda命令集）_conda 创建环境-CSDN博客</a></p>
<p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42232045/article/details/135113112">大模型笔记之-3090显卡推理70B参数模型|基于PowerInfer 一个 CPU&#x2F;GPU LLM 推理引擎-CSDN博客</a></p>
<ol>
<li>使用 <code>Conda</code> 创建环境，这里 Python 版本需要大于 <code>3.8</code>：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n powerinfer1 python=3.8</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>激活 <code>Conda</code> 环境：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate powerinfer1</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>克隆 <code>PowerInfer</code> 框架代码：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:SJTU-IPADS/PowerInfer.git</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>安装所需依赖：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>使用 <code>CMake</code> 进行编译（<code>CMake</code> 版本需要大于：<code>3.17+</code>）</p>
<p>这里很大概率可能会出现编译器版本与 CUDA 版本不一致的情况，解决方案：<a target="_blank" rel="noopener" href="https://blog.csdn.net/lishiyu93/article/details/114599859">fatal error C1189: #error: – unsupported Microsoft Visual Studio version! - CSDN博客</a></p>
<p>这里我有三个 <code>CUDA</code> 版本，貌似修改其中任意一个就可以，这里我修改的是 <code>CUDA v11.6</code> 版本。</p>
<p>① 如果是 <code>NVIDIA GPUs</code>，需要使用如下方式进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake -S . -B build -DLLAMA_CUBLAS=ON</span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<p>② 如果是 <code>AMD GPUs</code>，需要使用下面的方式进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace &#x27;1100&#x27; to your card architecture name, you can get it by rocminfo</span></span><br><span class="line">CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++ cmake -S . -B build -</span><br><span class="line">DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1100</span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<p>③ 如果是 <code>CPU ONLY</code>，需要使用下面的方式进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake -S . -B build</span><br><span class="line">cmake --build build --config Release</span><br></pre></td></tr></table></figure>

<p>这里我有一块 <code>Nvidia 1050ti</code> 所以我使用<strong>方式 ①</strong>进行编译。</p>
</li>
<li><p>对于我们下载的模型，可以使用提供的方式进行转化，转化为 PowerInfer 可以使用的类型：</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make sure that you have done `pip install -r requirements.txt`</span></span><br><span class="line">python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR</span><br><span class="line"><span class="comment"># python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py --outfile D:/LMStudio/models/Publisher/Repository/qwen1_5-7b-chat-q4_0.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>或者将要 原始模型转化为 GGUF 模型</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert-dense.py --outfile /PATH/TO/DENSE/GGUF/REPO/MODELNAME.gguf /PATH/TO/ORIGINAL/MODEL</span><br></pre></td></tr></table></figure>

<ol start="8">
<li>运行模型</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/Release/main.exe -m C:/Users/NilEra/Downloads/llama-7b-relu.powerinfer.gguf -n 128 -t 2 -p <span class="string">&quot;Once upon a time&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中/home/user/data/ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.q4.powerinfer.gguf为GPTQ量化过的模型文件</span></span><br></pre></td></tr></table></figure>

<ol start="9">
<li>一些问题：</li>
</ol>
<p><img src="/./Deploy_LLMs_ON_PC/Issues22.png" alt="Issus22"></p>
<h3 id="🔧-3-在-Windows-上搭建-llama-cpp"><a href="#🔧-3-在-Windows-上搭建-llama-cpp" class="headerlink" title="🔧 3. 在 Windows 上搭建 llama.cpp"></a>🔧 3. 在 Windows 上搭建 <code>llama.cpp</code></h3><ol>
<li><p>在 Windows 上搭建 <code>llama.cpp</code> 是需要安装很多工具，且安装完成后也存在无法正常成功编译的情况（存在依赖、库等各种问题），因此这里我们可以使用 <code>w64devkit</code> 工具，使用他可以方便我们进行 <code>llama.cpp</code> 的编译。首先我们先下载 <code>w64devkit</code></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_49186516/article/details/142653813">参考网址01</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/oandy0/article/details/135712598">参考网址02</a></p>
</li>
<li><p>然后再 <code>make</code> 可以了</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-01T07:29:16.000Z" title="2024/8/1 15:29:16">2024-08-01</time>发表</span><span class="level-item"><time dateTime="2024-08-13T12:18:19.674Z" title="2024/8/13 20:18:19">2024-08-13</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">2 分钟读完 (大约289个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/01/LLM-General-Education/">LLM_General_Education</a></p><div class="content"><h2 id="🤗-大语言模型通识"><a href="#🤗-大语言模型通识" class="headerlink" title="🤗 大语言模型通识"></a>🤗 大语言模型通识</h2><h3 id="大语言模型的配置需求"><a href="#大语言模型的配置需求" class="headerlink" title="大语言模型的配置需求"></a>大语言模型的配置需求</h3><p>首先要搞清楚，本地可以部署什么大模型，取决于个人电脑的硬件配置，尤其需要关注 <code>GPU</code> 的显存。一般来说，只要本地机器 <code>GPU</code> 的显存能够满足大模型的要求，那基本上都可以本地部署。</p>
<p>那么大模型类别这么多，有 $7B$、$13B$、$70B$ 等等，<code>GPU</code> 显存如何准备呢？</p>
<p>在没有考虑任何模型量化技术的前提下，有公式如下：<br>$$<br>GB &#x3D; B × 2<br>$$</p>
<p>其中为 $GB$ 模型显存占用，$B$ 为大模型参数量。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv33371756/">AI大模型本地化部署Q&#x2F;A硬件篇</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35054222/article/details/139123564">如何找到最新的大模型、如何判断本地硬件是否满足大模型需求、如何快速部署大模型</a></p>
<p><a target="_blank" rel="noopener" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-leaderboard">大模型综合评测对比 | 当前主流大模型在各评测数据集上的表现总榜单 | 数据学习 (DataLearner)</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-08-01T07:29:16.000Z" title="2024/8/1 15:29:16">2024-08-01</time>发表</span><span class="level-item"><time dateTime="2024-10-23T12:38:43.409Z" title="2024/10/23 20:38:43">2024-10-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></span><span class="level-item">几秒读完 (大约9个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/08/01/LLM-Knowledge-Base/">LLM_General_Education</a></p><div class="content"><h2 id="📦-大模型知识库"><a href="#📦-大模型知识库" class="headerlink" title="📦 大模型知识库"></a>📦 大模型知识库</h2><p>[TOC]</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/page/0/">上一页</a></div><div class="pagination-next"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">1</a></li><li><a class="pagination-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatarMyself.jpg" alt="NilEra"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">NilEra</p><p class="is-size-6 is-block">C/C++ Developer!</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Jinan Shandong</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">48</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">41</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">47</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/NilEra-K" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/NilEra-K"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/NilEra-K" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">GitHub</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="https://nano.chemtian.top/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Thymol Blue</span></span><span class="level-right"><span class="level-item tag">nano.chemtian.top</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/CppDev/"><span class="level-start"><span class="level-item">CppDev</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/CppDev/Gaming/"><span class="level-start"><span class="level-item">Gaming</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/QT6/"><span class="level-start"><span class="level-item">QT6</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"><span class="level-start"><span class="level-item">多线程编程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"><span class="level-start"><span class="level-item">网络编程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/"><span class="level-start"><span class="level-item">语法点</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/bind/"><span class="level-start"><span class="level-item">bind</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/enum-class/"><span class="level-start"><span class="level-item">enum class</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/%E8%99%9A%E5%87%BD%E6%95%B0/"><span class="level-start"><span class="level-item">虚函数</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/JavaDev/"><span class="level-start"><span class="level-item">JavaDev</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/JavaDev/Project/"><span class="level-start"><span class="level-item">Project</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/JavaDev/SpringBoot/"><span class="level-start"><span class="level-item">SpringBoot</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/%E8%BD%AF%E4%BB%B6%E6%BA%90/"><span class="level-start"><span class="level-item">软件源</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"><span class="level-start"><span class="level-item">大数据技术</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/ECharts/"><span class="level-start"><span class="level-item">ECharts</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/HBase/"><span class="level-start"><span class="level-item">HBase</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"><span class="level-start"><span class="level-item">数据可视化</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/"><span class="level-start"><span class="level-item">项目</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="level-start"><span class="level-item">开发工具</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Axure-RP-9/"><span class="level-start"><span class="level-item">Axure RP 9</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/SVN/"><span class="level-start"><span class="level-item">SVN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/VSCode/"><span class="level-start"><span class="level-item">VSCode</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">数据结构/算法</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E5%93%88%E5%B8%8C-Hash/"><span class="level-start"><span class="level-item">哈希(Hash)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%88-Stack/"><span class="level-start"><span class="level-item">栈(Stack)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%91-Tree/"><span class="level-start"><span class="level-item">树(Tree)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E8%A1%A8-List/"><span class="level-start"><span class="level-item">线性表(List)</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/"><span class="level-start"><span class="level-item">LLMs</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Stable-Diffusion/"><span class="level-start"><span class="level-item">Stable_Diffusion</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"><span class="level-start"><span class="level-item">网络架构</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/Kolmogorov-Arnold-Networks/"><span class="level-start"><span class="level-item">Kolmogorov-Arnold Networks</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/%E9%97%B2%E8%81%8A/"><span class="level-start"><span class="level-item">闲聊</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E9%97%B2%E8%81%8A/%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">计划</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-24T02:01:05.000Z">2024-10-24</time></p><p class="title"><a href="/2024/10/24/LLM-FINETUNE/">LLM-FINETUNE</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-17T00:46:50.000Z">2024-10-17</time></p><p class="title"><a href="/2024/10/17/Run-Others-Java-Projects/">Run-Others-Java-Projects</a></p><p class="categories"><a href="/categories/JavaDev/">JavaDev</a> / <a href="/categories/JavaDev/Project/">Project</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-20T02:07:10.000Z">2024-09-20</time></p><p class="title"><a href="/2024/09/20/List/">List</a></p><p class="categories"><a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/">数据结构/算法</a> / <a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E8%A1%A8-List/">线性表(List)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-12T02:03:09.000Z">2024-09-12</time></p><p class="title"><a href="/2024/09/12/RedHat-Build-LLM/">RedHat-Build-LLM</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/">LLMs</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-11T01:06:12.000Z">2024-09-11</time></p><p class="title"><a href="/2024/09/11/SVN-Quick-IN/">SVN-Quick-IN</a></p><p class="categories"><a href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a> / <a href="/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/SVN/">SVN</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">十月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">八月 2024</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">七月 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">五月 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">四月 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">三月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Annotation/"><span class="tag">Annotation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Axure-RP-9/"><span class="tag">Axure RP 9</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-11-%E6%96%B0%E6%A0%87%E5%87%86/"><span class="tag">C++ 11 新标准</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CppDev/"><span class="tag">CppDev</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ECharts/"><span class="tag">ECharts</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EasyX/"><span class="tag">EasyX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Fine-Tune-%E5%BE%AE%E8%B0%83/"><span class="tag">Fine-Tune(微调)</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GameDev/"><span class="tag">GameDev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HBase/"><span class="tag">HBase</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hadoop/"><span class="tag">Hadoop</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaDev/"><span class="tag">JavaDev</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KAN/"><span class="tag">KAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kaggle/"><span class="tag">Kaggle</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLMs/"><span class="tag">LLMs</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LM-Studio/"><span class="tag">LM-Studio</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Maven/"><span class="tag">Maven</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MyBatis-Plus/"><span class="tag">MyBatis-Plus</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PowerInfer/"><span class="tag">PowerInfer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QT6/"><span class="tag">QT6</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/R/"><span class="tag">R</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RedHat/"><span class="tag">RedHat</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SVN/"><span class="tag">SVN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scala/"><span class="tag">Scala</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spark/"><span class="tag">Spark</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SpringBoot/"><span class="tag">SpringBoot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stable-Diffusion/"><span class="tag">Stable Diffusion</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tomcat/"><span class="tag">Tomcat</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VSCode/"><span class="tag">VSCode</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ZooKeeper/"><span class="tag">ZooKeeper</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"><span class="tag">多线程编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"><span class="tag">大数据技术</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">大模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="tag">开发工具</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">数据结构</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"><span class="tag">网络编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E6%B3%95%E7%82%B9/"><span class="tag">语法点</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BD%AF%E4%BB%B6%E6%BA%90/"><span class="tag">软件源</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%97%B2%E8%81%8A/"><span class="tag">闲聊</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A1%B9%E7%9B%AE/"><span class="tag">项目</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/StarLogo.svg" alt="Hello, NilEra :-)" height="28"></a><p class="is-size-7"><span>&copy; 2024 NilEra</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024 前方⚡高能</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/NilEra-K"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container" id="algolia-input"></div><div id="algolia-poweredby" style="display:flex;margin:0 .5em 0 1em;align-items:center;line-height:0"></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div><div class="searchbox-footer"></div></div></div><script src="https://cdnjs.loli.net/ajax/libs/algoliasearch/4.0.3/algoliasearch-lite.umd.js" crossorigin="anonymous" defer></script><script src="https://cdnjs.loli.net/ajax/libs/instantsearch.js/4.3.1/instantsearch.production.min.js" crossorigin="anonymous" defer></script><script src="/js/algolia.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadAlgolia({"applicationId":"2TB5ZZYPCO","apiKey":"00a43f1d62ca7b24c8b78d5f0223c065","indexName":"dev_nilera_blog"}, {"hint":"想要查找什么...","no_result":"未找到搜索结果","untitled":"(无标题)","empty_preview":"(无内容预览)"});
        });</script></body></html>