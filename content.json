{"posts":[{"title":"CentOS7更换阿里&#x2F;清华源","text":"Blog网址：https://www.cnblogs.com/wswind/p/10173591.html","link":"/2024/04/23/CentOS7%E6%9B%B4%E6%8D%A2%E9%98%BF%E9%87%8C-%E6%B8%85%E5%8D%8E%E6%BA%90/"},{"title":"Axure RP 9 产品设计入门","text":"🚪Axure RP 9 产品设计入门 Axure RP 9 软件的安装可以直接官网下载，然后去某宝买一个账号的激活码，详情不再赘述，可以自行 Bing。下载网址：https://www.axure.com/release-history/rp9学习参考视频：https://www.bilibili.com/video/BV1hU4y1L77u/ Axure 软件的一些基础设置 偏好设置 文件 → Preferences（首选项、偏好设置） 网格设置 视图设置 → 标尺·网格·辅助线 → 网格设置 我个人认为舒服的设置：网格对齐，间距 10 像素，样式更改为线段，颜色 #F2F2F2 如何预览、发给别人可以浏览的文件发布 → 生成 HTML 文件（选择一个文件夹，会生成很多文件，因此最好选择一个独立的文件夹） 常用的设计尺寸手机端：365*667 开始设计绘制一个简单的UI界面 用矩形绘制一个背景：使用线性方式填充 用水平线绘制隔断 用矩形绘制按钮 如何绘制按钮使用矩形绘制一个按钮 绘制矩形 右键矩形 → 交互样式 美化输入框 绘制矩形 Ctrl + D 复制一份，调整大小 添加输入框和一个搜索图标 母版选择绘制好的组件，右键选择转换为母版 单选框的使用和自定义单选框（美化操作）使用单选框时，需要对单选框进行编组。编组方式：框选对应组的单选框，右键选择指定单选按钮的组，然后命名组名即可。自定义单选框操作步骤： 使用圆形绘制一个大圆以及一个小圆 内部的小圆的线条和填充设置为透明 右键添加交互样式，设置当选中时，内部小圆的填充和线条更改颜色，外部大圆线条更改颜色 添加文字后进行编组，对于编组内容设置交互（和交互样式不同），在菜单栏的右侧，交互面板 效果如下： 常用工具页面工具 页面工具中可以创建多个页面 页面工具中可以按住 Ctrl + 左右方向键 来调整页面的级别 页面工具中可以按住 Ctrl + 上下方向键 来调整页面的顺序 概要工具 概要工具可以看到整个页面全部的元件 当存在遮罩关系时，可以选择概要工具中需要修改的元件，不需要破坏遮罩关系 可以通过筛选按钮来对元件进行筛选 建议给元件起好名字 钢笔工具 世上无难事，只怕有心人 使用钢笔工具来绘制自定义图案 绘制复杂的UI界面 常用的交互事件及操作页面跳转 页面跳转原理：事件 → 动作 → 目标 对于跳转来说：事件（点击） → 动作（跳转） → 目标（页面2） 事件的选择在“交互”面板处（注：不是交互样式） 实现时点击右侧交互面板新建交互，选择单击时，动作设置为打开链接，目标设置为要切换到的界面 当点击预览时候，会发现元件的显示往往出现在屏幕中心位置，如果想要让元件按照画布的绝对位置显示，可以点击页面，选择样式中的“页面排列”，在其中调整相关显示样式。 热区 为什么使用热区？ 对于图片中的情况，如果我们想点击按钮完成跳转操作，可以直接给使用方式 ① 按钮添加一个交互事件。 但是当我们想要点击这个区域时进行跳转，一个元件一个元件添加事件过于繁琐（技巧：可以通过 Ctrl + C 在右侧交互面板直接复制整个事件，然后点击想要添加该事件的元件，通过 Ctrl + V 粘贴到元件上），而且不方便后续的修改。 于是可以使用方式 ② 将要整个部分编组，然后为整个组添加事件。但是当我们因为某种原因破坏了整个组时，可能会遗忘添加事件，导致整个事件消失。 所以热区就是为了解决上面的问题，我们可以通过方式 ③ 给组件添加热区，只要点击这个区域即可完成指定事件。 且热区占用内存更小。 显示和隐藏设置文字设置某元件上的文字 设置选中设置一个元件为选中状态 启用或禁用 启用/禁用是针对于一个元件 常用场景是：当满足某条件时启用/禁用某元件 例如：当用户输入账号和密码后启用登录按钮（瞎编的），当用户同意协议才可进行下一步这种 移动 移动一个元件，可以在“移动”处设置经过（相对位置）、到达（绝对位置）来移动元件 等待 有时候等待看起来并不生效，需要观察是否是等待动作前的动作设置了动画。 例如：移动动作设置了 500ms 的动画，其实移动动作是瞬间完成的，动画只是一种呈现的方式。所以动画呈现的效果和等待是几乎同时进行的（“移动”动作设置 500ms 的动画和 500ms 的“等待”，动画开始播放时“等待”同时进行，动画结束播放时，“等待”结束）。 旋转设置尺寸设置透明度制作更复杂的动效 Axure RP 9 的动画效果是可以叠加的。 等待可以分割两个本来融合的叠加效果。 使用该上图所示的操作，会导致“移动”和“旋转”的融合无法叠加，先执行“移动”，再执行“旋转”。 文本框的长度是可以固定或者跟随的，当文本框样式如下时，则说明文本框此时的长度是固定的。此时可以双击两侧中间的锚点来使文本框变为跟随模式。下图为固定长度：下图为跟随长度：双击角上的锚点可以设置是否跟随高度、宽度（若不跟随，则此时文本框高度和宽度完全固定）。 复杂动效总体流程： 滚动到元件想要动画展示，一般设置动画的时间为 500ms，但是对于用户来说，500ms 看起来有些卡顿，所以 350ms 往往是个不错的选择 更多事件 事件 条件 备注 单击时 点击形状时 一般单击事件不能和“鼠标按下”/“鼠标松开” 事件同时使用 双击时 双击形状时 鼠标右击时 形状的上下文菜单被触发时（右键点击形状时候） 鼠标按下时 鼠标在形状上按下时 鼠标松开时 鼠标在形状上松开时 鼠标移动时 鼠标移动到形状上时 鼠标移入时 鼠标指针进入形状元件区域中时 鼠标移出时 鼠标指针离开形状元件区域中时 鼠标停放时 鼠标指针在停放在形状上超过 1s 时 鼠标长按时 鼠标指针在按压在形状上超过 1s 时 按键按下时 当焦点在形状上并按下键盘上的任意按键时 按键松开时 当焦点在形状上并按下键盘上的任意按键松开时 移动时 当形状发生移动时 旋转时 当形状发生旋转时 尺寸改变时 当形状尺寸发生改变时 显示时 当形状发生显示时 隐藏 当形状发生被隐藏时 获取焦点时 当形状获取焦点时 失去焦点时 当形状失去焦点时 选中 当形状被选中时 取消选中时 当形状取消选中时 选中改变时 当形状的选中状态发生改变时 载入时 形状已加载时（首次显示页面时） 更多动作内部框架 若概念不明确，往往会将内部框架和快照混淆。 快照往往用于在一个页面预览所有的页面，点击快照后跳转到快照包含的页面，并且在跳转后的页面进行修改。 而内部框架和快照不同，内部框架相当于将需要包含的页面复制了一份，粘贴到了内部框架中，因此可以直接在内部框架中修改内容，修改内部框架中的内容不会影响原页面内容。 动态面板 动态面板也是一个元件，也可以显示或隐藏。 我们可以将动态面板形象的理解为一个盒子。在盒子里没有东西时，动态面板“无色无味”，看不见摸不到，双击后可以键入盒子内部。 盒子内部也可以添加很多层： 我们可以通过一个按钮，添加“设置动态面板”动作来切换层，因此我们可以想到动态面板常用于实现轮播图（当然还有很多其他的玩法） 动态面板案例 1：遮罩使用动态面板可以实现遮罩的一些操作，如： 超出指定范围的元件可以使用动态面板，隐藏超出部分，右键直接将元件转换为动态面板即可： 将元件一直固定再某一位置选择固定到浏览器，并且进一步选择固定位置，当浏览器页面过长需要滚动时，动态面板的位置不会发生改变。 通栏选择 “100%宽度（仅浏览器中有效）”，即可将动态面板的宽度按照浏览器的宽度进行设置，也就是所谓的通栏（Banner，Banner也可以用来表示轮播图） 动态面板是可以嵌套的 动态面板案例 2：浮窗 动态面板案例 3：弹窗 动态面板案例 4：轮播 动态面板案例 5：Tab切页 动态面板案例 6：列表切页 动态面板案例 7：拖动 使用动态面板实现拖动的难点在于使用“动态面板的嵌套”和“设置边界” 使用动态面板的嵌套是为了防止直接拖动最外层的动态面板，而是拖动动态面板内部的一个面板。 使用边界的难点在于 Axure RP 9 在设计这个功能时的让人难以理解。这里简单的理解是： 对于“左侧”，$-35 &lt;= x &lt;= 0$ 对于“左侧”可以移动 0 像素或移动 -35 像素 对于“左侧”，可以不移动，或者向左移动 35 像素 用不熟练的时候多尝试 中继器 添加局部变量的方式： 首先点击添加局部变量 变量全局变量可以跨页面传参 判断 添加判断当添加交互事件时，不进一步选择动作，而是选择“启用情形”，在启用情形中添加判断。 局部变量获取元件位置，结合判断可以用 [[元件局部变量.x]] 获取元件的 x 位置 多条件判断多条件判断可以通过在“添加情形”部分添加行来增加判断条件，“匹配所有”和“匹配任何”两个情况，分别代表的是 and 和 or。 嵌套判断 案例：绘制注册 中间页","link":"/2024/04/12/Axure-RP-9-%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1%E5%85%A5%E9%97%A8/"},{"title":"[20240412] 下一步计划","text":"[Date: 20240412] 下一步计划[TOC] 📕 计划 1: 学习 Axure RP 9[*注] 2024.04.20 该计划已经完成主要是为了计划2做准备 📕 计划 2: 开发计划开发一个好玩的小工具这个其实是一个奇思妙想啦，目前还不知道能不能实现，总的来说就是希望用户上传一张文件，然后我们转化成命令行图片。就例如我用的网站博客的框架，他能用符号绘制一个很好看终端欢迎界面，我觉得是一个很有意思的东西，所以我准备在接下来的一年时间里实现这个小工具。目前连文件夹都没有创建，哈哈哈 :P 12345678INFO ======================================= ██╗ ██████╗ █████╗ ██████╗ ██╗ ██╗███████╗ ██║██╔════╝██╔══██╗██╔══██╗██║ ██║██╔════╝ ██║██║ ███████║██████╔╝██║ ██║███████╗ ██║██║ ██╔══██║██╔══██╗██║ ██║╚════██║ ██║╚██████╗██║ ██║██║ ██║╚██████╔╝███████║ ╚═╝ ╚═════╝╚═╝ ╚═╝╚═╝ ╚═╝ ╚═════╝ ╚══════╝============================================= 使用 QT 6 开发一个好玩的桌面端应用刷抖音的时候看到了，一些很好玩的评论，因为大家好像觉得小红书是一个给女孩子交流的平台，所以男孩子也希望有一个自己交流的平台，大概就是小蓝书的样子。这也是个很大的项目了，需要很长时间设计，不过最起码要等我学完Axure RP 9 吧，当然肯定不是全部实现，我就是想边学边开发，能做多少是多少的样子。 📕 计划 3: 读书📕 计划 4: 复习数据结构与算法复习的列表会开一篇新博客来归档","link":"/2024/04/12/20240412-%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AE%A1%E5%88%92/"},{"title":"PyTorch快速上手指南","text":"PyTorch 深度学习框架快速上手指南PyTorch 可以说是目前最常用的深度学习框架 , 常应用于搭建深度学习网络 , 完成一些深度学习任务 (CV、NLP领域) 要想快速上手 PyTorch , 你需要知道什么 : 一个项目的完整流程 , 即到什么点该干什么事 几个常用 (或者说必备的) 组件 剩下的时间你就需要了解 , 完成什么任务 , 需要什么网络 , 而且需要用大量的时间去做这件事情 $^{(e.g.)}$例如 : 你现在有一个图像分类任务 , 完成该任务需要什么网络, 你需要通过查找资料来了解需要查找什么网络。 需要注意的是 , 有一些常识性的问题你必须知道 , 例如: 图像层面无法或很难使用机器学习方法 , 卷积神经网络最多的是应用于图像领域等 下面我将通过一个具体的分类项目流程来讲述到什么点该干什么事一个完整的 PyTorch 分类项目需要以下几个方面: 准备数据集 加载数据集 使用变换(Transforms模块) 构建模型 训练模型 + 验证模型 推理模型 准备数据集 一般来说 , 比赛会给出你数据集, 不同数据集的组织方式不同 , 我们要想办法把他构造成我们期待的样子 分类数据集一般比较简单, 一般是将某个分类的文件全都放在一个文件夹中, 例如: 二分类问题 : Fake(文件夹) / Real(文件夹) 多分类问题 : 分类 1(文件夹) / 分类 2(文件夹) / … / 分类 N(文件夹) 当然有些时候他们会给出其他方式 , 如 UBC-OCEAN , 他们将所有的图片放在一个文件夹中 , 并用 csv 文件存储这些文件的路径(或者是文件名) , 然后在 csv 文件中进行标注(如下): 以后你可能还会遇到更复杂的目标检测的数据集, 这种数据集会有一些固定格式 , 如 VOC格式 , COCO格式等 在数据集方面 , 需要明确三个概念——训练集、验证集和测试集 , 请务必明确这三个概念 , 这是基本中的基本 训练集(Train) : 字如其名 , 简单来说就是知道数据 , 也知道标签 的数据 , 我们用其进行训练 验证集(Valid) : 验证集 和 测试集 是非常容易混淆的概念 , 简单来说 , 验证集就是我们也知道数据和标签 , 但是我们的一般不将这些数据用于训练 , 而是将他们当作我们的测试集 , 即我们已经站在了出题人的角度 , 给出参赛者输入数据 , 而我们知道这个数据对应的输出 , 但是我们不让模型知道 测试集(Test) : 测试集就是 , 我们不知道输入数据的输出标签 , 只有真正的出题人知道 , 一般来说 , 我们无法拿到测试集 , 测试集是由出题人掌控的 需要注意的是 , 如果你通过某种途径知道了所有的测试集的标签时 , 不可使用测试集进行训练 , 这是非常严重的学术不端行为 , 会被学术界和工业界唾弃 1234567891011# 现在我们已经有了一个数据集 , 我将以 FAKE_OR_REAL 数据集为例 , 展示我们数据集的结构# D:\\REAL_OR_FAKE\\DATASET# ├─test --------- 测试集路径, 这里可以放你自己的数据, 你甚至可以将他们分类, 但是请注意, 实际情况下你只能通过这种方式来“得到”测试集# │ ├─fake ------ 你自己分的类, 开心就好# │ └─real ------ 同上# ├─train -------- 训练集路径, 这里面放的是题目给出的数据, 下面有 fake 和 real 两个文件夹, 这两个文件夹中就是两个类别, 我们要用这里面的图片进行分类 # │ ├─fake# │ └─real# └─valid -------- 验证集路径, 这里面放的是题目给出的数据, 下面有 fake 和 real 两个文件夹, 这两个文件夹中就是两个类别, 这里面的图片不需要进行训练# ├─fake# └─real 加载数据集 请务必记住 , 不管是什么数据集 , 数据集是如何构成的 , 在使用 PyTorch 框架时 , 我们都要像尽办法将他们加载入 Dataset 类中 简单来说 , Dataset 类就是描述了我们数据的组成的类 需要注意 , PyTorch 实现了许多自己的 Dataset 类 , 这些类可以轻松的加载特定格式的数据集 , 但是我强烈建议所有的数据集都要自己继承Dataset类 , 自行加载 , 这样我们可以跟清晰的指导数据集的组成方式 , 也可以使得我们加载任意格式的数据集 实现 DataSet 类需要我们先继承 Dataset 类 , 在继承 Dataset 类后, 我们只需要实现其中的__init__、__len__和__getitem__三个方法 , 即可完成对数据集的加载 , 这三个方法就和他的名字一样 : __init__ 方法是构造函数 , 用于初始化 __len__ 方法用于获取数据集的大小 __getitem__ 方法用于获取数据集的元素 , 我将从下面的代码中进行更详细的解释 有些数据集并不分别提供 Train训练集 和 Valid验证集, 我们可以使用 random_split() 方法对数据集进行划分 需要注意的是, 每次重新划分数据集时, 必须重新训练模型, 因为 random_split() 方法随机性, 划分后的数据不可能和之前的数据完全重合, 因此会导致数据交叉的情况, 下面一段使用 random_split() 进行划分的 Python 代码示例 : 12345678# 下面演示使用 random_split 来划分数据集的操作# 我们假设已经定义了 CustomImageDataSetsplit_ratio = 0.8 # 表示划分比例为 8 : 2dataset = CustomImageDataSet(fake_dir, real_dir) # 定义 CustomImageDataSet 类, 假设此时没有划分训练集和验证集train_dataset_num = int(dataset.lens * split_ratio) # 定义训练集的大小valid_dataset_num = dataset.lens - train_dataset_num # 定义验证集的大小# random_split(dataset, [train_dataset_num, valid_dataset_num]) 表示将 dataset 按照 [train_dataset_num: valid_dataset_num] 的比例进行划分train_dataset, valid_dataset = random_split(dataset, [train_dataset_num, valid_dataset_num]) 当数据集不是很大的时, 推荐人为的将数据集进行划分, 可以写一个 Python 脚本(.py) 或者 批处理脚本(.bat) 来完成这个操作 完整的数据集加载代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torchfrom torch.utils.data import Datasetimport osfrom PIL import Image# 这里我们定义了一个 CustomImageDataset(...) 类, 括号中的内容表示我们继承了 ... 类# 因此我们这里 CustomImageDataset(Dataset): 表示我们定义了一个“自定义图片”类, 这个“自定义图片”类继承自 Dataset 类class CustomImageDataset(Dataset): # 这里我们实现 __init__ 方法, __init__ 方法其实就是一个类的构造函数, 他也分有参构造和无参构造, 只是在这里我们说无参构造基本没啥意义 # 因此我们常常实现这个类, 使得可以指定这个类的输入输出 # 比如下面我们写的 def __init__(self, fake_dir, real_dir, transform=None): # self : 自己, 我一般直接理解为 this 指针, 如果有兴趣了解更深层的东西可以查阅一些资料, 这个是必填的 # fake_dir : 用于指定 fake 类型图片的位置的 # real_dir : 用于指定 real 类型图片的位置的 # transform : 用于指定变换, 简单来说就是对输入进行某些操作, 我会在下面的板块中进行详细叙述 def __init__(self, fake_dir, real_dir, transform=None): self.fake_dir = fake_dir # 这里表示这个类内定义了一个 fake_dir, 其值为传入的 fake_dir self.real_dir = real_dir # 这里表示这个类内定义了一个 real_dir, 其值为传入的 real_dir self.transform = transform # 这里表示这个类内定义了一个 transform, 其值为传入的 transform, 当没有传入时, 这个变量为 None self.fake_images = os.listdir(fake_dir) # 传入的 fake_dir 是一个路径, 我们使用 os.listdir(fake_dir) 可以加载 fake_dir 文件夹下的内容, 也就是所有 fake 图片 self.real_images = os.listdir(real_dir) # 传入的 real_dir 是一个路径, 我们使用 os.listdir(real_dir) 可以加载 real_dir 文件夹下的内容, 也就是所有 real 图片 self.total_images = self.fake_images + self.real_images # 总图片列表, 就是将 fake 图片列表和 real 图片列表进行组合 self.labels = [0]*len(self.fake_images) + [1]*len(self.real_images) # 对图片打标签, fake 为 0, real 为 1 # [0] * 10 得到的结果为 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # [1] * 10 得到的结果为 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # 这里我们实现 __len__ 方法, 这个方法用于获取数据集的大小 def __len__(self): return len(self.total_images) # 这里我们直接返回总图片列表的长度即可, 这里的实现方式不唯一, 只要能做到表示数据集大小即可 # 这里我们实现 __getitem__ 方法, 这个方法用于获取数据集中的某个元素 # 其中 idx 表示索引, 这个参数是必须的, 当然可以起其他名字, 不过最好还是使用 idx # __getitem__(self, idx) 表示获取 idx 位置的元素 def __getitem__(self, idx): # 这里表示获取一个元素的逻辑 # 当 idx 位置的标签为 0 时, 图片的路径为 fake_dir + self.total_images[idx], idx 即为图片的索引位置 # 当 idx 位置的标签为 1 时, 图片的路径为 real_dir + self.total_images[idx] image_path = os.path.join(self.fake_dir if self.labels[idx] == 0 else self.real_dir, self.total_images[idx]) # 使用 PIL 库加载图片, 通过 image_path 打开图片, 并且将图片转化为 RGB 格式 image = Image.open(image_path).convert('RGB') # 这里是 transform, 表示变换, 当其值为 None 时不进行操作, 当传入自己的 transform 时即为非空, 即对输入数据进行变换 if self.transform: # 我们将变换后的图片直接保存在原位置 image = self.transform(image) # 最后函数的返回值为 image 和 self.labels[idx], 即表示索引位置 idx 处的图片和标签 return image, self.labels[idx] 使用 Transforms 不要简单的使用原始图片进行训练 , 当然如果一定要使用原始图片进行训练, 也可以使用 transforms 模块 一般来说, 训练集和验证集的 transforms 是不同的, 因为我们希望验证集和测试集的图片贴合真实的情况 下面的代码演示了如何定义 transforms 在定义完 transforms 我们就可以完全定义我们的 Dataset 和 Dataloader 了 123456789101112131415161718192021222324252627282930import torchfrom torchvision import transforms# 定义transform# transforms.Compose(transforms) 实际上就是将多个 transform 方法变为逐步执行, 一般我们直接使用这种方式来对图片进行连续的变换train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(), # 随机水平翻转 transforms.RandomVerticalFlip(), # 随机垂直翻转 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # 改变图像的属性, 将图像的brightness亮度/contrast对比度/saturation饱和度/hue色相 随机变化为原图亮度的 10% transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # 对图片先进行随机采集, 然后对裁剪得到的图像缩放为同一大小, 意义是即使只是该物体的一部分, 我们也认为这是该类物体 transforms.RandomRotation(40), # 在[-40, 40]范围内随机旋转 transforms.RandomAffine(degrees=0, shear=10, scale=(0.8,1.2)), # 随机仿射变换 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 色彩抖动 transforms.ToTensor(), # [重点] 将图片转化为 Tensor 张量, 在 PyTorch 中, 一切的运算都基于张量, 请一定将你的输入数据转化为张量 # 请理解什么是张量 : 我们在线性代数中有向量的概念, 简单来说就是张量就是向量, 只不过张量往往具有更高的维度 # 而大家一般习惯将高于三维的向量称为张量, 某些人(比如我)也习惯所有的向量统称为张量 # 可以简单的将数组的维数来界定张量的维度 # 例如 [ ] 为一维张量, [[ ]] 为二维张量, [[[ ]]]为三维张量, [[[[ ]]]]为四维张量 # 对于图像来说, jpg 图像实际为三维矩阵, png 图像实际为四维矩阵, 这个维数是根据图像的通道数进行划分的 # 例如 jpg 有 R、G、B三个通道, png 具有 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 归一化, 可以对矩阵进行归一化 # 详细查看这个Blog : https://blog.csdn.net/qq_38765642/article/details/109779370 transforms.RandomErasing() # 随机擦除])valid_transform = transforms.Compose([ transforms.Resize((256, 256)), # Resize 操作, 将图片转换到指定的大小 transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) 12345678910111213141516171819from torch.utils.data import DataLoader# 定义 Dataset 实例train_dataset = CustomImageDataset(fake_dir=&quot;./dataset/train/fake&quot;, real_dir=&quot;./dataset/train/real&quot;, transform=train_transform)valid_dataset = CustomImageDataset(fake_dir=&quot;./dataset/valid/fake&quot;, real_dir=&quot;./dataset/valid/real&quot;, transform=valid_transform)# 创建 DataLoader 实例# 这里将要涉及到超参数的概念, 什么是超参数: 简单来将, 超参数就是我们自己能指定的一些数据, 超参数的选择将很大程度上影响模型的性能# 因此 深度学习领域的工程师 常称自己为 炼丹师、调参师等batch_size = 32 # batch_size 就是一个超参数, batch 即为 “批次”, 表示一次使用 DataLoader 加载多少张图片进行运算 # 这个数值并不是越大越好, 也不是越小越好, 但是往往大一些比较好, 这个数字最大能选择多大和你的图片大小和显卡显存有很大的关系 # 当出现 [Out Of Memery] 错误时往往表明你选取了过大的 batch_size, 导致显卡出现了爆显存的问题# batch_size : 每次训练时，模型所看到的数据数量。它是决定训练速度和内存使用的重要参数。# shuffle : 是否在每个训练周期之前打乱数据集的顺序。这对于许多模型（如卷积神经网络）是很有帮助的，因为它可以帮助模型避免模式识别。# sampler : 定义如何从数据集中抽样。默认情况下，它使用随机采样。但你可以使用其他更复杂的采样策略，如学习率调度采样。# batch_sampler : 与sampler类似，但它在批处理级别上进行采样，而不是在整个数据集上。这对于内存使用效率更高的场景很有用。# num_workers : 定义了多少个工作进程用于数据的加载。这可以加快数据加载的速度，但需要注意内存的使用情况。train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) 1234567891011# 查看Dataloader数据# 为了了解Dataloader中的数据, 我们可以使用以下方法来查看:# 使用 Python 的 len() 函数 : 我们可以直接通过 len() 函数获取 Dataloader 的长度, 即数据集中数据块的数量# 使用 torch.utils.data.DataLoader.len() 方法 : 这个方法也会返回Dataloader的长度。# 使用 iter() 函数：Dataloader是一个可迭代对象，我们可以直接通过iter()函数对其进行迭代，以获取每个批次的数据。# 使用torchvision.utils.save_image()函数 : 如果我们正在处理的是图像数据集，那么可以使用这个函数来保存Dataloader中的图像数据。 len(train_loader) # 401len(valid_loader) # 100images, labels = next(iter(train_loader))print(images)print(labels) 构建模型 构建模型是比较重要的一部分, 一般来说做好数据集之后, 最重要的事情就是修改模型, 通过训练结果改进模型, 判断自己的模型的正确性, 这里就是整个你要用到的神经网络的部分 , 需要注意的是 , 这里指定什么输入 , 推理的时候就要指定什么输入 简单用几个符号说明一下就是: $^{Train} model (inputX, inputY, …)$ → $^{Valid} model (inputX, inputY, …)$ 如何确定输入是什么: 看 forward() 的输入是啥模型的输入就是啥 我下面展现了我复现的 ResNet50 , 用这种方式可以顺便教你如何复现网络结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140import torch.nn as nnfrom torch.nn import functional as F# 这里是对 ResNet50 的实现, 请对照论文来进行对照阅读# 定义 ResNet50Basic类, 这里并不是完整的模型, 而是模型的一个部分class ResNet50BasicBlock(nn.Module): def __init__(self, in_channel, outs, kernerl_size, stride, padding): # super(ResNet50BasicBlock, self).__init__() 这里是干什么的? # 1. 首先找到 ResNet50BasicBlock 的父类, 这里是 nn.Module # 2. 把类 ResNet50BasicBlock 的对象self转换为 nn.Module 的对象 # 3. &quot;被转换&quot;的 nn.Module 对象调用自己的 init 函数 # 简单理解一下就是 : 子类把父类的 __init__ 放到自己的 __init__ 当中, 这样子类就有了父类的 __init__ 的那些东西 super(ResNet50BasicBlock, self).__init__() # 这里只是定义部分, 在这里的定义并不一定会在推理过程中使用 self.conv1 = nn.Conv2d(in_channel, outs[0], kernel_size=kernerl_size[0], stride=stride[0], padding=padding[0]) self.bn1 = nn.BatchNorm2d(outs[0]) self.conv2 = nn.Conv2d(outs[0], outs[1], kernel_size=kernerl_size[1], stride=stride[0], padding=padding[1]) self.bn2 = nn.BatchNorm2d(outs[1]) self.conv3 = nn.Conv2d(outs[1], outs[2], kernel_size=kernerl_size[2], stride=stride[0], padding=padding[2]) self.bn3 = nn.BatchNorm2d(outs[2]) # 输入是啥看 forward(), 例如这里是 forward(self, x), 则表示输入是 x, 也就是一个 def forward(self, x): # nn.Conv2d 是卷积层, 请了解[1]什么是卷积层, 以及[2]卷积层是干啥用的, [3]卷积后会变成什么 # 卷积运算的目的是提取输入的不同特征, 第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级, 更多层的网路能从低级特征中迭代提取更复杂的特征 out = self.conv1(x) # [*] 什么是 ReLU, ReLU是激活函数, 请了解 [1]什么是激活函数, [2]为什么要使用激活函数 # [*] 什么是 Batch Normalization层, BN 层是批次归一化层 out = F.relu(self.bn1(out)) out = self.conv2(out) out = F.relu(self.bn2(out)) out = self.conv3(out) out = self.bn3(out) return F.relu(out + x)# 定义 ResNet50DownBlock类, 这里并不是完整的模型, 而是模型的一个部分class ResNet50DownBlock(nn.Module): def __init__(self, in_channel, outs, kernel_size, stride, padding): super(ResNet50DownBlock, self).__init__() self.conv1 = nn.Conv2d(in_channel, outs[0], kernel_size=kernel_size[0], stride=stride[0], padding=padding[0]) self.bn1 = nn.BatchNorm2d(outs[0]) self.conv2 = nn.Conv2d(outs[0], outs[1], kernel_size=kernel_size[1], stride=stride[1], padding=padding[1]) self.bn2 = nn.BatchNorm2d(outs[1]) self.conv3 = nn.Conv2d(outs[1], outs[2], kernel_size=kernel_size[2], stride=stride[2], padding=padding[2]) self.bn3 = nn.BatchNorm2d(outs[2]) self.extra = nn.Sequential( nn.Conv2d(in_channel, outs[2], kernel_size=1, stride=stride[3], padding=0), nn.BatchNorm2d(outs[2]) ) def forward(self, x): x_shortcut = self.extra(x) out = self.conv1(x) out = self.bn1(out) out = F.relu(out) out = self.conv2(out) out = self.bn2(out) out = F.relu(out) out = self.conv3(out) out = self.bn3(out) return F.relu(x_shortcut + out)class ResNet50(nn.Module): def __init__(self): super(ResNet50, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # Sequential 类是 torch.nn 模块中的一个容器, 可以将多个层封装在一个对象中, 方便顺序连接 self.layer1 = nn.Sequential( ResNet50DownBlock(64, outs=[64, 64, 256], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(256, outs=[64, 64, 256], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(256, outs=[64, 64, 256], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ) self.layer2 = nn.Sequential( ResNet50DownBlock(256, outs=[128, 128, 512], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50BasicBlock(512, outs=[128, 128, 512], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(512, outs=[128, 128, 512], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(512, outs=[128, 128, 512], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.layer3 = nn.Sequential( ResNet50DownBlock(512, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50BasicBlock(1024, outs=[256, 256, 1024], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(1024, outs=[256, 256, 1024], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.layer4 = nn.Sequential( ResNet50DownBlock(1024, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50DownBlock(2048, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(2048, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1, ceil_mode=False) # self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) self.fc = nn.Linear(2048, 10) # 使用卷积代替全连接 self.conv11 = nn.Conv2d(2048, 10, kernel_size=1, stride=1, padding=0) def forward(self, x): out = self.conv1(x) out = self.maxpool(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) # avgpool 平均池化层, 了解什么是平均池化层 out = self.avgpool(out) out = self.conv11(out) out = out.reshape(x.shape[0], -1) # out = self.fc(out) return out# 这里展现了对 ResNet 的一个具体的应用# x = torch.randn(1, 3, 224, 224) # 这个是我们 ResNet50 期待的输入样子, 可以看到他是 [1] 个 [3] 通道, 宽度为[224], 高度为 [224]的张量 image_path = './dataset/test/fake/test_fake_1.png'image = Image.open(image_path).convert('RGB') # 图片加载transform = transforms.ToTensor() # 将图片转化为张量, 此时的 张量的形状为[3, 1024, 1024]# 当输入数据的维度不足时, 我们可以通过 unsqueeze() 添加维度, 这个东西简单理解一下就是, 在某个维度外面加括号[], 即可拓展出更高的维度img_tensor = transform(image).unsqueeze(dim=0)# print(x.shape) 我们可以使用 shape 来查看一个张量的形状# print(img_tensor.shape)# 这里加载我们的网络架构net = ResNet50()# 这里进行输入, 输入 img_tensor, 进入 forword() 部分, 然后得到最终输出的结果out = net(img_tensor)print(out) 1234567891011121314151617181920212223242526import torchfrom torchvision import models# 这里为了方便, 我们直接加载 PyTorch 预训练好的 ResNet50 的模型# PyTorch 已经为我们提供了不少已经预训练好的模型, 我们只需要加载他们与训练好的模型即可# 但是我还是希望你可以掌握上面这种自定义模型的方法, 这样遇到 PyTorch 未提供的模型, 我们也可以尝试自己实现该模型model = models.resnet50(pretrained=True)# 冻结参数 : 即不更新模型的参数# 可以看到下面的代码, 这里表示冻结了所有层for param in model.parameters(): param.requires_grad = False# 但是我们可以通过替换层来接触某些层的冻结num_ftrs = model.fc.in_features # 这里是获取 ResNet50 的 fc 层的输入特征数model.fc = torch.nn.Linear(num_ftrs, 2) # 这里是对 fc 层进行修改, Linear(input_feather_num, output_feather_num) # 这里输入特征数是 num_ftrs, 输出特征数为 2 # 这一行很重要, 指定了模型的位置, cuda 可以理解为 GPU 设备, cuda: 0 表示使用编号为 0 的GPU进行训练# 当有多块 GPU 时, 可以用其他的方式指定 GPU# model = torch.nn.DataParallel(model, device_ids=[0, 1, 2]), 当然向我们这种小白(穷B), 当然还是单卡为主# 为了避免出现多卡的情况, 我在下面放入两篇博客, 有兴趣可以参考这两篇文章进行多卡训练# https://zhuanlan.zhihu.com/p/102697821# https://blog.csdn.net/qq_34243930/article/details/106695877device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model = model.to(device) 训练模型 + 验证模型 这里需要直接对模型进行训练 , 一般来说 , 在训练的过程中我们会加入 tqdm 库使得训练过程可视化 , 有时我们还会在训练过程中保存更好的训练结果 , 并且设置断点训练等操作 , 我只使用最简单的方式进行预测 train 部分的代码因人而异, 基本上每个人的写法都可能不同, 没有固定的写法 对于训练完的模型我们需要对其进行评价, 一般来说, 训练和验证都是放在一起的, 不可分开的 记得保存一下训练后的模型, 使用如下代码保存/加载整个模型123456# 保存模型model_path = &quot;xxxx.pth&quot; # xxxx 表示一个你喜欢的名字torch.save(model, model_path) # 使用 torch.save(model, model_path) 保存模型# 加载模型model = torch.load(model_path) # 使用 torch.load(model_path) 即可加载模型 完整的”训练模型 + 验证模型”代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from tqdm import tqdmfrom sklearn.metrics import f1_score, accuracy_score# 定义损失函数和优化器# 这里包含了 PyTorch 的 19 种损失函数 https://blog.csdn.net/qq_35988224/article/details/112911110criterion = torch.nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters())# 计算 F1 值和 准确率def evaluate(loader, model): preds = [] targets = [] loop = tqdm(loader, total=len(loader), leave=True) for images, labels in loop: images, labels = images.to(device), labels.to(device) with torch.no_grad(): outputs = model(images) _, predicted = torch.max(outputs, 1) preds.extend(predicted.cpu().numpy()) targets.extend(labels.cpu().numpy()) # Update the progress bar loop.set_description(&quot;Evaluating&quot;) return f1_score(targets, preds), accuracy_score(targets, preds)# 训练循环best_f1 = 0.0loss_values = []num_epochs = 10 # 定义训练的轮次for epoch in range(num_epochs): model.train() # 将模型设置为训练模式 loop = tqdm(train_loader, total=len(train_loader), leave=True) print(loop) for images, labels in loop: images, labels = images.to(device), labels.to(device) # 前向推理 outputs = model(images) loss = criterion(outputs, labels) # 反向传播及优化 # 在用 PyTorch训练模型时, 通常会在遍历 Epochs 的过程中依次用到 # optimizer.zero_grad() : 先将梯度归零 # loss.backward() : 反向传播计算得到每个参数的梯度值 # optimizer.step() : 通过梯度下降执行一步参数更新 # 对于这三个函数, 这篇博客写的很好 : https://blog.csdn.net/PanYHHH/article/details/107361827 # 可以简单阅读一遍 optimizer.zero_grad() loss.backward() optimizer.step() # 保存该批次的损失 loss_values.append(loss.item()) # 更新进度条 loop.set_description(f&quot;Epoch [{epoch + 1}/{num_epochs}]&quot;) loop.set_postfix(loss=loss.item()) # 在每轮之后验证模型 model.eval() # 将模型设置为推理模式, 此时模型中的参数不会进行更新, 即完全用于推理/验证 f1_value, accuracy = evaluate(valid_loader, model) print(f'F1 score: {f1_value:.4f}, Accuracy: {accuracy:.4f}') # 保存 F1 值最高的模型 if f1_value &gt; best_f1: best_f1 = f1_value # 这里和上面 Markdown 的保存方式不同, model.state_dict(), 表示模型的参数, 简单来说呢我们仅仅保存了模型的参数, 但是我们并没有保存模型的结构 # 上面 Markdown 的保存方式是即保存了整个模型的结构, 也保存了模型的参数 torch.save(model.state_dict(), 'best_model.pth')print('训练结束') 当然我们也可以使用绘图函数，来展示过程中的相关数据。 12345678910import matplotlib.pyplot as pltplt.figure(figsize=(12, 8))plt.plot(loss_values, label='Train Loss')plt.title('Loss values over epochs')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.grid(True)plt.show() 推理模型 很高兴, 如果你到这一步, 你的水平肯定已经有了质的飞跃, 这里已经是最后一步了, 结束这个部分, 你就要开始自己的探索之路了 推理模型很简单, 我在上面说过, 构造模型时指定什么输入 , 推理的时候就要指定什么输入, 这里就是对应的部分了 1234567891011121314151617181920212223242526272829303132from torchvision.transforms import ToTensor, Resize, Normalize# predict_by_file 表示推理一个文件, 我们需要传入文件路径以及模型def predict_by_file(file_path, model): # image = Image.open(file_path).convert('RGB') # 这里的 transform 有与没有都无所谓, 纯看心情 transform = transforms.Compose([ Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) image = transform(image) # 这里和上面一样, 表示在最外面加一层括号, 使 [3, 256, 256] 变为 [1, 3, 256, 256] image = image.unsqueeze(0).to(device) model.eval() with torch.no_grad(): outputs = model(image) # 模型推理 # torch.max(...) # input (Tensor) – 输入张量 # dim (int) – 指定的维度 _, predicted = torch.max(outputs, 1) # 返回指定维度的最大值, 其实这里只有一维 print(outputs) # tensor([[0.7360, 0.2668]], device='cuda:0') print(outputs.shape) # torch.Size([1, 2]) return &quot;Fake&quot; if predicted.item() == 0 else &quot;Real&quot;path = './dataset/test/real/test_real_7.jpg'print(predict_by_file(path, model))","link":"/2024/03/20/PyTorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"title":"Git常用指令","text":"[TOC] Git 常用命令 初始化仓库:git init 查看当前仓库的状态git status 切换分支 切换到已有 branchName 分支git checkout branchName 创建新分支，创建的同时切换到该分支git checkout -b newBranch 查看所在目录的分支git branch -a 拉取请求git pull 从 Gitee 拉取请求git pull 上传到远程仓库(GitHub) 在 GitHub 配置 SSH_KEY 查看本地是否具有已存在 ssh_key : ll ~/.ssh 如果不存在则生成 ssh_key : ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot; 如果存在则复制 ssh_key 的内容 : cat ~/.ssh/id_rsa.pub 在 GitHub 上添加公钥 : Settings → SSH and GPG Keys → New SSH Key 验证是否成功 : ssh -T git@github.com 克隆远程仓库 : git clone &lt;SSH_Addr&gt; 添加远程仓库地址:git remote add origin &lt;git_addr&gt; 添加文件 git add &lt;files&gt; 添加注释 git commit -m &quot;f&quot; 推送请求 git push -u origin main Git 常见错误解决方案Failed To Push Some Refs To … 参考链接问题原因：远程库与本地库不一致，在 hint 中也有提示把远程库同步到本地库就可以了解决办法：使用命令行： 1git pull --rebase origin main","link":"/2024/03/26/Git%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/03/17/hello-world/"},{"title":"VSCode C++ 的安装和配置","text":"VSCode C++ 的安装和配置 下载 VSCodeVSCode的安装地址 : https://code.visualstudio.com/ 安装和配置 Remote SSH 连接远程的 Linux 服务器 在 VSCode 安装插件 Remote-SSH 点击 左下角→连接到主机→配置SSH主机→选择要更新的SSH的配置文件(C:\\Users\\UserName\\.ssh\\config) 配置文件123Host 服务器的 IP 地址HostName 服务器的 IP 地址User 用户名 连接远程服务器, 按照要求选择操作系统, 输入用户名和密码等即可(第一次连接会较慢) 配置免密登录 Windows 系统下查看是否具有 ssh-key : cd C:/Users/UserName/.ssh 如果没有 ssh-key 则生成 ssh-key : ssh-keygen -t rsa -b 4096 如果有 ssh-key 则上传至 Windows : scp id_rsa.pub UserName@IPAddr:FilePath 在 Linux 系统下进行签名: cat pubPath &gt;&gt; authorized_keys","link":"/2024/03/27/VSCode-C-%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"},{"title":"在 Hadoop 高可用的基础上搭建 HBase 高可用","text":"在 Hadoop 高可用的基础上搭建 HBase 高可用当Hadoop高可用搭建完成后，需要进一步再Hadoop高可用集群上搭建HBase高可用时，过程如下： 前提说明： 我的集群为三台机器，每台机器上都有ZooKeeper，使用用户名和主机名（Username@Hostname）分别如下： 123master@mastermaster@slaver01master@slaver02 首先保证 ZooKeeper 正常部署 1zkServer.sh start 需要保证Hadoop正常部署 12[master@master ~]$ start-dfs.sh[master@slaver01 ~]$ start-yarn.sh 解压HBase 配置环境变量 生效环境变量 修改配置文件 修改 hbase-site.xml 文件 123456789101112131415161718192021222324252627282930313233343536&lt;configuration&gt; &lt;!-- HBase数据在HDFS中的存放的路径 --&gt; &lt;!-- 这里 ns 是 Hadoop 的 nameservice的值, 指向的是一个高可用的通道 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase 的运行模式 --&gt; &lt;!-- false是单机模式, 若为 false, HBase 和 ZooKeeper 会运行在同一个 JVM 里面 --&gt; &lt;!-- true是分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slaver01,slaver02&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper快照的存储位置 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.6/data&lt;/value&gt; &lt;/property&gt; &lt;!-- V2.1版本，在伪分布式情况下, 设置为 false --&gt; &lt;!-- 当使用 hdfs 时, 设置为 true --&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 需要注意的是： 这里使用了ns这个高可用通道，因此需要将Hadoop的 core-site.xml 与 hdfs-site.xml移动到/opt/module/hbase-2.1.0/conf下 修改 regionserver 文件 123masterslaver01slaver02","link":"/2024/05/04/Hadoop2HBase/"},{"title":"Hash","text":"哈希(Hash)[TOC] 哈希的基本概念 哈希（Hash）在我的理解中是一种映射关系，例如，将学生映射到学号上、将口红的颜色映射到一个色号上。 哈希函数（Hash Function）就是将一种你想要查询的关键字，比如说姓名、手机号码、数字或者字符串等，映射成便于查找的东西(一般来说是一个数字)的函数。 一般而言，一个好的哈希函数可以帮我们将我们想要查找的东西，从一个较大集合映射到一个较小集合，然后我们可以将这个较小集合中的信息存储下来，例如存入一个数组，这个用于存储较小集合的数组就称之为哈希表。 一张格式如下的表： 学号 姓名 0001 张三 0002 李四 0003 王五 0004 赵六 … … 这张表就可以理解为一个姓名和学号的哈希表，我们通过学号就可以获得学号对应的人的姓名，即：学号[0001] -&gt; &quot;张三&quot;，反映到代码中，可以理解为一个一维数组通过下标直接访问。 而在一些加密工作中，可能需要将要简单的组合复杂化，比如密码组合，这时会有一种类似反向哈希（加密）的过程。 比较常见的哈希函数的例子：$$H(x) = x ; mod ; 11$$ 这个函数可以让我们将任意的整数映射到 0 ~ 10 的范围中 哈希的基本操作 定义哈希函数 1234567const int modnum = 11;int hashTable[modnum];// 定义哈希函数int hash(int x) { return x % modnum;} 在哈希表中插入元素 12345// 在哈希表中插入元素void insert(int x) { int addr = hash(x); hashTable[addr] = x;} 在哈希表中查找元素 12345// 在哈希表中查找元素bool isExist(int x) { int addr = hash(x); return hashTable[addr] == x;} 哈希表中解决冲突的方式不同关键字通过相同哈希函数计算出相同的哈希地址，该种现象称为哈希冲突或哈希碰撞，或者称为哈希冲突。哈希表在存放数据时有可能出现冲突的情况，以上文中的哈希函数为例我们分别向其中插入元素，如下：因为希望哈希表底层数组的容量小于实际要存储的关键字的数量，这就会导致一个问题：冲突的发生是必然的，我们能做的是尽量的降低冲突率。 冲突的解决方式一般有以下两种： 方式 1：顺延一种很好想到的解决方式是将哈希表中插入时产生冲突的元素向后顺延，直到找到一个空位置再进行插入。这种方式称为顺延，也有说法称之为线性探测。此时插入和查找的代码也要发生相应的改变，插入时需要我们需要找到一个空位置来执行插入操作；相对应的查找方式也要做出改变，当我们查询一个数时，也要查询哈希函数对应的位置，并依次比较连续的非空的哈希表中的值： 插入操作 1234567void insert(int x) { int addr = hash(x); while(hashTable[addr] NOT NULL) { // 当哈希表的插入位置不为空 addr = (addr + 1) % modnum; } hashTable[addr] = x;} 查找操作 12345678910void isExist(int x) { int addr = hash(x); while(hashTable[addr] NOT NULL) { // 当哈希表的查询位置不为空(查询一段连续的哈希表) if(hashTable[addr] == x) { // 如果查询到指定元素, 返回 true return true; } addr = (addr + 1) % modnum; } return false;} 可以定义多种解决冲突的顺延函数，即addr = (addr + 1) % modnum，实际使用中可以是每次$+k$，或者$+1^2$，$+2^2$，$+3^2$，…等。 但是这种顺延方式会存在一定的问题：插入时可能会出现多次冲突，当哈希表即将满的时候，插入操作的冲突可能会出现的更多，此时插入和查询操作都会变成一个非常耗时的操作。 方式 2：哈希链表我们可以通过链表的方式，来实现在一个位置放置多个元素的操作。在 C++ 的 STL 库中，我们可以使用 STL 库中提供的 vector 来简单的替代链表。通过这种方式，每次查找元素时，先找到对应的链表头，然后遍历这个位置的整张链表即可。 此时的哈希表的定义、插入操作和查询操作要发生相应的变化： 定义哈希函数 12345678910#include &lt;iostream&gt;#include &lt;vector&gt;const int modnum = 11;vector&lt;int&gt; hashTable[modnum];// 定义哈希函数int hash(int x) { return x % modnum;} 插入操作 1234void insert(int x) { int addr = hash(x); hashTable[addr].push_back(x);} 查询操作 12345678910111213void isExist(int x) { int addr = hash(x); int tableSize = hashTable[addr].size(); // 这里不使用 for(int i = 0; i &lt; hashTable[addr].size(); i++) 的写法, 而是首先计算出 hashTable[addr].size() // 因为 vector 的 size() 是一个比较耗时的操作, 他是通过将 vector 按照一个一个数出来的方式来进行计数的 // 在数据量小的时候可能并不明显, 当数据量大的时候可能就会出现较为严重的耗时问题 for(int i = 0; i &lt; tableSize; i++) { if(hashTable[addr][i] == x) { return true; } } return false;} 但是这种方式还是不能彻底解决我们的问题。对于插入操作来说，时间复杂度可以看作是$O(1)$，对于查询操作来说，时间复杂度和其冲突次数相关联。 哈希函数的设计面对上面的问题，设计好哈希函数才是解决问题的关键。哈希函数在设计的时候，一般要注意几个原则： 设计哈希函数时，我们需要尽可能让查询的值均匀地存储在哈希表中，或者说尽量分散再哈希表中。 在手搓哈希函数时，我们会要求 $H(x) = x ; mod; p$，其中的 $p$ 为素数 哈希函数中的对 $p$ 取摸的操作，会使得哈希值落在 $0 &lt;= value &lt;= p-1$ 的范围内，这个哈希表的长度 $p$，一般被称为哈希表的容量（Capacity）。 插入哈希表的元素总数除以哈希表的容量得到的数，称为负载因子，这里可以用 $\\alpha$ 表示，即：$$\\alpha = ElumNum \\div p$$ 当负载因子$\\alpha$达到一定程度时（一般认为是$0.7\\sim0.8$），则说明再对哈希表继续进行操作，就会面临大量冲突的情况，这时就要考虑增大哈希表的容量以避免出现更多的冲突。 哈希函数的冲突率和负载因子的关系一般如下： 字符串哈希字符串哈希是学习或者工作中经常遇到的一种操作，常用于比较两组字符串的内容等操作。通过比较两个字符串的哈希值就可以完成字符串的比较。当然这个操作也可以用于数组的哈希，因为字符串本质就是一个字符数组。$$s = s_1s_2s_3 \\dots s_n\\qquad s_i \\in a, b \\dots z$$ 一个字符串 $s$ 由 $n$ 个字符组成，每个字符 $s_i$ 属于 $a \\sim z$。其哈希函数为：$$\\begin{aligned}H(S)&amp;=(\\sum_{i=1}^{n} c_i × base^{n-i});mod ;p\\&amp;=(c_1 × base ^ {n-1} + c_2 × base ^ {n-2} + \\dots + c_{n-1} × base ^ {1}) ; mod ; p\\&amp;=base^{n-(n-1)}(base\\dots(base(base × c_1 + c_2)));mod ;p\\&amp;=base^{1}(base\\dots(base(base × c_1 + c_2)+c_3)+\\dots + c_n)\\end{aligned};mod ;p$$ 其中 $c_i$ 是一个和 $s_i$ 有关的数字，我们可以将字符映射到数字，例如：$a → 1$、$b → 2$ 等。这里不将 a 映射为 0，因为如果将 a 映射为 0，字符串 a 和 ab 的哈希值是相等的。$base$ 是一个可以自己指定的数字，其值一般是大于字符集中的字符数量（$c_i$的最大值）的素数，这里可以取 31，常见的选择是 9999971。$p$ 是一个素数，常见的选择是 101 或 137。 用代码实现为： 12345678int hash(char s[], int n) { int res = 0; for(int i = 0; i &lt; n; i++) { // 为什么是 res * base, 见上文的描述的公式推导 res = (res * base + (s[i] - 'a' + 1)) % p; } return res;} 当 $base$ 为 31，$p$ 为 101 时。当 $s$ 为 $a$ 时，hash(char s[], int n) 的值为 1。过程可以描述如下： 12[1] res = (0 * base + ('a' - 'a' + 1)) % p&gt;&gt;&gt; res = 1 当 $s$ 为 $ab$ 时，hash(char s[], int n) 的值为 1。过程可以描述如下： 1234[1] res = (0 * base + ('a' - 'a' + 1)) % p&gt;&gt;&gt; res = 1[2] res = (1 * base + ('b' - 'a' + 1)) % p&gt;&gt;&gt; res = 33 当我们定义好一个良好的哈希函数之后，因为哈希函数的值相等的概率比较小，当两个字符串的哈希值相同的时候，我们可以认为两个字符串也相同，从而避免使用按位比较，节约时间。 $base$ 为什么一般是一个大于字符集中的字符数量（$c_i$的最大值）的素数呢？当 $base$ 值过小时，此处假定为 $11$，会出现有可能两个字符运算完后还没有超过字母表的范围。如：字母表的范围为 $1 \\sim 26$，字符串 ba 运算完成后的结果为 $23$，两个字符串的运算结果小于 $26$，字符串w运算完成后的结果为 $23$，可以发现 ba 和 w 出现了冲突。 我们字符串的定义方式有一种好处，可以使得我们借助类似前缀和的思想，快速得到任意一段字符串 $s$ 的字串的哈希值。 我们可以使用一个数组 $a$ 来记录我们计算 hash(s) 的中间过程，即：1234a[1] = hash(c1)a[2] = hash(c1c2)...a[i] = hash(c1c2...ci) 那么字符串 $s$ 的任意子串 $s_{l, r} = s_ls_{l+1}\\dots s_r$ 的哈希值为：$$hash(s_{l, r}) = (a[r] - a[l-1] × base^{r - l + 1});mod;p$$ 双哈希冲突往往是不可避免的，纵使我们的哈希函数再好，也有可能会出现冲突的情况，那么我们如何尽可能避免这个问题呢？这时候，我们可以使用双哈希的方法来避免冲突。在上文字符串哈希中，我们的字符串哈希函数为： $$H(S) = (\\sum_{i=1}^{n} c_i × base^{n-i});mod ;p$$ 那么如何实现双哈希呢？我们可以选取两组 $base$ 和 $p$，然后使用这分别使用这两组 $base$ 和 $p$，分别来求取哈希值。如果这两组参数得到的哈希值都相同，我们则认为两个字符串相等。在竞赛中，最好书写双哈希，甚至三哈希，因为这样出错误的概率会呈指数级的降低。 STL中的哈希 unordered_map在 C++ 的STL库中，有一个非常好用的数据结构 unordered_map，可以帮助我们实现大多数哈希表的操作。 1unordered_map&lt;K, V&gt; hash_table; 其中 K 为我们想要存储的关键字信息，V 表示与他关联的一些信息。例如： 1unordered_map&lt;string, int&gt; hash_table; 这个表会以string作为K，int作为V进行存储，基于此我们可以很方便的实现一些功能。例如我们现在要保存一个班级上学生的年龄信息: 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;int query(string name) { if(hash_table.find(name) == hash_table.end()) { cout &lt;&lt; &quot;Cannot Find...&quot; &lt;&lt; endl; } return hash_table[name];}int main(void) { unordered_map&lt;string, int&gt; hash_table; hash_table[&quot;zhangsan&quot;] = 2000; hash_table[&quot;lisi&quot;] = 2001; query(&quot;zhangsan&quot;); return 0;}","link":"/2024/05/05/Hash/"},{"title":"栈(Stack)","text":"栈(Stack)栈的基本概念 栈是一种 先进后出(First in Last Out, FILO) 的数据结构，其类似于我们生活中的衣娄，衣服只能从最顶部放入，也只能从最顶部拿出，要想拿出中间的某件衣服，就需要将顶部的衣服全部拿出，再进行后续的操作。 衣娄对应到栈上，就是以下的概念： 栈(Stack) 是一种类似于衣娄的数据结构，我们可以向其内部存入或者取出数据 栈按照 先进后出 的原则存储数据，每次新进入的数据都会被放在最上面，越先进入的越靠下，越后进入的数据越靠上。 我们只能对最上面的数据进行操作 栈的两大元素：栈的大小和栈顶指针Top（该指针指向栈最顶部的位置） 栈的基本操作 新建栈(题目简单时可以用数组模拟栈) 插入数据 删除栈顶数据 查询栈顶数据 清空栈","link":"/2024/05/04/Stack/"},{"title":"复习数据结构&#x2F;算法清单","text":"复习数据结构清单有日期则表示在该日期已经复习完毕，或者表示复习后的最新一次更新 数据结构 链接 备用链接(Backup Link) 日期 栈(Stack) - - - 队列(Queue) - - - 链表(Link List) - - - 二叉树(Binary Tree) https://hello-nilera.com/2024/05/07/Tree/ https://blog.csdn.net/NilEra/article/details/138624929?spm=1001.2014.3001.5501 2024.05.09 哈希(Hash) - - - 算法复习清单 算法 链接 备用链接(Backup Link) 日期","link":"/2024/05/04/ReviewAlgorithm/"},{"title":"Heap","text":"堆（Heap）[TOC] 堆的基本概念二叉堆的基本概念和基本性质堆是一种树形结构，有二叉树就有二叉堆。 二叉堆总是一棵完全二叉树 堆中某个结点的值总是不大于或不小于其父结点的值 根节点的值为整个堆中的最小/最大值。 父节点中的值大于等于两个子节点中的值，根节点的值最大的堆称为大根堆。 父节点中的值小于等于两个子节点中的值，根节点的值最小的堆称为小根堆。 我们可以在堆中插入和删除元素，然后通过调整元素的位置来维护堆的性质。 堆的操作 堆的初始化在建立堆之前，需要初始化一些东西： 一个空数组，用于存储堆中的元素 一个记录堆中元素个数的变量123const int MAXSIZE = 10000;int len = 0;int heap[MAXSIZE + 1]; 堆中元素的插入堆在插入时，需要首先将插入元素放在数组末尾，然后插入元素不断的和其父节点比较，直到位置合适。下面是对小根堆插入过程的模拟：小根堆插入的代码如下： 12345678910111213141516void insert(int x) { heap[++len] = x; up(len);}void up(int k) { while(k &gt; 1 &amp;&amp; heap[k] &lt; heap[k/2]) { swap(heap[k], heap[k/2]); k /= 2; }}int main(void) { insert(x); return 0;} 大根堆和小根堆插入元素的方式基本相同，只需要改变大于/小于符号，插入操作的时间复杂度为 $O(log;n)$ 堆顶元素的删除堆最常用的功能就是维护最小/最大值。在使用小根堆时，我们经常会求得最小的数字，然后让它出堆，这时就要从堆中删除堆顶数据。这时除了堆顶为空，它的左子树堆和右子树堆仍满堆结构。为了操作简单，一般选择将堆尾部元素放到堆顶，然后将其逐步下移的方式，下移时，如进行交换操作，交换的是该节点左右儿子中较小的一个与该节点。下图模拟小根堆删除堆顶元素的操作：小根堆删除元素的代码如下： 123456789101112131415void pop(void) { swap(heap[1], heap[len]); len--; down(1);}void down(void) { while(k * 2 &lt;= len) { int j = k * 2; if(k*2+1 &lt; len &amp;&amp; heap[j+1] &lt; heap[j]) j++; if(heap[k] &lt;= heap[j]) break; swap(heap[k], heap[j]); k = j; }} 堆中任意位置的删除删除堆中的任意一个元素时，我们可以发现这个时候这个元素下的左子树堆和右子树堆也满足堆结构，但是我们不可以像删除堆顶节点一样，和数组尾部元素互换，然后尝试下移，原因如下：此时无需向下调整，因为 $5&lt;8$ 且 $5&lt;9$，依旧满足小根堆的性质，但是其父节点 $6&gt;5$，破坏了小根堆的性质，因此此时需要上移。所以我们删除堆中的任意一个元素，跟数组尾元素互换时，不仅要考虑下移，还有可能会上移。小根堆中删除一个位于数组位置 pos 的元素的代码如下:1234567891011121314151617void deleteElem(int pos) { if (pos == len) { heap[len] = 0; len--; return; } int x = heap[pos], y = heap[len]; swap(heap[pos], heap[len]); len--; if (y &lt; x) { // 堆尾的数比原数大, 尝试上移 up(pos); } else { // 堆尾的数比原数小, 尝试下移 down(pos); }} STL 中的 priority_queue(优先队列)STL 库中的 priority_queue 是一个很类似于堆的结构，它包含如下操作： empty - 判断是否为空 size - 返回队列内元素个数 top - 访问队首元素 push - 往队列中插入一个元素 pop - 弹出队首元素 这里的 priority_queue 相当于堆，队首元素相当于堆顶元素。我们可以使用如下语句创建一个小根堆： 1priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 这段C++语句创建了一个优先队列 q，其中元素类型为 int，底层容器使用 vector&lt;int&gt;，并且使用 greater&lt;int&gt; 作为比较器。在优先队列中，当元素被插入队列时，会根据比较器的规则进行排序，从而实现堆的性质。 在这段语句中，greater&lt;int&gt; 是一个函数对象，代表使用“大于”运算符进行比较。因此，当要创建一个小根堆时，即希望队列中的元素按照从小到大的顺序排列，可以利用 greater 作为比较器，这样队列中的最小元素将位于队首。同样的，我们可以使用如下语句创建一个大根堆： 1234// 写法 1：priority_queue&lt;int&gt; q;// 写法 2：priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt; q; 当使用 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 创建一个小根堆后，可以按以下方式操作这个小根堆： 插入元素：使用 push() 方法将元素插入小根堆。 123q.push(5); // 插入元素5q.push(3); // 插入元素3q.push(7); // 插入元素7 获取堆顶元素：使用 top() 方法获取小根堆的头部元素。 12int topElement = q.top(); // 获取小根堆的头部元素cout &lt;&lt; &quot;Top element of the min heap: &quot; &lt;&lt; topElement &lt;&lt; endl; 删除堆顶元素：使用 pop() 方法删除小根堆顶部的元素。 1q.pop(); // 删除小根堆的头部元素 查看堆是否为空：使用 empty() 方法检查小根堆是否为空。 12345if (q.empty()) { cout &lt;&lt; &quot;Min heap is empty&quot; &lt;&lt; endl;} else { cout &lt;&lt; &quot;Min heap is not empty&quot; &lt;&lt; endl;}","link":"/2024/05/07/Heap/"},{"title":"Tree","text":"树(Tree)[TOC] 树的基本概念树是一种非常重要的非线性数据结构，树的一个节点可能会生出多个分支。一般而言，一棵树会包含一个根节点，向下延伸出若干子节点，每个末端的节点被称为叶子节点。 有根树有根树存在一个根节点Root，如下：对于图中概念的一些补充： 节点拥有的子节点个数叫做节点的度。 具有相同深度的节点处于同一层，方便表示。 节点和节点之间的线叫做边。 路径：指从树上一点到另外一点所经过的不重合的点和边的集合，题目中有时会单指点或边的集合。 一颗 $n$ 个节点的树，一定有 $n-1$ 条边 无根树 二叉树二叉树是一种特殊的树。 所有节点的度都不超过2的树称为二叉树。 因为每个二叉树的节点最多只会有两个子结点，它的两个子节点一般会被称为左、右儿子，两棵子树一般会被称为左、右子树。 左、右儿子甚至根节点本身都有可能缺失（一个节点都没有可以称为空二叉树）。 满二叉树和完全二叉树二叉树也有两个比较特殊的类型：满二叉树和完全二叉树。 满二叉树：所有层的节点全满。 满二叉树的一些规律 第 $n$ 层的节点个数为 $2^{n-1}$ 深度为 $n$ 的满二叉树节点数为 $2^0 + 2^1 + 2^2 + \\dots + 2^{n-1}= 2^n-1$ 完全二叉树：除了最后一层以外，其他层的节点个数全满，而且最后一层的节点从左到右排满直到最后一个节点。 完全二叉树的一些规律 完全二叉树的节点个数不会少于 $(2^{n-1}-1)+1 = 2^{n-1}$ 完全二叉树的节点个数不会多于 $2^{n} - 1$ 一棵完全二叉树，设当前节点为 $t$，其父节点为 $t/2$，其左儿子为 $2t$，其右儿子为 $2t+1$，借助该规律，我们可以将完全二叉树使用数组进行存储。 完全二叉树的存储 完全二叉树由于它的特性，可以简单用数组来模拟其结构 一般会以数组$[1]$位置为根节点建立二叉树 数组$[t]$位置的左儿子和右儿子对应的位置分别为$[2t]$和$[2t+1]$，父节点的位置为$[t/2]$。 堆、线段树等数据结构的建立也会参考这个方式 完全二叉树的建立（使用数组），使用这种方法建立非完全二叉树，会导致空间的浪费： 12345678void build(int t) { // 添加数据 UpdateData(t); // 如果子节点存在 Build(2 * t); Build(2 * t + 1);} 为了解决这个问题，我们可以使用其他方法来完成一般二叉树的存储，可以用数组下标模拟节点编号，用多个数组来记录节点信息。为了方便，我们也可以使用结构体来存储这些信息： 12345// 使用结构体来实现上述操作struct TreeNode { int value; int l, r, fa;}a[100010]; 当然，作为一种树形结构，使用指针显然是更合适的方法： 123456789// 使用指针来实现上述操作struct TreeNode { int value; TreeNode* l; TreeNode* r; TreeNode* fa;};TreeNode* root; 使用指针的一些操作： 新建节点： 1234567struct TreeNode { int value; TreeNode *l, *r, *fa; // 初始为 NULL TreeNode(int x){ value = x; }};TreeNode* treeNode = new TreeNode(x); 根节点初始化： 12TreeNode* root;root = new TreeNode(v); 插入节点： 1234567891011void Insert(TreeNode* fa, TreeNode* p, int flag){ // flag = 0 插入到左边 // flag = 1 插入到右边 if (!flag) fa-&gt;l = p; else fa-&gt;r = p; p-&gt;fa = fa;}TreeNode* treeNode = new TreeNode(v);Insert(fa, treeNode, flag); 删除节点 1// 删除节点 二叉树的遍历二叉树的遍历可分为先序遍历、中序遍历和后序遍历，这三种方式以访问根节点的时间来区分。先序遍历（Degree-Left-Right, DLR）：根→左→右中序遍历（Left-Degree-Right, LDR）：左→根→右先序遍历（Left-Right-Degree, LRD）：左→右→根 在该图中，先序遍历的结果为 1 2 4 5 3 6 7，先序遍历代码如下： 1234567void preOrder(TreeNode* treeNode) { cout &lt;&lt; p-&gt;value &lt;&lt; endl; if(treeNode-&gt;l) preOrder(treeNode-&gt;l); if(treeNode-&gt;r) preOrder(treeNode-&gt;r);}preOrder(root); 在该图中，中序遍历的结果为 4 2 5 1 6 3 7，中序遍历代码如下： 1234567void inOrder(TreeNode* treeNode) { if(treeNode-&gt;l) inOrder(treeNode-&gt;l); cout &lt;&lt; p-&gt;value &lt;&lt; endl; if(treeNode-&gt;r) inOrder(treeNode-&gt;r);}inOrder(root); 在该图中，后序遍历的结果为 4 5 2 6 7 3 1，后序遍历代码如下： 1234567void postOrder(TreeNode* treeNode) { if(treeNode-&gt;l) postOrder(treeNode-&gt;l); if(treeNode-&gt;r) postOrder(treeNode-&gt;r); cout &lt;&lt; p-&gt;value &lt;&lt; endl;}postOrder(root); 除了上述的几种遍历方式，还有层级遍历（BFS）方式对树进行遍历。层级遍历是借助队列（Queue）来实现的，其过程可以描述如下： 层级遍历的代码如下： 1234567891011121314TreeNode* q[N];void bfs(TreeNode* root) { int front = 1, rear = 1; q[1] = root; while (front &lt;= rear) { TreeNode* p = q[front]; // 选取队列中最前面的节点 front++; cout &lt;&lt; p-&gt;value &lt;&lt;endl; if(p-&gt;l) q[++rear] = p-&gt;l; if(p-&gt;r) q[++rear] = p-&gt;r; }}bfs(root); 计算节点的深度我们可以在遍历树的时候同时进行节点深度的记录，简单来讲就是：$$depth_{儿子} = depth_{父亲} + 1$$ 有根树(Tree)这里不再是二叉树这种特殊的树，而是一般意义的树。 树的存储方式 vector/链表 123456789101112131415161718// vector 方式vector&lt;int&gt; nodes[N + 1];int n, father[N + 1];// 在 x 和 y之间构建一条边void addEdge(int x, int y) { nodes[x].push_back(y);}// 遍历 x 的所有儿子int l = nodes[x].size();for (int i = 0; i &lt; l; i++) { nodes[x][i];}for (auto i: nodes[x]) { ...} 123456789101112131415161718// 链表方式struct Node { int where; Node *next;} *head[N + 1], a[M];int n, father[N + 1], l = 0;void addEdge(int x, int y) { a[++i].where = y; a[l].next = head[x]; head[x] = &amp;a[l];}// 遍历 x 的所有儿子for (Node* p = head[x]; p; p-&gt;next) { p-&gt;where;} 有根树遍历遍历一棵树一般有 DFS 和 BFS 两种方式。DFS：深度优先搜索，从一个节点开始，选择一条路径并走到底，并通过回溯来访问所有节点。BFS：广度优先搜索，也称层级顺序探索，从一个节点开始，遍历该节点的所有子节点，或称按照深度从小到大的顺序依次遍历所有点。 有根树的DFS序有根树的 DFS 序是指，从根节点开始的深度优先搜索过程中，依次记录的点所生成的序列。对于上图，所生成的 DFS 序即为 ABCDEFGHIJKLMN。当然这个只是其中一种 DFS 序，因为 A 可以走向 B，也可以走向 E，当然也可以走向 F。不同的走向会有不同的 DFS 序。 1234567891011vector&lt;int&gt; dfn; // 用于存储 DFS 序, 常用 DFN 表示 DFS 序 // dfn 中的元素即为 DFS 序void dfs(int x) { dfn.push_back(x); // for x的所有儿子y { dfs(y); } // for (Node* p = x; p; p-&gt;next){ dfs(p-&gt;next) }}dfs(root); 有根树的BFS序有根树的 BFS 序是指，从根节点开始的广度优先搜索过程中，依次记录的点所生成的序列。对于上图，所生成的 BFS 序即为 ABENCDFMGJHIKL。当然这个只是其中一种 BFS 序，因为同一深度可能会有不同的遍历顺序，如深度为 $2$ 时，BEN、BNE、EBN、…都是可能出现的顺序，不同的顺序会有不同的 BFS 序。 12345678910111213void bfs(int root) { // 将 root 加入队列 q; q.push(root); // 遍历队列 q while(队列 q 非空) { x = q.top(); // 取队首元素 q.pop(); // x 出队 for x的所有儿子y { y 入队; } }} 无根树(Unrooted Tree)无根树即没有固定根结点的树，树中的节点只有相邻关系而没有父子关系。无根树有几种等价的形式化定义（建议搭配图论一起学习）： 有 $n$ 个结点， $n−1$ 条边的连通无向图 无向无环的连通图 任意两个结点之间有且仅有一条简单路径的无向图 任何边均为桥的连通图 没有圈，且在任意不同两点间添加一条边之后所得图含唯一的一个圈的图 如下图所示，即一棵无根树：无根树中的任意一个节点可以被指定为根，变成一棵有根树。 无根树的遍历遍历一棵无根树一般也有 DFS 和 BFS 两种方式。遍历无根树时，可以从任意一个节点开始，以类似有根树的方式，遍历整棵树。唯一的区别是在进入一个新节点时，需要记录这个节点的来源节点，在遍历新节点的相邻节点时，避免重复访问来源节点即可。 无根树的 DFS 123456789void dfs(int from, int x) { for x的所有响铃节点y { if (y != from) { dfs(x, y); } }}dfs(-1, x); 无根树的 BFS 12345678910111213141516void bfs(int x) { // 将 x 加入队列 q，x 的来源为空 while (队列 q 非空) { x = q.top(); from = x的来源节点; q.pop; for x的所有相邻节点 y { if (y != from) { y 入队; 记录 y 的来源节点为 x; } } }}bfs(x); 树的直径 树的直径是指树上任意两个节点之间最长（路径的长度一般指的是路径经过的边的数量）的路径。 一棵树可以存在很多条直径，他们的长度相等。 树的直径的中间节点被称为树的中心（图中C节点），如果直径上有偶数个节点，那么中间的两个节点都可以是树的中心。 树的中心到其它点的最长路径最短。","link":"/2024/05/07/Tree/"},{"title":"BELKA_2024","text":"[TOC] Leash Bio - Predict New Medicines with BELKA用 BELKA 预测新药 Predict small molecule-protein interactions using the Big Encoded Library for Chemical Assessment (BELKA) 使用化学评估大编码库（BELKA）预测小分子蛋白质相互作用 OverviewIn this competition, you’ll develop machine learning (ML) models to predict the binding affinity of small molecules to specific protein targets – a critical step in drug development for the pharmaceutical industry that would pave the way for more accurate drug discovery. You’ll help predict which drug-like small molecules (chemicals) will bind to three possible protein targets. 在这场比赛中，你将开发机器学习（ML）模型来预测小分子与特定蛋白质靶标（目标蛋白）的结合亲和力——这是制药行业药物开发的关键一步，将为更准确的药物发现铺平道路。你将帮助预测哪种药物样的小分子（化学物质）将与三种可能的蛋白质靶点结合。 DescriptionSmall molecule drugs are chemicals that interact with cellular protein machinery and affect the functions of this machinery in some way. Often, drugs are meant to inhibit the activity of single protein targets, and those targets are thought to be involved in a disease process. A classic approach to identify such candidate molecules is to physically make them, one by one, and then expose them to the protein target of interest and test if the two interact. This can be a fairly laborious and time-intensive process. 小分子药物是与细胞蛋白质机制相互作用并以某种方式影响该机制功能的化学物质。通常，药物旨在抑制单个蛋白质靶标的活性，而这些靶标被认为与疾病过程有关。识别这类候选分子的一种经典方法是一个接一个地进行物理制造，然后将其暴露于感兴趣的蛋白质靶点，并测试两者是否相互作用。这可能是一个相当费力和耗时的过程。 The US Food and Drug Administration (FDA) has approved roughly 2,000 novel molecular entities in its entire history. However, the number of chemicals in druglike space has been estimated to be 10^60, a space far too big to physically search. There are likely effective treatments for human ailments hiding in that chemical space, and better methods to find such treatments are desirable to us all. 美国食品药品监督管理局（FDA）已经批准了大约2000种新型分子实体在其整个历史. 然而，类药物领域的化学物质数量估计为$10^60$，这个空间太大了，无法进行物理搜索。在这个化学空间里，可能有有效的治疗人类疾病的方法，而找到更好的治疗方法对我们所有人来说都是可取的。 To evaluate potential search methods in small molecule chemistry, competition host Leash Biosciences physically tested some 133M small molecules for their ability to interact with one of three protein targets using DNA-encoded chemical library (DEL) technology. This dataset, the Big Encoded Library for Chemical Assessment (BELKA), provides an excellent opportunity to develop predictive models that may advance drug discovery. 为了评估小分子化学中潜在的搜索方法，比赛主办方Leash Biosciences使用DNA编码化学文库（DEL）技术对约133M个小分子进行了物理测试，以确定它们与三个蛋白质靶标之一相互作用的能力。该数据集，即化学评估大编码库（BELKA），为开发可能促进药物发现的预测模型提供了极好的机会。 Datasets of this size are rare and restricted to large pharmaceutical companies. The current best-curated public dataset of this kind is perhaps bindingdb, which, at 2.8M binding measurements, is much smaller than BELKA. 这种规模的数据集非常罕见，仅限于大型制药公司。目前这类最好的公共数据集可能是bindingdb，在2.8M的结合测量值下，比BELKA小得多。 This competition aims to revolutionize small molecule binding prediction by harnessing ML techniques. Recent advances in ML approaches suggest it might be possible to search chemical space by inference using well-trained computational models rather than running laboratory experiments. Similar progress in other fields suggest using ML to search across vast spaces could be a generalizable approach applicable to many domains. We hope that by providing BELKA we will democratize aspects of computational drug discovery and assist the community in finding new lifesaving medicines. 这项竞赛旨在通过利用ML技术彻底改变小分子结合预测。ML方法的最新进展表明，使用训练有素的计算模型而不是进行实验室 实验，通过推理搜索化学空间是可能的。其他 领域的类似进展表明，使用ML在广阔的空间中搜索可能是一种适用于许多领域的通用方法。我们希望通过提供BELKA，我们将使计算药物发现的各个方面民主化，并帮助社区寻找新的救命药物。 Here, you’ll build predictive models to estimate the binding affinity of unknown chemical compounds to specified protein targets. You may use the training data provided; alternatively, there are a number of methods to make small molecule binding predictions without relying on empirical binding data (e.g. DiffDock, and this contest was designed to allow for such submissions). 在这里，你将建立预测模型来估计未知化合物与特定蛋白质靶标的结合亲和力。您可以使用提供的培训数据；或者，有许多方法可以在不依赖经验结合数据的情况下进行小分子结合预测（例如DiffDock，而本次竞赛旨在允许此类提交）。 Your work will contribute to advances in small molecule chemistry used to accelerate drug discovery. 你的工作将有助于促进用于加速药物发现的小分子化学的进步。 EvaluationThis metric for this competition is the average precision calculated for each (protein, split group) and then averaged for the final score. Please see this forum post for important details. 这项比赛的指标是为每个（蛋白质、分组）计算的平均精度，然后为最终得分取平均值。请参阅此论坛帖子了解重要细节。 Here’s the code for the implementation. 这是代码以供实施。 Submission FileFor each id in the test set, you must predict a probability for the binary target binds target. The file should contain a header and have the following format: 对于测试集中的每个id，您必须预测二进制目标“绑定”目标的概率。该文件应包含一个标头，并具有以下格式： 12345id,binds295246830,0.5295246831,0.5295246832,0.5etc. Timeline April 4, 2024 - Start Date. July 1, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete. July 1, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams. July 8, 2024 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes First Prize: $12,000 Second Prize: $10,000 Third Prize: $10,000 Fourth Prize: $8,000 Fifth Prize: $5,000 Top Student Group: $5,000 to the highest performing student team. A team would be considered a student team if majority members (e.g. at least 3 out of a 5 member team) are students enrolled in a high school or university degree. In the case of an even number of members, half of them must be students. Competition HostLeash Biosciences is a discovery-stage biotechnology company that seeks to improve medicinal chemistry with machine learning approaches and massive data collection. Leash is comprised of wet lab scientists and dry lab scientists in equal numbers, and is proudly headquartered in Salt Lake City, Utah, USA. Additional DetailsChemical RepresentationsOne of the goals of this competition is to explore and compare many different ways of representing molecules. Small molecules have been [represented](https://pubs.acs.org/doi/10.1021/acsinfocus.7e7006?ref=infocus%2FAI_&amp; Machine Learning) with SMILES, graphs, 3D structures, and more, including more esoteric methods such as spherical convolutional neural nets. We encourage competitors to explore not only different methods of making predictions but also to try different ways of representing the molecules. We provide the molecules in SMILES format. 这场比赛的目标之一是探索和比较许多不同的分子表现方式。小分子已经用SMILES、图形、3D结构等表示，包括更深奥的方法，如球形卷积神经网络。我们鼓励竞争对手不仅探索不同的预测方法，还尝试不同的分子表示方法。 我们提供SMILES格式的分子。 SMILESSMILES is a concise string notation used to represent the structure of chemical molecules. It encodes the molecular graph, including atoms, bonds, connectivity, and stereochemistry as a linear sequence of characters, by traversing the molecule graph. SMILES is widely used in machine learning applications for chemistry, such as molecular property prediction, drug discovery, and materials design, as it provides a standardized and machine-readable format for representing and manipulating chemical structures. The SMILES in this dataset should be sufficient to be translated into any other chemical representation format that you want to try. A simple way to perform some of these translations is with RDKit. SMILES是一种简明的字符串表示法，用于表示化学分子的结构。它通过遍历分子图，将分子图（包括原子、键、连接性和立体化学）编码为线性字符序列。SMILES广泛用于化学的机器学习应用，如分子性质预测、药物发现和材料设计，因为它为表示和操纵化学结构提供了标准化和机器可读的格式。该数据集中的SMILES应该足以转换为您想要尝试的任何其他化学表示格式。执行其中一些翻译的一种简单方法是使用RDKit. Details about the experimentsDELs are libraries of small molecules with unique DNA barcodes covalently attachedTraditional high-throughput screening requires keeping individual small molecules in separate, identifiable tubes and demands a lot of liquid handling to test each one of those against the protein target of interest in a separate reaction. The logistical overhead of these efforts tends to restrict screening collections, called libraries, to 50K-5M small molecules. A scalable solution to this problem, DNA-encoded chemical libraries, was described in 2009. As DNA sequencing got cheaper and cheaper, it became clear that DNA itself could be used as a label to identify, and deconvolute, collections of molecules in a complex mixture. DELs leverage this DNA sequencing technology. These barcoded small molecules are in a pool (many in a single tube, rather than one tube per small molecule) and are exposed to the protein target of interest in solution. The protein target of interest is then rinsed to remove small molecules in the DEL that don’t bind the target, and the remaining binders are collected and their DNA sequenced. DEL是共价连接有独特DNA条形码的小分子库传统高通量筛选需要将单个小分子保持在单独的、可识别的管中，并且需要大量的液体处理来在单独的反应中针对感兴趣的蛋白质靶标测试其中的每一个。这些工作的后勤开销往往将筛选收藏（称为文库）限制在5000万至500万个小分子以内。这个问题的一个可扩展的解决方案，DNA编码的化学文库，在2009年描述. 随着DNA测序变得越来越便宜，很明显，DNA本身可以用作标签来识别和消除复杂混合物中分子的聚集。DELs这种DNA测序技术。这些条形码小分子在一个池中（许多在单管中，而不是每个小分子一管），并暴露于溶液中感兴趣的蛋白质靶标。然后冲洗感兴趣的蛋白质靶标，以去除DEL中不与靶标结合的小分子，收集剩余的结合物并对其DNA进行测序。 DELs are manufactured by combining different building blocksAn intuitive way to think about DELs is to imagine a Mickey Mouse head as an example of a small molecule in the DEL. We attach the DNA barcode to Mickey’s chin. Mickey’s left ear is connected by a zipper; Mickey’s right ear is connected by velcro. These attachment points of zippers and velcro are analogies to different chemical reactions one might use to construct the DEL. We could purchase ten different Mickey Mouse faces, ten different zipper ears, and ten different velcro ears, and use them to construct our small molecule library. By creating every combination of these three, we’ll have 1,000 small molecules, but we only needed thirty building blocks (faces and ears) to make them. This combinatorial approach is what allows DELs to have so many members: the library in this competition is composed of 133M small molecules. The 133M small molecule library used here, AMA014, was provided by AlphaMa. It has a triazine core and superficially resembles the DELs described here. DEL是通过组合不同的构建块来制造的一个思考DEL的直观方法是想象一个米老鼠的头作为DEL中一个小分子的例子。我们把DNA条形码贴在米奇的下巴上。米奇的左耳由拉链连接；米奇的右耳是用尼龙搭扣连接的。拉链和尼龙搭扣的这些连接点类似于可能用于构建DEL的不同化学反应。我们可以购买十个不同的米老鼠脸、十个不同拉链耳朵和十个不同尼龙搭扣耳朵，并用它们来构建我们的小分子库。通过创建这三者的每一个组合，我们将拥有1000个小分子，但我们只需要30个构建块（脸和耳朵）就可以制造它们。这种组合方法使DEL能够拥有如此多的成员：这场竞争中的文库由133M个小分子组成。这里使用的133M小分子文库AMA014由AlphaMa提供。它有一个三嗪核心，表面上类似于此处描述的DEL。 AcknowledgementsLeash Biosciences is grateful for the generous cosponsorship of Top Harvest Capital and AlphaMa. CitationAndrew Blevins, Ian K Quigley, Brayden J Halverson, Nate Wilkinson, Rebecca S Levin, Agastya Pulapaka, Walter Reade, Addison Howard. (2024). Leash Bio - Predict New Medicines with BELKA. Kaggle. https://kaggle.com/competitions/leash-BELKA Dataset DescriptionOverviewThe examples in the competition dataset are represented by a binary classification of whether a given small molecule is a binder or not to one of three protein targets. The data were collected using DNA-encoded chemical library (DEL) technology. 比赛数据集中的例子由给定小分子是否与三个蛋白质靶标之一结合的二元分类表示。使用DNA编码化学文库（DEL）技术收集数据。 We represent chemistry with SMILES (Simplified Molecular-Input Line-Entry System) and the labels as binary binding classifications, one per protein target of three targets. 我们用SMILES（简化分子输入 行输入系统)和二元绑定分类来表示化学，三个靶标中的每个蛋白质靶标都有一个。 Files[train/test].[csv/parquet] - The train or test data, available in both the csv and parquet formats. id - A unique example_id that we use to identify the molecule-binding target pair. buildingblock1_smiles - The structure, in SMILES, of the first building block buildingblock2_smiles - The structure, in SMILES, of the second building block buildingblock3_smiles - The structure, in SMILES, of the third building block molecule_smiles - The structure of the fully assembled molecule, in SMILES. This includes the three building blocks and the triazine core. Note we use a [Dy] as the stand-in for the DNA linker. protein_name - The protein target name binds - The target column. A binary class label of whether the molecule binds to the protein. Not available for the test set. sample_submission.csv - A sample submission file in the correct format [train/test].[csv/parquet] - 训练或测试数据，csv和parquet格式均可。 id - 我们用来识别分子结合靶标对的唯一示例_id。 buildingblock1_smiles - 第一个构建块的结构，以SMILES表示 buildingblock2_smiles - 第二个构建块的结构，以SMILES表示 buildingblock3_smiles - 第三个构建块的结构，以SMILES表示 molecule_smiles - 完全组装的分子的结构，以SMILES表示。这包括三个构建块和三嗪核心。请注意，我们使用[Dy]作为DNA连接子的替代。 protein_name - 蛋白质靶标名称 binds - 目标列。分子是否与蛋白质结合的二进制类标签。不适用于测试集。 Competition dataAll data were generated in-house at Leash Biosciences. We are providing roughly 98M training examples per protein, 200K validation examples per protein, and 360K test molecules per protein. To test generalizability, the test set contains building blocks that are not in the training set. These datasets are very imbalanced: roughly 0.5% of examples are classified as binders; we used 3 rounds of selection in triplicate to identify binders experimentally. Following the competition, Leash will make all the data available for future use (3 targets × 3 rounds of selection × 3 replicates × 133M molecules, or 3.6B measurements). 所有数据均由Leash Biosciences公司内部生成。我们为每种蛋白质提供了大约 98M 个训练实例，为每种蛋白提供了 200K 个验证实例，为每个蛋白质提供了 360K 个测试分子。为了测试可推广性，测试集包含不在训练集中的构建块。这些数据集非常不平衡：大约0.5%的示例被归类为绑定；我们使用了三轮一式三份的选择来实验鉴定粘合剂。比赛结束后，Leash将提供所有数据供未来使用（3个靶标×3轮选择×3个重复×3.33M个分子，或3.6B测量值）。 TargetsProteins are encoded in the genome, and names of the genes encoding those proteins are typically bestowed by their discoverers and regulated by the Hugo Gene Nomenclature Committee. The protein products of these genes can sometimes have different names, often due to the history of their discovery. We screened three protein targets for this competition. 蛋白质在基因组中编码，编码这些蛋白质的基因的名称通常由其发现者命名，并由雨果基因命名委员会监管。这些基因的蛋白质产物有时可能有不同的名称，通常是由于它们的发现历史。我们为这次比赛筛选了三个蛋白质靶点。 EPHX2 (sEH)The first target, epoxide hydrolase 2, is encoded by the EPHX2 genetic locus, and its protein product is commonly named “soluble epoxide hydrolase”, or abbreviated to sEH. Hydrolases are enzymes that catalyze certain chemical reactions, and EPHX2/sEH also hydrolyzes certain phosphate groups. EPHX2/sEH is a potential drug target for high blood pressure and diabetes progression, and small molecules inhibiting EPHX2/sEH from earlier DEL efforts made it to clinical trials. EPHX2/sEH was also screened with DELs, and hits predicted with ML approaches, in a recent study but the screening data were not published. We included EPHX2/sEH to allow contestants an external gut check for model performance by comparing to these previously-published results. We screened EPHX2/sEH purchased from Cayman Chemical, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino sequence is positions 2-555 from UniProt entry P34913, the crystal structure can be found in PDB entry 3i28, and predicted structure can be found in AlphaFold2 entry 34913. Additional EPHX2/sEH crystal structures with ligands bound can be found in PDB. 第一个靶标环氧化物水解酶2由EPHX2基因座编码，其蛋白产物通常被命名为“可溶性环氧化物水解酶”，或缩写为sEH。水解酶是催化某些化学反应的酶，EPHX2/sEH也水解某些磷酸基团。EPHX2/sEH是高血压和糖尿病进展的潜在药物靶点，早期DEL研究中抑制EPHX2/s EH的小分子已进入临床试验.EPHX2/sEH也用DEL进行了筛选，并用ML方法预测了命中率(https://blog.research.google/2020/06/unlocking-chemome-with-dna-encoded.html学习https://pubs.acs.org/doi/10.1021/acs.jmedchem.0c00452)但筛选数据没有公布。我们纳入了EPHX2/sEH，通过与之前公布的结果进行比较，让参赛者能够对模型性能进行外部检查。我们筛选了EPHX2/sEH购自开曼化学. 在PDB中可以发现具有结合配体的额外的EPHX2/sEH晶体结构。 BRD4The second target, bromodomain 4, is encoded by the BRD4 locus and its protein product is also named BRD4. Bromodomains bind to protein spools in the nucleus that DNA wraps around (called histones) and affect the likelihood that the DNA nearby is going to be transcribed, producing new gene products. Bromodomains play roles in cancer progression and a number of drugs have been discovered to inhibit their activities. BRD4 has been screened with DEL approaches previously but the screening data were not published. We included BRD4 to allow contestants to evaluate candidate molecules for oncology indications. We screened BRD4 purchased from Active Motif, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 44-460 from UniProt entry O60885-1, the crystal structure (for a single domain) can be found in PDB entry 7USK and predicted structure can be found in AlphaFold2 entry O60885. Additional BRD4 crystal structures with ligands bound can be found in PDB. ALB (HSA)The third target, serum albumin, is encoded by the ALB locus and its protein product is also named ALB. The protein product is sometimes abbreviated as HSA, for “human serum albumin”. ALB, the most common protein in the blood, is used to drive osmotic pressure (to bring fluid back from tissues into blood vessels) and to transport many ligands, hormones, fatty acids, and more. Albumin, being the most abundant protein in the blood, often plays a role in absorbing candidate drugs in the body and sequestering them from their target tissues. Adjusting candidate drugs to bind less to albumin and other blood proteins is a strategy to help these candidate drugs be more effective. ALB has been screened with DEL approaches previously but the screening data were not published. We included ALB to allow contestants to build models that might have a larger impact on drug discovery across many disease types. The ability to predict ALB binding well would allow drug developers to improve their candidate small molecule therapies much more quickly than physically manufacturing many variants and testing them against ALB empirically in an iterative process. We screened ALB purchased from Active Motif. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 25 to 609 from UniProt entry P02768, the crystal structure can be found in PDB entry 1AO6, and predicted structure can be found in AlphaFold2 entry P02768. Additional ALB crystal structures with ligands bound can be found in PDB. Good luck!","link":"/2024/05/19/BELKA-2024/"},{"title":"IDEA 2022 搭建 Tomcat 环境","text":"[TOC] Tomcat 环境的搭建参考教程 下载 TomcatTomcat官网地址在 Tomcat 官网中下载指定版本的 Tomcat，左侧 Download 处有相应版本，这里推荐 Tomcat 9 版本（因为Tomcat 10 在配置时会出现一定的问题）。下载后解压到指定位置即可。 配置环境变量即可配置 Tomcat 环境变量前一定要配置好 Java 的环境变量，尤其是JAVA_HOME，这里我一开始并没有配置 JAVA_HOME，我的环境变量是JAVA_HOME_180=xxx，这种方式Tomcat是找不到JAVA_HOME的，因此我又重新配置了JAVA_HOME。我的 JAVA_HOME 环境变量为： 1JAVA_HOME=D:\\JDK\\jdk1.8.0_231 下面是 Tomcat 的环境变量配置：新建 CATALINA_HOME 环境变量： 1CATALINA_HOME=D:\\tomcat\\apache-tomcat-9.0.89 修改Path，在 Path 后添加（新建）如下环境变量： 123%CATALINA_HOME%\\lib%CATALINA_HOME%\\bin%CATALINA_HOME%\\lib\\servlet-api.jar 验证是否配置成功在命令行中，执行命令：startup.bat，若正常打印相关配置变量、且 Tomcat 进程被阻塞，即证明环境搭建成功。访问localhost:8080，出现以下界面即证明成功搭建。使用 shutdown.bat 命令即可使阻塞的 Tomcat 进程被关闭，推荐使用这种方式关闭 Tomcat。 可能会出现的问题 协议处理程序初始化失败：参考教程这个问题有可能是由于8080端口被占用了，在Windows中可以使用如下命令查看端口的占用情况： 1netstat -aon|findstr &quot;8080&quot; 如果确实被占用了，可以使用如下命令杀死端口号为 &lt;PIDNUM&gt; 的进程。 1taskkill -PID &lt;PIDNUM&gt; -F 闪退可能原因是：环境变量配置不正确，仔细检查环境变量的配置。 乱码问题描述：打开startup.bat后汉字乱码解决方法：在.\\apache-tomcat-9.0.43\\conf下打开logging.properties文件将java.util.logging.ConsoleHandler.encoding = UTF-8替换为java.util.logging.ConsoleHandler.encoding = GBK 社区版 IDEA 如何配置 TomcatCSDN 上大多数教程使用 Maven 创建 Tomcat 项目，但是这种方法实在是过于麻烦，社区版和专业版又有些不同，找不到很多东西。 如何配置 IDEA 2022 社区版中的 Tomcat 安装插件在 File → Settings → Plugin 中安装插件，搜索 Tomcat，安装插件。 配置Tomcat路径安装插件后，在 File → Settings → Plugin → Tomcat Server添加配置如下： 完成","link":"/2024/05/22/Build-Tomcat/"},{"title":"Better QT","text":"[TOC] 🍞 Better QT🍦 1. Qt 的一些常用技巧1.1 快捷键 快捷键 Ctrl + Tab 可以切换文件； 快捷键 Alt + ENTER 弹出代码生成提示，可以快速提示错误修改方案，类似于 IDEA 的 Alt + ENTER； 快捷键 Alt + 鼠标 同时输入； 快捷键 Ctrl + R 运行程序； 快捷键 Ctrl + M 创建书签（Bookmark），或者直接在某行代码前右键添加书签； 快捷键 Ctrl + ENTER 在当前行下方插入空行； 快捷键 Ctrl + Shift + ENTER 在当前行下方插入空行； 快捷键 Ctrl + I 代码对齐； 快捷键 Ctrl + ; 格式化代码； 快捷键 Shift + Delete 剪切当前行，可以当删除用； 快捷键 Ctrl + Shift + R 局部变量统一修改； 快捷键 Ctrl + Shift + V 复制历史； 用键盘模拟鼠标操作： 功能键 方向键 备注 Ctrl Shift Alt 左/右 上/下 Home/End 方向键具有移动光标的作用 × × × 字符 字符 行首/行尾 - √ × × 单词 滚动条 文件头/尾 - √ √ × 单词 移动 行首/行尾 Shift具有选中文本的作用 √ × √ - 向上/下复制选中部分 - - 快捷键 F1 查看帮助、文档 快捷键 F2 快速到变量或者函数间切换 快捷键 F4 快速在.cpp文件和.h文件间切换 快捷键 Ctrl + Shift + U 查找所有使用该符号的地方 快捷键 Ctrl + K 打开定位器 快捷键 Ctrl + L 跳转到某一行 快捷键 Ctrl + [Shift] + F 查找/替换当前文件[项目]当前选中的内容 快捷键 [Shift] + F3 查找下[上]一个 快捷键 Ctrl + B 编译工程 快捷键 Ctrl + R 运行工程 快捷键 F5 调试运行 快捷键 Ctrl + Shift + F5 重启调试 快捷键 F9 设置和取消断点 快捷键 F10 单步跳过 快捷键 F11 单步进入 1.2 Creator 片段片段简单理解一下就是已经写好的一些模式化的代码，用户可以使用内置片段或者根据自己的需要自定义片段。 自带片段示例 自定义片段一个用户的自定义片段需要以下几个内容：$$片段 = 一级标题 + 二级标题 + 片段文本$$需要通过：编辑（Edit）→首选项（Preferences）→文本编辑器（Text Editor）→片段（Snippets）进行设置比如我要添加一个自定义片段 note，用来表示文件注释，可以选择 Group 为 C++，然后选择 Add，添加指定的内容： 🍦 2. Qt 代码/文件解释Qt的源代码和文件解释 2.1 Qt 代码 hellocosbrowser.h 1234567891011121314151617181920212223242526272829#ifndef HELLOCOSBROWSER_H#define HELLOCOSBROWSER_H#include &lt;QWidget&gt;#include &lt;QMessageBox&gt;QT_BEGIN_NAMESPACEnamespace Ui { class HelloCOSBrowser; }QT_END_NAMESPACEclass HelloCOSBrowser : public QWidget // QWidget 是所有应用程序窗口的基类{ Q_OBJECT // Qt的宏, 支持 Qt 的特性, 如信号与槽、对象树、元对象等public: // 这里 HelloCOSBrowser 指定父窗口指针为 nullptr, 则它会作为一个独立的窗口进行展示, 否则则会作为父窗口的一个控件 // 关于这个父窗口指针, 一个很典型的应用就是 微信 // 当我们创建新窗口的时候, 如果不指定父窗口, 就会弹出一个独立的新窗口, 即电脑任务栏的图标会多出来一个 // 如果指定了父窗口, 则不会创建一个独立的窗口, 即电脑任务栏处的图标不会增加 HelloCOSBrowser(QWidget *parent = nullptr); ~HelloCOSBrowser();private slots: void showDialog();private: Ui::HelloCOSBrowser *ui;};#endif // HELLOCOSBROWSER_H 2.2 Qt 工程文件解释文件列表 文件名称 描述 pro 文件 该文件是 Qt 的项目文件，qmake工具可以根据此文件生成 Makefile pro.user 文件 该文件包含和用户相关的项目信息（用户不需要关注此文件） ui 文件 Qt 的设计师界面文件 .cpp 文件 C++ 源文件 .h 文件 C++ 头文件 🍦 3. MOC编译器MOC(Meta-Object Compiler)编译器C++ 编译器本身不支持 Qt 的某些机制，Qt 希望对 C++ 代码进行自动扩展，这里就需要用到宏（例如：Q_Object）和继承。此外为了方便用户使用，希望用户无感知，可以将这一操作直接集成到框架中。 3.1 Qt 编译过程12345预编译 -&gt; 编译 -&gt; 汇编 -&gt; 链接 -&gt; 目标 ↑ +-------------------------+ ↑拓展代码 -&gt; MOC编译器 -&gt; 新CPP代码 通过上述方式，实现 Qt 的某些特性。我们可以发现，当我们写完代码进行编译后，会产生一个 debug 文件夹，此时我们进入该文件夹，会看到一些元对象编译器编译的文件，如 moc_xxxx.cpp 或 moc_xxx.h 等文件。 3.2 MOC 的使用方法 MOC 编译工具由 Qt 框架自动调用 扫描 C++ 头文件，寻找 Q_OBJECT 宏 生成拓展 C++ 代码，再进行预编译 程序员在使用时，需要继承 QObject 类或者是 QObject 子类，并且包含 Q_OBJECT 宏。 🍦 4. Qt应用程序开发4.1 Qt Designer 设计师界面使用① Qt 控件编辑模式② Qt 信号与槽编辑模式③ Qt 伙伴关系编辑模式④ Qt Tab 顺序编辑模式：可以设置按下 Tab 键的高亮顺序 4.2 Qt 核心——信号与槽信号与槽的基本概念 Qt 中的信号和槽是支持多对多的，即一个信号可以对应多个槽，一个槽可以由多个信号触发。 Qt 中的信号无需实现，可以由函数（普通函数或者槽函数）通过 emit 关键字发送信号传递参数。 4.2.1 Qt中如何定义信号 继承 QObject 类或其派生类，同时包含 Q_OBJECT 宏 使用关键字 signals 声明函数信号函数，不需要具体实现信号函数 使用 emit 关键字发送信号 4.2.2 Qt中如何定义槽函数 必须包含 Q_OBJECT 宏 使用关键字 [public/protected/private] slots 声明函数 需要具体实现声明的槽函数 4.2.3 Qt中如何连接信号与槽（三种写法） SIGNAL/SLOT 宏写法：QObject::connect(this, SIGNAL(...), this, SLOT(...)); 函数指针写法：QObject::connect(this, &amp;SignalFunction, this, &amp;SlotFunction) lambda 表达式写法：QObject::connect(this, &amp;SignalFunction, this, [=]() { qDebug() &lt;&lt; &quot;...&quot;; }) 三种写法的比较： 连接信号与槽 宏 函数指针 编译 运行 编译 运行 参数类型 完全相同 √ √ √ √ 隐式转换 向上 √ × √ √ 向下 √ × √ √ 不可以隐式转换 √ × × × 参数个数 信号=槽 √ √ √ √ 信号>槽 √ √ √ √ 信号","link":"/2024/05/22/Better-QT/"},{"title":"Evaluation Indicators in AI","text":"","link":"/2024/05/27/Evaluation-Indicators-in-AI/"},{"title":"QuickPassHBase","text":"快速上手HBase[TOC] ⚙ 1. HBase简介1.1 HBase的定义Apache HBase 是以 HDFS 为数据存储的，一种分布式、可扩展的 NoSQL 数据库。 HBase 的设计理念依据 Google 的 BigTable 论文，论文中对于数据模型的首句介绍。 BigTable是一个稀疏的、分布式的、持久的多维排序映射(Map)。该映射由行键、列键和时间戳索引作为键(Key)，映射中的每个值(Value)都是一个未解释的字节数组。 HBase 使用与 BigTable 非常相似的数据模型。用户将数据行存储在带标签的表中。数据行具有可排序的键和任意数量的列。该表存储稀疏，因此如果用户喜欢，同一表中的行可以具有疯狂变化的列。 1.2 HBase的数据模型1.2.1 HBase 的逻辑结构12345678910111213141516171819202122232425{ &quot;row_key1&quot;: { &quot;personal_info&quot;: { &quot;name&quot;: &quot;ZhangSan&quot;, &quot;city&quot;: &quot;Beijing&quot;, &quot;phone&quot;: &quot;156****0000&quot; }, &quot;office_info&quot;: { &quot;tel&quot;: &quot;010-1234567&quot;, &quot;address&quot;: &quot;Shandong&quot; } }, &quot;row_key11&quot;: { &quot;personal_info&quot;: { &quot;city&quot;: &quot;Shanghai&quot;, &quot;phone&quot;: &quot;133****0000&quot; }, &quot;office_info&quot;: { &quot;tel&quot;: &quot;010-1234567&quot;, } }, &quot;row_key2&quot;: { ... }} 列族→ personal_info office_info RowKey↓ name city phone tel address row_key1 ZhangSan Beijing 156****0000 010-1234567 Shandong row_key11 Shanghai 131****0000 010-1234567 row_key2 ... ... ... ... ... 在上面的表格中： personal_info、office_info称为列族 name、city、phone、tel、address称为列 row_key1、row_key11称为行键。 将一整张大表按照行进行拆分，拆分为多个表，拆分后的每个表称为**块(Region)**，用于实现分布式结构。 将一整张大表按照列族进行拆分，拆分为多个**存储(Store)**，用于在底层存储到不同的文件夹中，便于文件对应。 存储数据稀疏，数据存储多维，不同的行具有不同的列。数据存储整体有序，按照RowKey的字典序排列，RowKey为一个Byte数组。 1.2.2 HBase 的物理结构物理存储结构即为数据映射关系，而在概念视图的空单元格，底层实际根本不存储。 在HDFS中划分好的存储Store如下： personal_info RowKey name city phone row_key1 ZhangSan Beijing 156****0000 row_key11 Shanghai 131****0000 row_key2 ... ... ... 其底层一定是以映射(Map)的方式进行存储的，格式为**(Key, Value)，Value一定是“ZhangSan”**这种字段。那么Key是什么呢？ 为了确定Value值**”ZhangSan”，我们需要用Key对应到Value**，于是得到存储如下： Row Key Column Family Column Qualifier Timestamp Type Value row_key1 personal_info name t1 Put ZhangSan row_key1 personal_info city t2 Put Beijing row_key1 personal_info phone t3 Put 156****0000 row_key1 personal_info phone t4 Put 156****0001 row_key1 personal_info phone t5 Delete 156****0001 … … … … … … 因为 HDFS 是无法修改数据的，而 HBase 需要修改数据，那么就需要解决这一问题，于是就有了**时间戳(Timestamp)**。不同版本（version）的数据根据 Timestamp 进行区分，读取数据默认读取最新的版本。 在上面的表格中，t4相对于t3来说就是进行了修改，将t3时的**phone从156****0000修改为t4时的156****0001，读取时默认读取t4时的phone**值，通过这种方式完成了修改。 同样的，我们也不好删除数据，因此我们只需要插入一条**Type**为Delete的数据即可。 1.2.3 数据模型 Name Space 命名空间 类似于关系型数据库的 Database 概念，每个命名空间下有多个表。HBase 两个自带的命名空间，分别是 hbase 和default，hbase 中存放的是 HBase 内置的表，default表是用户默认使用的命名空间。 Table 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需要声明具体的列。因为数据存储时稀疏的，所有往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。 需要注意的是，列族的存在是动态添加列（或称字段）的基础。 Row HBase 表中的每行数据都由*一个行键(RowKey)和多个列(Column)组成，数据是按照 RowKey的字典顺序存储的，*并且查询数据时只能根据 RowKey进行检索**，所以RowKey的设计十分重要。 Column HBase 中的每个列都由列族(Column Family)和列限定符(Column Qualifier)进行限定，例如info:name, info:age。建表时，只需指明列族，而列限定符无需预先定义。列限定符听起来很高端，其实就是列名的意思。 Time Stamp 用于标识数据的**不同版本(Version)**，每条数据写入时，系统会自动为其加上该字段，其值为写入 HBase 的时间。 Cell 由 {rowkey, Column Family: Column Qualifier, Timestamp} 唯一确定的单元，Cell 中的数据全部是字节码形式存储。 1.3 HBase 基本架构 Master 主要进程，具体实现类为HMaster，通常部署在NameNode上。 主要功能：负责通过 ZK 监控 RegionServer 进程状态，同时是所有元数据变化的接口，内部启动监控执行 region 的故障转移和拆分的线程。 功能的详细描述： 管理元数据表格 hbase:meta：接收用户对表格创建、修改、删除的命令并执行。 监控 RegionServer 是否需要进行负载均衡、故障转移和Region拆分。通过启动多个后台线程监控实现上述功能： LoadBalancer 负载均衡器 周期性监控 region分布在 RegionServer 上面是否均衡，由参数 hbase.balancer.period控制周期时间，默认5分钟。 CatalogJanitor元数据管理器 定期检查和清理hbase:meta中的数据。 MasterProcWAL Master 预写日志处理器 把Master需要执行的任务记录到预写日志WAL中，如果Master宕机，则让BackupMaster继续操作。 RegionServer 主要进程，具体实现类为HRegionServer，通常部署在DataNode上。 功能：主要负责数据 Cell 的处理，同时在执行区域的拆分和合并的时候，由 RegionServer 来实际执行。 功能的详细描述： 负责数据 Cell 的处理，例如写入数据put，查询数据get等。 拆分合并 region 的实际执行者，有 Master 监控，有RegionServer 执行。 ZooKeeper HBase 通过 ZooKeeper 来做 Master的高可用、记录 RegionServer 的部署信息、并且存储有 meta 表的位置信息。HBase 对于数据的读写操作时是直接访问 ZooKeeper 的，在 2.3 版本推出 Master Registry 模式，客户端可以直接访问 Master。使用此功能，会加大对 Master的压力，减轻对 ZooKeeper 的压力。 HDFS HDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高容错的支持。 上图中的Region由三个RegionServer随机管理，尽量均衡。表名hbase:meta是一个特例，他存储在HDFS，但是由Master管理。 🔧 2. 快速上手2.1 安装部署2.1.1 分布式部署 至少 3 台虚拟机 123hadoop101hadoop102hadoop103 保证 ZooKeeper 正常部署，并且启动 ZooKeeper 1zkServer.sh start 保证 Hadoop 正常部署，并且启动 Hadoop 1start-dfs.sh 配置 HBase 环境 ① 下载 HBase 安装包（压缩包），这里假设为hbase-2.4.11-bin.tar.gz ② 解压 HBase 安装包到一个文件夹 1tar -zxvf /path/to/hbase-2.4.11-bin.tar.gz -C /path/to/module ③ 在用户目录下，添加用户环境变量 1vim .bashrc 123#HBase_HOMEexport HBASE_HOME = /path/to/module/hbase-2.4.11export PATH = $PATH:$HBASE_HOME/bin ④ 使环境变量生效 1source .bashrc ⑤ 修改配置文件 hbase-env.sh 123# 表示是否需要 HBase 管理维护一个自带的 ZooKeeper, 默认为 true# 我们需要使用本机已经配置好的 ZooKeeper, 所以修改为 Falseexport HBASE_MANAGES_ZK = false hbase-site.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!-- ZooKeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop101,hadoop102,hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase数据在HDFS中的存放路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hadoop101:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase的运行模式 --&gt; &lt;!-- false为单机模式, HBase和ZooKeeper会运行在同一个JVM虚拟机中 --&gt; &lt;!-- true 为分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper快照的存储位置 --&gt; &lt;!-- 这里替换为自己的 /path/to/ZooKeeperDir --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.6/data&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase 安全模式 --&gt; &lt;!-- 在分布式模式下, 设置为 false --&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers 123hadoop101hadoop102hadoop103 ⑥ 解决 log4j 不兼容的问题，移除 HBase或者 Hadoop的 .jar包 ⑦ 使用 scp 命令同步 HBase 配置，需要提前设置好免密登录。或者使用 xsync 启动 HBase 服务 单点启动 1234#单点启动HMasterhbase-daemon.sh start master#单点启动HRegionServerhbase-daemon.sh start regionserver 集群启动 1start-hbase.sh 停止服务 1stop-hbase.sh 2.1.2 高可用服务 如果 HBase 已经启动，先关闭HBase 1stop-hbase.sh 添加配置文件 backup-masters 123#使用touch命令或者echo命令均可touch /path/to/hbase-2.1.4/conf/backup-mastersvim /path/to/hbase-2.1.4/conf/backup-masters 添加内容：hadoop102 使用 scp 命令分发配置文件 启动HBase，正常启动进程如下： 123hadoop101 -&gt; HMaster HRegionServerhadoop102 -&gt; HMaster HRegionServerhadoop103 -&gt; HRegionServer 其中，hadoop101 的 HMaster 先启动作为主节点，hadoop102 的 HMaster后启动，作为**备用节点(Backup-Master)**。 2.2 使用操作2.2.1 Shell操作使用命令 hbase shell 启动 HBase 的 Shell 命令界面，所有命令均可以使用 help 查到。 当我们在 hbase shell中输入help命令时，将会弹出HBase的使用提示： 1hbase shell 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667hbase(main):001:0&gt; helpHBase Shell, version 2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Type 'help &quot;COMMAND&quot;', (e.g. 'help &quot;get&quot;' -- the quotes are necessary) for help on a specific command.Commands are grouped. Type 'help &quot;COMMAND_GROUP&quot;', (e.g. 'help &quot;general&quot;') for help on a command group.COMMAND GROUPS: Group name: general Commands: processlist, status, table_help, version, whoami Group name: ddl Commands: alter, alter_async, alter_status, clone_table_schema, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, list_regions, locate_region, show_filters Group name: namespace Commands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables Group name: dml Commands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve Group name: tools Commands: assign, balance_switch, balancer, balancer_enabled, catalogjanitor_enabled, catalogjanitor_run, catalogjanitor_switch, cleaner_chore_enabled, cleaner_chore_run, cleaner_chore_switch, clear_block_cache, clear_compaction_queues, clear_deadservers, close_region, compact, compact_rs, compaction_state, flush, hbck_chore_run, is_in_maintenance_mode, list_deadservers, major_compact, merge_region, move, normalize, normalizer_enabled, normalizer_switch, split, splitormerge_enabled, splitormerge_switch, stop_master, stop_regionserver, trace, unassign, wal_roll, zk_dump Group name: replication Commands: add_peer, append_peer_exclude_namespaces, append_peer_exclude_tableCFs, append_peer_namespaces, append_peer_tableCFs, disable_peer, disable_table_replication, enable_peer, enable_table_replication, get_peer_config, list_peer_configs, list_peers, list_replicated_tables, remove_peer, remove_peer_exclude_namespaces, remove_peer_exclude_tableCFs, remove_peer_namespaces, remove_peer_tableCFs, set_peer_bandwidth, set_peer_exclude_namespaces, set_peer_exclude_tableCFs, set_peer_namespaces, set_peer_replicate_all, set_peer_serial, set_peer_tableCFs, show_peer_tableCFs, update_peer_config Group name: snapshots Commands: clone_snapshot, delete_all_snapshot, delete_snapshot, delete_table_snapshots, list_snapshots, list_table_snapshots, restore_snapshot, snapshot Group name: configuration Commands: update_all_config, update_config Group name: quotas Commands: list_quota_snapshots, list_quota_table_sizes, list_quotas, list_snapshot_sizes, set_quota Group name: security Commands: grant, list_security_capabilities, revoke, user_permission Group name: procedures Commands: list_locks, list_procedures Group name: visibility labels Commands: add_labels, clear_auths, get_auths, list_labels, set_auths, set_visibility Group name: rsgroup Commands: add_rsgroup, balance_rsgroup, get_rsgroup, get_server_rsgroup, get_table_rsgroup, list_rsgroups, move_namespaces_rsgroup, move_servers_namespaces_rsgroup, move_servers_rsgroup, move_servers_tables_rsgroup, move_tables_rsgroup, remove_rsgroup, remove_servers_rsgroupSHELL USAGE:Quote all names in HBase Shell such as table and column names. Commas delimitcommand parameters. Type &lt;RETURN&gt; after entering a command to run it.Dictionaries of configuration used in the creation and alteration of tables areRuby Hashes. They look like this: {'key1' =&gt; 'value1', 'key2' =&gt; 'value2', ...}and are opened and closed with curley-braces. Key/values are delimited by the'=&gt;' character combination. Usually keys are predefined constants such asNAME, VERSIONS, COMPRESSION, etc. Constants do not need to be quoted. Type'Object.constants' to see a (messy) list of all constants in the environment.If you are using binary keys or values and need to enter them in the shell, usedouble-quote'd hexadecimal representation. For example: hbase&gt; get 't1', &quot;key\\x03\\x3f\\xcd&quot; hbase&gt; get 't1', &quot;key\\003\\023\\011&quot; hbase&gt; put 't1', &quot;test\\xef\\xff&quot;, 'f1:', &quot;\\x01\\x33\\x40&quot;The HBase shell is the (J)Ruby IRB with the above HBase-specific commands added.For more on the HBase Shell, see http://hbase.apache.org/book.html 根据上述信息，我们可以进一步的操作 HBase 数据库。我们实际开发中常用的**命令组(COMMAND GROUPS)**有：general、namespace、ddl、dml等，下面依次介绍这些内容： 通用命令 general 查看 HBase 状态 status，提供 HBase 的状态，如服务器的数量等 123hbase(main):001:0&gt; status1 active master, 0 backup masters, 1 servers, 0 dead, 4.0000 average loadTook 0.5268 seconds 查看 HBase 版本 version，提供正在使用 HBase 版本 123hbase(main):002:0&gt; version2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Took 0.0002 seconds 表引用命令提供帮助 table_help 提供有关用户的信息 whoami 1234hbase(main):003:0&gt; whoaminilera (auth:SIMPLE) groups: nileraTook 0.0283 seconds 操作命名空间 Namespace **命名空间(Namespace)**，相当于MySQL数据库中的DataBase。Namespace 命令包括：alter namespace、create_namespace、describe_namespace、drop_namespace、list_namespace、list_namespace_tables。下面将对一些常用命令进行介绍： 查看全部命名空间 list_namespace 123456hbase(main):001:0&gt; list_namespaceNAMESPACEdefaulthbase2 row(s)Took 0.5484 seconds 创建命名空间 create_namespace 用法：create_namespace 'ns' 123456789hbase(main):001:0&gt; create_namespace 'bigdata'Took 0.0432 secondshbase(main):002:0&gt; list_namespaceNAMESPACEbigdatadefaulthbase3 row(s)Took 0.0224 seconds 删除命名空间 drop_namespace 用法：drop_namespace 'ns'，删除命名空间时，命名空间必须为空。 查看命名空间 describe_namespace 用法：describe_namespace 'ns' 12345hbase(main):001:0&gt; describe_namespace 'bigdata'DESCRIPTION{NAME =&gt; 'bigdata'}Took 0.0068 seconds=&gt; 1 查看命名空间下的表 list_namespace_tables 用法：list_namespace_tables 'ns' 1234567hbase(main):001:0&gt; list_namespace_tables 'default'TABLElogsuser2 row(s)Took 0.3790 seconds=&gt; [&quot;logs&quot;, &quot;user&quot;] 数据定义语言 ddl DDL(Data Definition Language)数据定义语言，主要是进行定义/改变表的结构、数据类型、表之间的链接等操作。ddl 相关命令如下：alter、alter_async、alter_status、clone_table_schema、create、describe、disable、disable_all、drop、drop_all、enable、enable_all、exists、get_table、is_disabled、is_enabled、list、list_regions、locate_region、show_filters。下面将对一些常用命令进行介绍： 创建表 create 常见用法： ① create 'ns:tb', {NAME =&gt; 'cf', VERSIONS =&gt; 5} ​ 在命名空间 ns 下，创建一张表 tb，定义一个列族 cf。 ② 当在默认命名空间default下创建表时，可以省略 ns ③ create 'tb', 'cf1', 'cf2' ​ 在默认命名空间default下，创建一张表tb，并定义两个列族 cf1、cf2 ④ create 'tb', {NAME =&gt; 'cf1', VERSIONS =&gt; 5}, {NAME =&gt; 'cf2', VERSIONS =&gt; 5} ​ 在默认命名空间default下，创建一张表tb，并定义两个列族 cf1、cf2，并同时指定两个列族的版本为 5。 1234hbase(main):001:0&gt; create 'bigdata:person', {NAME =&gt; 'name', VERSIONS =&gt; 5}, {NAME =&gt; 'msg', VERSIONS =&gt; 5}Created table bigdata:personTook 1.5638 seconds=&gt; Hbase::Table - bigdata:person 查看表的详细信息 describe 用法：describe 'tb' 123456789101112hbase(main):010:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'msg', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}2 row(s)Took 0.1536 seconds 修改表 alter 表名创建时写的所有和列族相关的信息，都可以后续通过alter修改，包括增加、删除列族。 ① 增加列族和修改信息都使用覆盖的方法 ​ 修改列族的版本，VERSIONS =&gt; 6： 12345hbase(main):001:0&gt; alter 'bigdata:person', NAME =&gt; 'name', VERSIONS =&gt; 6Updating all regions with the new schema...1/1 regions updated.Done.Took 4.0145 seconds ​ 添加列族 tel： 12345hbase(main):002:0&gt; alter 'bigdata:person', NAME =&gt; 'tel', VERSIONS =&gt; 6Updating all regions with the new schema...1/1 regions updated.Done.Took 2.4498 seconds ​ 查看修改后的数据： 1234567891011121314151617hbase(main):003:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'msg', VERSIONS =&gt; '6', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'tel', VERSIONS =&gt; '6', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}3 row(s)Took 0.0795 seconds ② 删除列族 ​ 删除列族可以用以下两种方式： 12345hbase(main):001:0&gt; alter 'bigdata:person', NAME =&gt; 'tel', METHOD =&gt; 'delete'Updating all regions with the new schema...1/1 regions updated.Done.Took 2.1046 seconds 12345hbase(main):002:0&gt; alter 'bigdata:person', 'delete' =&gt; 'msg'Updating all regions with the new schema...1/1 regions updated.Done.Took 2.9721 seconds ​ 然后查询修改后的数据： 123456789hbase(main):003:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}1 row(s)Took 0.0391 seconds 禁用表 disable 用法： disable 'ns:tb'或disable 'tb' 12hbase(main):001:0&gt; disable 'bigdata:person'Took 0.9384 seconds 删除表 drop 用法： drop 'ns:tb'或drop 'tb'，删除表时需要保证表是禁用的，否则会出现以下错误： 1234567hbase(main):001:0&gt; drop 'bigdata:person'ERROR: Table bigdata:person is enabled. Disable it first.For usage try 'help &quot;drop&quot;'Took 0.0248 seconds ​ 禁用表后再删除表： 12hbase(main):001:0&gt; drop 'bigdata:person'Took 1.7106 seconds 数据操纵语言 dml DML(Data Manipulation Language)数据操纵语言，主要是对数据进行增加、删除、修改操作。 写入数据 put 在 HBase 中如果想要写入数据，只能添加结构中最底层的 Cell。可以手动写入时间戳指定 Cell 的版本，推荐不写，默认使用当前的系统时间。如果重复写入相同 rowKey，相同列的数据，会写入多个版本进行覆盖。所以他同时兼具写入和修改的功能。 用法： ① put 'ns:tb', 'rk', 'col', 'value' ​ 向命名空间ns中的tb表中的行键为rk，列为col的位置写入值value。其中col为cf:col（即列族:列名）的格式。 ​ 如果重复向相同行号rk，相同col写数据，则会进行覆盖。 12345678910111213hbase(main):001:0&gt; put 'bigdata:student', '1001', 'info:name', 'zhangsan'Took 0.2415 secondshbase(main):002:0&gt; put 'bigdata:student', '1001', 'info:name', 'lisi'Took 0.0121 secondshbase(main):003:0&gt; put 'bigdata:student', '1001', 'info:name', 'wangwu'Took 0.0342 secondshbase(main):004:0&gt; put 'bigdata:student', '1002', 'info:name', 'zhaoliu'Took 0.0082 secondshbase(main):005:0&gt; put 'bigdata:student', '1003', 'info:age', '10'Took 0.0050 secondshbase(main):006:0&gt; put 'bigdata:student', '1003', 'info:sex', 'male'Took 0.0054 seconds ② put 't1', 'r1', 'c1', 'value'用法同上。 读取数据 get/scan 读取数据的方法有两个：get 和 scan get最大范围是一行数据，也可以进行列的过滤，读取数据的结果为多行 Cell。 scan是扫描数据，能够读取多行数据，不建议扫描过多数据，推荐使用 startRow 和 stopRow 来控制读取的数据，默认范围左闭右开。 ① get命令 1234567891011121314Some examples: hbase&gt; t.get 'r1' #查看'r1'的数据 hbase&gt; t.get 'r1', {TIMERANGE =&gt; [ts1, ts2]} hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1'} #过滤单列, 只显示 'c1' hbase&gt; t.get 'r1', {COLUMN =&gt; ['c1', 'c2', 'c3']} #过滤多列, 只显示 'c1', 'c2', 'c3' hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1', TIMESTAMP =&gt; ts1} hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1', TIMERANGE =&gt; [ts1, ts2], VERSIONS =&gt; 4} hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1', TIMESTAMP =&gt; ts1, VERSIONS =&gt; 4} hbase&gt; t.get 'r1', {FILTER =&gt; &quot;ValueFilter(=, 'binary:abc')&quot;} hbase&gt; t.get 'r1', 'c1' hbase&gt; t.get 'r1', 'c1', 'c2' hbase&gt; t.get 'r1', ['c1', 'c2'] hbase&gt; t.get 'r1', {CONSISTENCY =&gt; 'TIMELINE'} hbase&gt; t.get 'r1', {CONSISTENCY =&gt; 'TIMELINE', REGION_REPLICA_ID =&gt; 1} 1234567891011121314151617hbase(main):001:0&gt; get 'bigdata:student', '1001'COLUMN CELL info:name timestamp=1717580289267, value=wangwu1 row(s)Took 0.0645 secondshbase(main):002:0&gt; get 'bigdata:student', '1001', {COLUMN =&gt; 'info:name'}COLUMN CELL info:name timestamp=1717580289267, value=wangwu1 row(s)Took 0.0107 secondshbase(main):003:0&gt; get 'bigdata:student', '1003', {COLUMN =&gt; 'info:age'}COLUMN CELL info:age timestamp=1717580366636, value=101 row(s)Took 0.0185 seconds ② scan 命令 12345678910111213141516Some examples: hbase&gt; scan 'hbase:meta' hbase&gt; scan 'hbase:meta', {COLUMNS =&gt; 'info:regioninfo'} hbase&gt; scan 'ns1:t1', {COLUMNS =&gt; ['c1', 'c2'], LIMIT =&gt; 10, STARTROW =&gt; 'xyz'} hbase&gt; scan 't1', {COLUMNS =&gt; ['c1', 'c2'], LIMIT =&gt; 10, STARTROW =&gt; 'xyz'} hbase&gt; scan 't1', {COLUMNS =&gt; 'c1', TIMERANGE =&gt; [1303668804000, 1303668904000]} hbase&gt; scan 't1', {REVERSED =&gt; true} hbase&gt; scan 't1', {ALL_METRICS =&gt; true} hbase&gt; scan 't1', {METRICS =&gt; ['RPC_RETRIES', 'ROWS_FILTERED']} hbase&gt; scan 't1', {ROWPREFIXFILTER =&gt; 'row2', FILTER =&gt; &quot; (QualifierFilter (&gt;=, 'binary:xyz')) AND (TimestampsFilter ( 123, 456))&quot;} hbase&gt; scan 't1', {FILTER =&gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)} hbase&gt; scan 't1', {CONSISTENCY =&gt; 'TIMELINE'} hbase&gt; scan 't1', {ISOLATION_LEVEL =&gt; 'READ_UNCOMMITTED'} hbase&gt; scan 't1', {MAX_RESULT_SIZE =&gt; 123456} 123456789101112131415hbase(main):001:0&gt; scan 'bigdata:student'ROW COLUMN+CELL 1001 column=info:name, timestamp=1717580289267, value=wangwu 1002 column=info:name, timestamp=1717580320927, value=zhaoliu 1003 column=info:age, timestamp=1717580366636, value=10 1003 column=info:sex, timestamp=1717581149533, value=male3 row(s)Took 0.0338 secondshbase(main):025:0&gt; scan 'bigdata:student', {STARTROW =&gt; '1001', STOPROW =&gt; '1003'}ROW COLUMN+CELL 1001 column=info:name, timestamp=1717580289267, value=wangwu 1002 column=info:name, timestamp=1717580320927, value=zhaoliu2 row(s)Took 0.0118 seconds 删除数据 delete/deleteall 删除数据的方式有两个：delete和deleteall delete 表示删除一个版本的数据，即为 1 个 Cell，不填写版本默认删除最新的一个版本。 deleteall 表示删除所有版本的数据，即为当前行当前列的多个 Cell。执行命令会标记数据为要删除，不会直接彻底删除，删除只在特定时期清理磁盘时进行。 ① delete 123456789101112131415161718192021222324252627hbase(main):001:0&gt; put 'bigdata:student', '1001', 'info:name', 'zhangsan'Took 0.3910 secondshbase(main):002:0&gt; put 'bigdata:student', '1001', 'info:name', 'lisi'Took 0.2024 secondshbase(main):003:0&gt; put 'bigdata:student', '1001', 'info:name', 'wangwu'Took 0.1559 secondshbase(main):004:0&gt; scan 'bigdata:student'ROW COLUMN+CELL 1001 column=info:name, timestamp=1717584831277, value=wangwu 1002 column=info:name, timestamp=1717580320927, value=zhaoliu 1003 column=info:age, timestamp=1717580366636, value=10 1003 column=info:sex, timestamp=1717581149533, value=male3 row(s)Took 0.0083 secondshbase(main):005:0&gt; delete 'bigdata:student', '1001', 'info:name'Took 0.0055 secondshbase(main):006:0&gt; scan 'bigdata:student'ROW COLUMN+CELL 1001 column=info:name, timestamp=1717584831277, value=lisi 1002 column=info:name, timestamp=1717580320927, value=zhaoliu 1003 column=info:age, timestamp=1717580366636, value=10 1003 column=info:sex, timestamp=1717581149533, value=male3 row(s)Took 0.0087 seconds ② deleteall 1 2.2.2 API操作根据官方 API 介绍，HBase 的客户端连接由 ConnectionFactory 类来创建，用户使用完成之后需要手动关闭连接。同时连接是一个重量级的，推荐一个进程使用一个连接。对 HBase 的命令通过连接中的两个属性 Admin 和 Table 来实现。其中 Admin 主要管理 HBase 的元数据，如创建、修改表格信息，也就是 DDL 操作；Table 主要用于表格的增加、删除数据，也就是 DML 操作。 环境搭建 使用 IDEA 创建 Maven 项目，并修改 pom.xml 文件，添加 HBase 所需要用到的依赖。 1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.4.11&lt;/version&gt; &lt;!-- 如果报错, 需要排除 javax.el 拓展 --&gt; &lt;!-- 因为 2.4.11 对应的是一个测试版本的 javax.el 包 --&gt; &lt;!-- 需要先排除这个包后再添加正式版的 javax.el 包 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- 添加正式版的 javax.el 包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;version&gt;3.0.1-b06&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 单线程使用连接 下面展示了一种单线程使用连接的方式，实际开发中实际上很少这样做。 123456789101112131415161718192021222324252627282930313233package com.sdutcm;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.AsyncConnection;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.CompletableFuture;public class HBaseConnection { public static void main(String[] args) throws IOException { // 1. 创建连接配置对象 Configuration conf = new Configuration(); // 2. 添加配置参数 conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;bigdata&quot;); // 这些配置都写在 hbase-site.xml 中 // 3. 创建连接 // 默认创建同步连接 Connection connection = ConnectionFactory.createConnection(conf); // 也可以创建异步连接: 不推荐使用异步连接 CompletableFuture&lt;AsyncConnection&gt; asyncConnection = ConnectionFactory.createAsyncConnection(conf); // 4. 使用连接 System.out.println(connection); // 5. 关闭连接 connection.close(); }} 多线程使用连接 实际开发中，因为 HBase 的连接是重量级的，所以我们在每个客户端中一般只创建一个（类似于单例模式）。所以我们对代码进行修改，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.sdutcm;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.AsyncConnection;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.CompletableFuture;public class HBaseConnection { // 声明一个静态属性 public static Connection connection = null; static { // 1. 创建连接配置对象: 当完成 resources 目录的配置后, 我们可以直接注释掉创建配置的部分 // 直接进行创建连接操作 // Configuration conf = new Configuration(); // 2. 添加配置参数 // 实际开发中, 不应该在代码中显式的写参数, 而是将参数写在 resources 下的配置文件中 // 将虚拟机的 hbase-site.xml 放到 resources 目录下 // conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;bigdata&quot;); // 这些配置都写在 hbase-site.xml 中 // 3. 创建连接 // 默认创建同步连接 try { // 这里修改为无参构造 // connection = ConnectionFactory.createConnection(conf); // 这里通过查看 ConnectionFactory.createConnection() -&gt; 查看 create() -&gt; 可以发现 HBase 官方文档添加了两个配置文件 // 分别为 hbase-default.xml 和 hbase-site.xml // 所以我们可以直接复制虚拟机的 hbase-site.xml 添加到 resources 目录下, 并且将这里改为无参构造 // 无参则默认使用读取本地 hbase-site.xml 文件的方式添加参数 connection = ConnectionFactory.createConnection(); } catch (IOException e) { e.printStackTrace(); } } // 关闭连接方式 public static void closeConnection() throws IOException { // 判断连接是否为空 if (connection != null) { connection.close(); } } public static void main(String[] args) throws IOException { // 直接使用创建好的连接, 不要在 main 线程里面单独创建连接 System.out.println(HBaseConnection.connection); // 使用完连接后需要关闭连接 HBaseConnection.closeConnection(); }} 获取 Admin 123// 获取 Admin// Admin 的连接式轻量级的, 不是线程安全的, 不推荐池化或者缓存这个连接Admin admin = connection.getAdmin(); 创建命名空间 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.sdutcm;import org.apache.hadoop.hbase.NamespaceDescriptor;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import java.io.IOException;public class HBaseDDL { // 声明一个静态属性, 这样我们可以在不同的类中, 调用到同一个对象 public static Connection connection = HBaseConnection.connection; /** * @brief 创建命名空间 * @param namespace 命名空间名称 */ public static void createNamespace(String namespace) throws IOException { // 1. 获取 Admin // Admin 的连接式轻量级的, 不是线程安全的, 不推荐池化或者缓存这个连接 Admin admin = connection.getAdmin(); // 2. 调用方法创建命名空间 // 2.1 创建命名空间描述 NamespaceDescriptor.Builder builder = NamespaceDescriptor.create(namespace); // 2.2 给命名空间添加需求 builder.addConfiguration(&quot;user&quot;, &quot;sdutcm&quot;); // 2.3 使用 builder 构造出对应的添加完参数的对象, 完成创建 admin.createNamespace(builder.build()); // 关闭 admin admin.close(); } public static void main(String[] args) throws IOException { // 测试创建命名空间 createNamespace(&quot;sdutcm&quot;); // 其他代码 System.out.println(&quot;其他代码&quot;); // 关闭 HBase 连接 HBaseConnection.closeConnection(); }} ​ 结果如下： 12345678910111213hbase(main):001:0&gt; list_namespaceNAMESPACEdefaulthbasesdutcm &lt;&lt;&lt; 可以看到 sdutcm 已经被创建出来了3 row(s)Took 8.0120 secondshbase(main):002:0&gt; describe_namespace &quot;sdutcm&quot;DESCRIPTION{NAME =&gt; 'sdutcm', user =&gt; 'sdutcm'} &lt;&lt;&lt; 这里是我们添加的描述Took 0.7576 seconds=&gt; 1 多异常处理 1 判断表格是否存在 创建表格 📕 3. 底层原理3.1 进程架构3.1.1 Master架构3.1.2 RegionServer架构3.2 写流程3.2.1 写入顺序3.2.2 刷新机制3.3 读流程3.3.1 读取顺序3.3.2 合并数据优化3.4 文件合并3.4.1 大合并3.4.2 小合并Region拆分自定义预分区系统拆分🔧 企业开发TSDB模式基础表格模式自定义API整合框架Phoenix 读写数据Hive 分析数据","link":"/2024/06/02/QuickPassHBase/"},{"title":"ScalaQuickIN","text":"快速入门 Scala[TOC] 1. 学习目标 2. Scala 介绍2.1 Scala 简介Scala 创始人为 Martin Odersky，马丁·奥德斯基。JDK5 和 JDK8 版本的Java编译器 javac 都是由他和他的团队编写的。 2.2 Scala 的六大特征一句话总结：Scala 是一门以 JVM 为运行环境的静态类型编程语言，具备面向对象及函数是编程的特性。下面是官网对Scala特性的介绍： ① SEAMLESS JAVA INTEROP 无缝与 JAVA互操作 Java 和 Scala 可以混编，在写 Scala 代码时可以引用 Java 的类。 Scala 运行在 JVM 上，所以 Java 和 Scala 可以自由的混合使用。这也就是在日常编写代码的过程中，经常看到Scala引用了很多Java包的原因。 ② TYPE INFERENCE 类型推断（自动推测类型） 使类型系统不是那么的死板（静态）。不要为了类型系统工作，而是让类型系统为你工作。 Scala是一种弱类型语言，即它在编写代码时，不需要像强类型语言（如 Java、C++等）定义变量的类别。 Scala有Int、Long、Short、Byte、Float、Double、Char、Boolean这些基础数据类型。但与 Java 不同，在定义变量时，Scala用 var/val 来定义，让其自主判断数据类型。 其中val是常量，var是变量，一般来说，用val比较多（涉及资源的利用）。 12345// Java 中定义一个变量String name = &quot;ZhangSan&quot;;// Scala 中定义一个变量var name = &quot;ZhangSan&quot;; ③ CONCURRENCY &amp; DISTRIBUTION 并发和分布式（Actor） Scala 在操作集合时使用了数据并行操作。并且使用Actor（Actor是一种不共享数据，依赖于消息传递的并发编程模式， 有效的避免了死锁、资源争夺等情况）来解决并发和分布式中的一些问题。或者使用 futures 类进行异步编程。 使用 Scala 进行并发以及分布式场景下的开发时有得天独厚的优势。 ④ TRAITS 特性 将 Java 风格接口的灵活性与类的强大功能相结合。认为是一种多重继承。 其实这就相当于 Java 的接口，但实际上它比接口还功能强大。 与接口不同的是，它还可以定义属性和方法的实现。 简单来说：TRAITS = Interface + Abstract。 ⑤ PATTERN MATCHING 模式匹配 更强大的 Switch，它可以匹配类的层级结构、序列以及更多。 ⑥ HIGHER-ORDER FUNCTIONS 高阶函数 函数是一级对象。在保证类型安全的情况下编写它们。在任何地方使用它们，传递给任何东西，即函数可以作为参数传递到函数中。 2.3 具体应用 Kafka：分布式消息队列，内部代码经常用来处理并发的问题，用 Scala 可以大大简化其代码。 Spark：方便处理多线程场景，另外Spark主要用作内存计算，经常要用来实现复杂的算法，利用Scala这种函数式编程语言可以大大简化代码。 3. Scala 的安装和使用3.1 Windows 下安装 ScalaScala下载地址：The Scala Programming Language (scala-lang.org) ① 进入下载地址后，选择合适的Scala版本进行安装。记住安装的路径，方便配置环境变量。 ② 配置环境变量 SCALA_HOME ③ 验证是否安装成功：进入命令行CMD，输入 scala -version，显示版本号及表明配置成功。 3.2 IDEA 2022 中配置 Scala 插件① 进入IDEA，Files→Settings→Plugins ② 进入Marketolace界面，直接搜索Scala，安装即可。 ③ 在编写 Scala 代码时，双击 Shift，选择 Add Framework 后，添加 Scala 框架，即可编写 Scala 代码。 4. Scala 基础4.1 数据类型 数据类型 描述 Byte 8-bit 的有符号数字，范围在 -128 ~ 127 Short 16-bit 的有符号数字，范围在 -32768 ~ 32767 Int 32-bit 的有符号数字，范围在 -2147483648 ~ 2147483647 Long 64-bit 的有符号数字，范围在 -9223372036854775808 ~ 9223372036854775807 Float 32-bit IEEE 754 单精度浮点数 Double 64-bit IEEE 754 双精度浮点数 Char 16-bit Unicode 字符，范围 U+0000 ~ U+FFFF String 字符串 Boolean 布尔类型 Unit 表示空值，与其他语言中的 void 相同 Null 空值或者空引用 Nothing 所有其他类型的子类型，表示没有值 Any 所有类型的超类，任何类型都属于Any AnyRef 所有引用类型的超类 AnyVal 所有值类型的超类 Nil 表示长度为0的List 比较特殊的 None，是Option的两个子类之一，另一个是Some，用于安全的函数返回值。 Scala 推荐在可能返回空的方法使用 Option[X] 作为返回类型。如果有值就返回 Some[X]，否则返回 None。 123456def get(key: A): option[B] = { if (contains(key)) Some(getValue(key)) else None} 4.2 变量和常量的声明 变量用 var 定义，可修改 常量用 val 定义，不可修改 定义变量或常量的时候，也可以写上返回值的类型，一般省略，如 val a: Int = 10 常量不可以再赋值 1234567var name = &quot;zhangsan&quot;println(name) // zhangsanname = &quot;lisi&quot;println(name) // lisival gender = &quot;m&quot;gender = &quot;m&quot; // [Error] 不可以给常量赋值 4.3 键盘标准输入编程中可以通过键盘输入语句来接收用户输入的数据（也就是 Java 中的 Scanner 对象）。在 Scala 中只需要导入对应的包，比 Java 还要简单，不需要实例化对象。 1234567891011import scala.io.StdInobject Test { def main(args: Array[String]): Unit = { println(&quot;Please Input Your Name: &quot;) val name = StdIn.readLine() println(&quot;Please Input Your Age: &quot;) val age = StdIn.readInt() printf(&quot;Your Name is %s, Your Age is %d&quot;, name, age) }} 如上述代码一样，printf() 的用法和 Java 中一样，为格式化输出。注意使用规范即可。 符号 含义 %d 十进制数字 %s 字符串 %c 字符 %e 指数浮点数 %f 浮点数 4.4 类Class 和 对象Object 在编写 Scala 代码时，一般不加分号;，如果一行中有多条语句，则可以用分号隔开，如：var a = 10; var b = 20。 class 默认实现了 getter/setter 方法。 class 中如果有参数传入，那么这个构造器就是这个类的默认构造器。 class 在被 new 新建对象的时候，除了方法内部不执行，其他地方的代码都会执行，类似于Java中的工具类。 Object 里面不能传递参数，Object 里面的属性和方法都是静态的，类似于 Java 中 static 修饰的东西，类似于 Java 中的工具类。 伴生类和伴生对象，在一个 Scala 文件中，如果 Class 和 Object 的名字一样，则互为伴生类和伴生对象。他们可以互相访问到互相的私有成员变量。 123456789101112131415161718192021222324252627282930class Person(xname:String, xage: Int) { val name = xname val age = xage var money = 100 /** * 重写构造器, 必须调用类的默认构造器 * @param xname * @param xage * @param xmoney */ def this(xname: String, xage: Int, xmoney: Int) { this(xname, xage) money = xmoney } println(&quot;Checkpoint&quot;) // `class` 在被 `new` 新建对象的时候，除了方法内部不执行，其他地方的代码都会执行 def test: Unit = { println(&quot;Inside Function&quot;) // 方法内部不会执行 } println(&quot;Checkpoint&quot;) // `class` 在被 `new` 新建对象的时候，除了方法内部不执行，其他地方的代码都会执行}object Test { def main(args: Array[String]): Unit = { val person1 = new Person(&quot;zhangsan&quot;, 20) val person2 = new Person(&quot;lisi&quot;, 20, 1000) println(person.name + &quot;:&quot; + person.age) // 默认实现了 getter/setter 方法 }} 4.5 IF-ELSE 语句Scala 中的条件判断语句同 Java 中的条件判断语句，语法结构基本一致。 123456789101112131415import scala.io.StdInobject IfDemo { def main(args: Array[String]): Unit = { println(&quot;请输入年龄&quot;) val age = StdIn.readInt() if (age &gt;= 18 &amp;&amp; age &lt;= 100) { println(&quot;成年&quot;) } else if (age &gt;= 0 &amp;&amp; age &lt; 18) { println(&quot;未成年&quot;) } else { println(&quot;请输入 0 ~ 100 的数字&quot;) } }} 4.6 Loop 循环语句 to 和 until 语句的区别 1234567object ToUntilDemo { def main(args: Array[String]): Unit = { println(1 to 10) // to 表示 [1, 10], 输出: Range(1, 2, 3, ..., 10) println(1 until 10) // until 表示 [1, 10), 输出: Range(1, 2, 3, ..., 9) println(1 to (10, 2)) // 表示按照步长为`2`来输出数据, 输出: Range(1, 3, 5, 7, 9) }} for 循环 12345678910111213141516171819202122232425262728293031object ForDemo { def main(args: Array[String]): Unit = { for (i &lt;- 1 to 10) { print(i + ' ') // 输出: 1 2 3 4 5 6 7 8 9 10 } // Scala 中的 `for` 循环可以将 if 语句直接写在 for 循环中 for (i &lt;- 1 to 10; if i &gt; 5; if i % 2 == 0) { printf(&quot;%d &quot;, i) // 输出: 6 8 10 } // 上面的写法可以等同于下面的写法 for (i &lt;- 1 to 10) { if (i &gt; 5 &amp;&amp; i % 2 == 0) { printf(&quot;%d &quot;, i) } } // 双重 for 循环 for (i &lt;- 1 to 10; j &lt;- 1 to 5) { println(i + &quot;:&quot; + j) } // 上面的写法可以等同于下面的写法 for (i &lt;- 1 to 10) { for (j &lt;- 1 to 5) { println(i + &quot;:&quot; + j) } } }} Scala 中不能使用类似于 x++、x-- 的操作，需要使用 x += 1 或者 x -= 1 来完成自增或者自减操作。 for 循环使用 yield 关键字返回一个集合，for { 子句 } yield {变量或表达式}，for 循环中的 yield 会把当前的元素记下来，保存在集合中，循环结束后将返回该集合。Scala 中 for 循环是有返回值的。如果被循环的是 Map，返回的就是 Map，被循环的是 List，返回的就是 List，以此类推。 12345678910111213object Loop { def main (args: Array[String]): Unit = { val range = 1 to 10; for (num &lt;- range if num % 2 == 0 if num &gt; 5) { printf(&quot;%d&quot;, num); // 6 8 10 } val result_1 = for (num&lt;-range if num % 2 == 0 if num &gt; 5) yield num // yield 变量 println(result_1) // Vector(6, 8, 10) val result_2 = for (num&lt;-range if num &gt; 5) yield num % 2 // yield 表达式 println(result_2) // Vector(0, 1, 0, 1, 0) }} while 和 do...while 1234567891011121314object Loop { def main (args: Array[String]): Unit = { var index = 0 while (index &lt; 10) { println(&quot;第&quot; + index + &quot;次循环&quot;) index += 1 } do { index += 1 println(&quot;第&quot; + index + &quot;次循环&quot;) } while(index &lt; 20) }} 5. Scala 函数(方法)5.1 函数的定义12345678// 函数(方法) 的定义// def 函数名(参数x: 参数类型, 参数y: 参数类型): 返回类型 = {// 函数体// }def function_name(x: Int, y: String): Unit = { function_body} 注意事项： Scala 使用 def 关键字告诉编译器这是一个函数（方法） 我们可以通过在参数列表后面加一个冒号:和类型来显式地指定返回类型。 函数可以写返回类型，也可以不写，会自动推断（最后一行是什么类型，就被推断成什么类型）。有时候不能省略，必须写，比如在递归函数中或者函数的返回值是函数类型的时候。 Scala 中函数有返回值时，可以写 return，也可以不写 return，不写 return 时会把函数中最后一行当做结果返回。当写 return 时，必须要写函数的返回类型。 传递给方法的参数可以在方法中使用，并且 Scala 规定：方法的传过来的参数为常量 val 而不是变量 var 。 如果去掉函数体前面的等号=，那么这个函数返回类型必定是 Unit。这种说法无论函数体里面什么逻辑都成立，Scala 可以把任意类型转换为 Unit。假设，函数里面的逻辑最后返回了一个 String，那么这个返回值会被转换成 Unit，原本逻辑的值会被丢弃。这种方法往往适用于无返回值的函数中。 5.2 递归函数 12345678910111213141516object FunctionDemo { /** * 递归函数: * 关键点在于递归的定义, 终止条件(避免无休止的递归, 导致栈溢出问题) */ def f1(num: Int): Int = { if (num == 1) { return num } num * f1(num-1) // f1(5) = 5*f1(4) = 5*4*f1(3) = 5*4*3*f1(2) = 5*4*3*2*f1(1) } def main (args: Array[String]): Unit = { println(f1(5)) }} 5.3 包含参数默认值的函数 和其他语言没有任何区别 默认值的函数中，如果传入的参数个数与函数定义相同，则传入的数值会覆盖默认值。 如果不想覆盖默认值，且传入的参数个数小于定义的函数的参数，则需要指定参数名称。 123456789101112131415object FunctionDemo { /** * 包含参数默认值的函数 */ def f2(x: Int=5, y: Int=10): Int = { a + b } def main (args: Array[String]): Unit = { println(f2()) // 输出: 15 println(f2(10, 20)) // 输出: 30 println(f2(10)) // 输出: 20 println(f2(y=30)) // 输出: 35 }} 5.4 可变参数个数的函数123456789101112131415161718object FunctionDemo { /** * 可变参数个数的函数 * 传入多个参数时, 多个参数之间用逗号分隔 * 传入的参数其实就是不定长数组 */ def f3(elements: Int*): Int = { var sum = 0 for (element &lt;- elements) { sum += element } sum } def main (args: Array[String]): Unit = { println(f3(1, 2, 3, 4, 5)) // 输出 }} 5.5 匿名函数匿名函数有以下几种： 有参匿名函数 无参匿名函数 有返回值的匿名函数 在定义和使用匿名函数时： 可以将匿名函数返回给 val 定义的值 匿名函数不能显示声明函数的返回类型 在 Scala 中 ，大多数情况下 =&gt; 是匿名函数的显著标志。 123456789101112131415161718192021222324252627object FunctionDemo { /** * 匿名函数: 经常和高阶函数一起使用 * 1. 有参匿名函数 * 2. 无参匿名函数 * 3. 有返回值的匿名函数 */ def main (args: Array[String]): Unit = { // 有参数匿名函数 val value1 = (a: Int) =&gt; { println(a) } value1(1) // 无参数匿名函数 val value2 = () =&gt; { println(&quot;无参数匿名函数&quot;) } value2() // 有返回值的匿名函数 val value3 = (a: Int, b: Int) =&gt; { a + b } println(value3(4, 4)) }} 5.6 嵌套函数嵌套函数其实就是函数里套了函数。 1234567891011121314151617object FunctionDemo { def f5 (num: Int): Int = { @tailrec def f6(a: Int, b: Int): Int = { if (a == 1) { b } else { f6(a-1, a*b) } } f6(num, 1) } def main (args: Array[String]): Unit = { f5(5) }} 5.7 偏应用函数偏应用函数是一种表达式，不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。 12345678910111213141516171819202122object FunctionDemo { def main (args: Array[String]): Unit = { // 这里只是一个普通的函数 def log(date: Date, log: String) { println(&quot;Date is &quot; + date + &quot;, Log is &quot; + log) } // 按照普通函数的方法使用 val date = new Date() // 与 Java 混编 log(date, &quot;log1&quot;) log(date, &quot;log2&quot;) log(date, &quot;log3&quot;) // 我们发现, 上面的程序除了 log 参数在改变, date 参数没有变化 // 此时我们可以使用偏应用函数来优化 // _ 下划线可以理解为一个变化的参数, 而 date 可以理解为一个固定的参数(他本身可能是变化的) val logWithDate = log(date, _:String) logWithDate(&quot;log_11&quot;) logWithDate(&quot;log_12&quot;) logWithDate(&quot;log_13&quot;) }} 5.8 高阶函数高阶函数：函数的参数是函数，或者函数的返回类型是函数，或者函数的参数和函数的返回类型是函数的函数。 函数的格式为 (A)=&gt;B, 后面没有函数体, 此函数接收类型 A 的参数, 返回类型 B 的函数。 函数的参数是函数：其实就是定义一个传入参数和返回类型的模板参数，但是这个模板需要从其他地方实现。 1234567891011121314151617// 函数的参数是函数def f7(a: Int, func: (Int, Int) =&gt; Int): Int { val result = func(1, 2) // 这里将函数作为参数传入 f7, 而 result 使用了这个函数 // 这里我理解为模板, result 使用了一个 参数为(Int, Int), 返回类型为 Int 的函数模板 // 那么这个模板的具体实现是什么, 则需要从其他地方实现 a * result}def f8(x: Int, y: Int): Int = { // 这里就是对于模板的一种实现 x + y}println(f7(5, f8)) // 使用这种方式将 f8 传入, 并且打印输出, 结果为: 15// 当然我们也可以结合匿名函数一起实现println(f7(5, (x: Int, y: Int) =&gt; {x + y})) 函数的返回类型是函数 12345678910// 函数的返回类型是函数def f9(a: Int, b: Int): (String, String) =&gt; String = { def f10(c: String, d: String): String = { a + &quot; &quot; + b + &quot; &quot; + c + &quot; &quot; + d } f10 // 返回一个函数}// 像这种括号连着括号的`()()`, 往往代表着出现了函数的返回类型是函数的函数println(f9(1, 2)(&quot;3&quot;, &quot;4&quot;)) // 输出: 1 2 3 4 函数的参数和函数的返回类型是函数 12345678def f11(x: Int, f: (Int, Int) =&gt; Int): (Int, Int) =&gt; Int = { f}println(f11(1, (a: Int, b: Int) =&gt; { a + b } ) (100, 200)) // 输出: 300// _ 类似于 Java 中的 *. 通配符, 变量只使用一次的时候可以简写如下:println(f11(1, (_ + _))(100, 200)) // 输出: 300 5.9 柯里化函数柯里化函数，或称颗粒化函数，将参数变成颗粒散落简而言之就是将参数不断拆分。柯里化函数基本是在做这么一件事情：只传递给函数一部分参数来调用它，让它返回一个函数去处理剩下的参数。如果写成公式文字就是这样： 1234fn(a, b, c, d) =&gt; fn(a)(b)(c)(d)fn(a, b, c, d) =&gt; fn(a)(b, c, d)fn(a, b, c, d) =&gt; fn(a, b)(c)(d)fn(a, b, c, d) =&gt; fn(a, b, c)(d) 可以理解为高阶函数的简化，类似于返回类型为函数的函数。 12345678def f12(a: Int, b: Int, c: Int, d: Int) = { a + b + c + d}// 柯里化函数: 可以理解为高阶函数的简化def f13(a: Int, b: Int)(c: Int, d: Int) = { a + b + c + d } 6. Scala 字符串Scala 中的字符串和 Java 中的字符串用法几乎完全相同。 String 和 StringBuilder的区别：String 不可修改，StringBuilder 可修改。 6.1 String123456789101112package com.szy.inspur.subowen.rdd.baseobject StringDemo { def main(args: Array[String]): Unit = { val str_1: String = &quot;aabbccdd&quot; println(str_1.indexOf(&quot;a&quot;)) // 输出: 0 println(str_1.indexOf(98)) // 输出: 2, 这里输入的整数是 ASCII 码 val str_2: String = &quot;AABBCCDD&quot; println(str_1 == str_2) // 输出: false println(str_1.compareToIgnoreCase(str_2)) // 输出: 0, 如果 str != str_2, 则输出 `-1` }} 6.2 StringBuilder1234567891011121314151617181920212223package com.szy.inspur.subowen.rdd.baseimport scala.collection.mutableobject StringDemo { def main(args: Array[String]): Unit = { val stringBuilder = new mutable.StringBuilder stringBuilder.append(&quot;abc&quot;) println(stringBuilder) val result = 1 .+ (2) // Scala 中的运算操作其实都是调用函数 // 因此, `result 1 .+ (2)` 其实和 result = 1 + 2 一样 // StringBuilder 可以 `+=` Char 类型, 而不能 `+=` String 类型 // 普通的 `+` 只有 Char 类型追加在后面 stringBuilder ++= &quot;c&quot;; println(stringBuilder) // 输出: abcc stringBuilder += 'd'; println(stringBuilder) // 输出: abccd // stringBuilder += &quot;e&quot;; println(stringBuilder) // [ERROR] // stringBuilder ++= 'f'; println(stringBuilder) // [ERROR] stringBuilder + 'e'; println(stringBuilder) // 输出: abccde stringBuilder + &quot;f&quot;; println(stringBuilder) // 输出: abccde, 可以看到 &quot;f&quot; 没有被添加到字符串后面 }} 6.3 String 的操作方法万金油append，append()不受类型的限制。 7. 集合7.1 数组7.1.1 创建数组12345678910111213141516171819202122232425262728293031323334/** * 创建数组的两种方式 * 1. new Array[String](3) * 2. 直接 Array */object ArrayDemo { def main(args: Array[String]): Unit = { // 方式 1 // 创建一个 Array val array = Array(1, 2, 3) println(array(0)) // 用 for 循环遍历一个 Array for(x &lt;- array) { println(x) } // 用 foreach 遍历一个 array // foreach( function ) array.foreach(i =&gt; {println(i)} ) array.foreach(println(_)) array.foreach(println) println() // 方式 2 val array_use_length = new Array[Int](3) array_use_length.foreach(i =&gt; { println(i) }) // 0, 0, 0 println(array_use_length.isEmpty) // 输出: false println(array_use_length.length) // 输出: 3 }} 7.2 集合 Set7.3 集合 Map7.4 元组 Tuple7.4.1 元组的定义","link":"/2024/06/10/ScalaQuickIN/"},{"title":"OnlineTravelBigdataPlatform","text":"🌳 在线旅游大数据平台项目 完成度： 100% 日期任务清单 项目开始日期: 2024-06-11 了解项目的背景以及整个系统的架构 了解系统需要完成的主要功能 了解系统整个架构 完成数据服务端的部署 完成数据客户端的部署 了解数据集 认识消息队列 Kafka 完成消息队列 Kafka 的部署 了解消息队列 Kafka 的基本应用 使用 Flume 收集数据到 Kafka 2024-06-12 了解实时数据分析所用到的技术 了解 SparkStreaming 和 Flink 了解 SparkStreaming 的核心概念 了解数据源 借助netcat实践Kafka 了解转换操作 具体实施任务：处理数据 2024-06-13 了解数据库连接池 了解如何向 MySQL 数据库中写入数据 借助 alibaba Druid 库实现一个数据库连接工具类 编写案例：WordCount 具体实施任务，将代码中生成的数据写入MySQL。 2024-06-14 了解什么是 Kafka Offset 维护 Kafka Offset 具体实施任务，将 Kafka 的 Offset 配合 MySQL 用代码进行维护。 2024-06-17 进行后端开发 进行前端开发 2024-06-18 进行热力图的绘制 进行人流量柱状图的绘制 进行人流量趋势图的绘制 2024-06-19 项目结束日期 文件目录 [TOC] 📕 1. 项目概述1.1 项目背景随着信息技术的飞速发展，旅游行业正迅速融入数字化转型的浪潮中。旅游大数据的产生和积累为行业提供了前所未有的洞察力。然而，传统的数据处理方法往往难以应对数据的海量性和实时性需求。 1.2 项目介绍本项目旨在构建一个旅游大数据实时分析和监控系统，系统主要包括旅游数据分析和实时监控两大模块，旅游数据分析模块是基于Spark Streaming对济南各景点的人流量数据进行实时处理和分析，实时监控模块是基于SpringBoot对分析的结果进行可视化展示。 🚧 2. 系统功能及架构2.1 系统主要功能 数据实时收集：通过 Flume 实时采集手机移动信令数据（数据生成器生成的模拟数据），发送到 Kafka。 数据实时处理分析：通过Spark Streaming 消费 Kafka 数据，主要完成以下分析： 各景点人流量实时统计（热力图，每秒钟） 各景点人流量随时间增长情况/各景点人流量随时间变化趋势(每分钟) 实时监控：通过 SpringBoot + MyBatis 构建旅游监控系统，基于高德地图完成每秒钟人流量热力图展示，基于 Echarts 完成每分钟流量柱状图和每分钟人流量变化折线图。 2.2 系统结构与技术选型 项目开发工具：IntelliJ IDEA 2019 数据收集分析：Flume + Kafka + SparkStreaming + MySQL/Redis 数据展示：SpringBoot + MyBatis + WebSocket + MySQL + LayUI + Echarts + 高德地图API 🔧 3.项目收集功能3.1 数据服务端与数据客户端部署我的主机信息如下： 123192.168.26.110 bigdata192.168.26.111 webserver01192.168.26.111 webserver02 3.1.1 数据服务端部署 将 logweb-1.0.jar 上传到服务器webserver01 以及 webserver02。 启动运行 logweb 程序 1nohup java -jar logweb-1.0.jar &amp; 运行成功后，查询日志文件结果，结果如下表示正常启动： 1234567891011121314151617181920 . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.7.10)2024-06-11 12:10:18.084 INFO 1288 --- [ main] c.s.i.w.s.web.logweb.LogwebApplication : Starting LogwebApplication v1.0 using Java 1.8.0_231 on webserver01 with PID 1288 (/home/subowen/serverJar/logweb-1.0.jar started by subowen in /home/subowen/serverJar)2024-06-11 12:10:18.090 INFO 1288 --- [ main] c.s.i.w.s.web.logweb.LogwebApplication : No active profile set, falling back to 1 default profile: &quot;default&quot;2024-06-11 12:10:20.604 INFO 1288 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 9527 (http)2024-06-11 12:10:20.666 INFO 1288 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2024-06-11 12:10:20.666 INFO 1288 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.73]2024-06-11 12:10:21.301 INFO 1288 --- [ main] o.a.c.c.C.[.[localhost].[/logweb] : Initializing Spring embedded WebApplicationContext2024-06-11 12:10:21.301 INFO 1288 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 3094 ms2024-06-11 12:10:23.099 INFO 1288 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 9527 (http) with context path '/logweb'2024-06-11 12:10:23.116 INFO 1288 --- [ main] c.s.i.w.s.web.logweb.LogwebApplication : Started LogwebApplication in 6.507 seconds (JVM running for 8.07)2024-06-11 12:14:14.608 INFO 1288 --- [nio-9527-exec-1] o.a.c.c.C.[.[localhost].[/logweb] : Initializing Spring DispatcherServlet 'dispatcherServlet'2024-06-11 12:14:14.608 INFO 1288 --- [nio-9527-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'2024-06-11 12:14:14.609 INFO 1288 --- [nio-9527-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 1 ms 3.1.2 数据客户端部署本次数据客户端直接部署在 Windosw 下，使用 IntelliJ IDEA 2019 进行开发。 IDEA 数据客户端结构如下： 修改 pom.xml 引入相应的包 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;travel_subowen423&lt;/artifactId&gt; &lt;groupId&gt;com.example.x.travel&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;logclient&lt;/artifactId&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- 配置 Spark 的版本 --&gt; &lt;spark.version&gt;3.0.1&lt;/spark.version&gt; &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.12.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;${httpclient.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 导入 logclient 代码，并且修改相应包名和部分代码 执行 ScenicAPP.java 随机生成数据。 3.1.3 数据格式说明 经度 纬度 景点名称 时间 117.024489 36.669213 曲水亭街 20240611155103 117.016089 36.661138 趵突泉 20240611155103 116.813744 36.541549 济南国际园博园 20240611155103 117.022959 36.668068 芙蓉街 20240611155103 117.034920 36.641749 千佛山 20240611155103 117.023837 36.674997 大明湖 20240611155103 117.023837 36.674997 大明湖 20240611155103 117.024489 36.669213 曲水亭街 20240611155103 117.016089 36.661138 趵突泉 20240611155103 117.021483 36.661473 济南泉城广场 20240611155103 … … … … 3.2 Kafka 消息队列3.2.1 Kafka 简介点击访问：Kafka官网 Apache Kafka 是一个开源分布式**事件(Event)**流平台，已被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用程序。 本项目主要使用发布（写入）和订阅（读取）事件流，包括从其他系统持续导入/导出数据；根据需要持久可靠地存储事件流；在事件发生时或回顾性地处理事件流。 3.2.2 Kafka 中的核心概念 名词 解释 Broker Kafka 集群包含一个或多个服务器，这些服务器称为 Broker Producer 生产者，负责将数据发送到 Kafka Consumer 消费者，负责从 Kafka 中读取数据 Consumer Group 消费者组，多个消费者组成的组 Topic 主题，每条发布到 Kafka 集群的消息都有一个类别，这个类别称为 Topic，可以理解为文件夹 Partition 分区，每个Topic包含一个或多个Partition 3.2.3 Kafka 部署 Kafka 的部署方式分为： 分布式部署（多节点多Broker） 单机部署（单节点单Broker/单节点多Broker） Kafka 是使用 Scala 编写的组件，依赖与Scala版本 Kafka 依赖于 ZooKeeper，必须要安装 ZooKeeper，再安装 Kafka 部署 Kafka 过程 解压 kafka 安装包 配置环境变量 修改配置 config/server.properties 文件 12345# 配置 Broker 的 ID, 在同一个集群上, 这个值必须是一个独一无二的整数值broker.id=0# 配置日志文件的路径log.dirs=/home/subowen/apps/kafka_2.12-2.8.0/kafka-logs 3.2.4 Kafka 的基本应用 启动 ZooKeeper 1[subowen@bigdata ~]$ zkServer.sh start 启动 Kafka 第一次启动：先使用前台启动，如果没有问题再使用后台启动 前台启动命令：kafka-server-start.sh $KAFKA_HOME/config/server.properties 后台启动命令：kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties 常用的 Kafka 命令 参考博客：Kafka 基础理论与常用命令详解 (超详细) Kafka常用命令和解释 - CSDN博客 Topics 常用命令 创建一个名为my_topic的主题： 1[subowen@bigdata ~]$ kafka-topics.sh --create --bootstrap-server bigdata:9092 --topic my_topic 查看名为my_topic的主题的详细信息： 1[subowen@bigdata ~]$ kafka-topics.sh --describe --bootstrap-server bigdata:9092 --topic my_topic Producer 常用命令 生产信息 1[subowen@bigdata ~]$ kafka-console-producer.sh --broker-list bigdata:9092 --topic my_topic Consumer 常用命令 3.2.5 使用 Flume 收集数据到 Kafka在之前的项目中，我学习了 离线类型 项目的 Flume 数据收集，因为本次需要进行在线的实时数据分析，所以按照之前的离线分析方式，使用 Flume 将数据收集到 HDFS 是不适合实时的数据分析环境的。将数据落地到 HDFS 则意味着数据进入磁盘，数据的读写会占用大量的磁盘 I/O，不适用于实时场景。因此在实时项目，考虑到数据的实时性，本次实时数据分析项目使用 消息队列（Kafka）进行数据的存放。 配置 webserver01 和 webserver02 上的 Flume 配置文件 taildir-avro-stream.conf 12345678910111213141516171819202122a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = TAILDIRa1.sources.r1.positionFile = /home/subowen/apps/apache-flume-1.9.0-bin/position/taildir_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /home/subowen/serverJar/logweb/logs/scenic.loga1.sources.r1.headers.f1.headerKey1 = inspur-szya1.sources.r1.fileHeader = truea1.sources.r1.maxBatchCount = 1000a1.sinks.k1.type = avroa1.sinks.k1.hostname = 192.168.26.110a1.sinks.k1.port = 4545a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels=c1a1.sinks.k1.channel=c1 配置 bigdata 上的 Flume 配置文件 avro-kafka-stream.conf 123456789101112131415161718192021222324252627282930313233343536# 配置 Agent a1各个组件的名称# Agent a1 的 source有一个, 叫做r1a1.sources = r1# Agent a1 的 sink也有一个, 叫做k1a1.sinks = k1# Agent a1 的 channel有一个, 叫做c1a1.channels = c1# 配置 Agent a1的source r1的属性a1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0# 监听的端口a1.sources.r1.port = 4545# 配置 Agent a1的sink k1的属性a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = subowena1.sinks.k1.kafka.bootstrap.servers = bigdata:9092a1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1a1.sinks.k1.kafka.producer.compression.type = snappy# 配置 Agent a1 的 channel c1 的属性, channel是用来缓冲Event数据的# channel 的类型是内存 channel, 顾名思义这个 channel 是使用内存来缓冲数据a1.channels.c1.type = memory# 内存channel的容量大小是1000, 注意这个容量不是越大越好, 配置越大, 一旦Flume 挂掉丢失的 event 也就越多a1.channels.c1.capacity = 1000# source 和 sink 从内存 channel 每次事务传输的event数量a1.channels.c1.transactionCapacity = 100# 把source和sink绑定到channel上# 与source r1绑定的channel有一个, 叫做c1a1.sources.r1.channels = c1# 与sink k1绑定的channel有一个, 叫做c1a1.sinks.k1.channel = c1 启动 webserver01 和 webserver02 的 Flume，这里为了简化操作，只启动一台机器上的 Flume 1[subowen@webserver01 config]$ ./start-flume.sh taildir-avro-stream.conf a1 启动 bigdata 上的 Flume 1[subowen@bigdata config]$ ./start-flume.sh avro-kafka-stream.conf a1 在 bigdata 上启动 **kafka-console-consumer**，使用消费者Shell进行测试消费，后期会更换为 SparkStreaming 进行消费。 1[subowen@bigdata ~]$ kafka-console-consumer.sh --topic subowen --bootstrap-server bigdata:9092 在 Windows 下运行客户端，模拟数据的产生，执行ScenicAPP代码，模拟数据的生成过程。 12345678910package com.example.x.data;import com.example.x.data.producer.DataProducer;public class ScenicApp { public static final String url = &quot;http://192.168.26.111:9527/logweb/upload&quot;; public static void main(String[] args) throws Exception{ DataProducer.producer(url); }} 📈 4. 数据实时分析Spark 3.0.1 官方文档入口：Overview - Spark 3.0.1 Documentation (apache.org) 4.1 SparkStreaming 概述SparkStreaming 官方文档入口：Spark Streaming - Spark 3.0.1 Documentation (apache.org) SparkStreaming 类似于之前学习的 SparkRDD 和SparkSQL，是 Spark API 的核心扩展，支持实时数据流的可扩展、高吞吐量和容错。数据可以从Kafka、Flume、Kinesis或TCP Socket 等许多来源中读取，并且可以使用复杂的算法进行处理，这些算法用高级函数（如 map、reduce、join 和 window）表示。最后，处理过的数据可以保存到文件系统、数据库和实时仪表板。事实上，您可以在数据流上应用 Spark 的机器学习和图形处理算法。 SparkStreaming 是 Spark 中用于处理实时数据的一个模块。 在内部，他的工作流程是：SparkStreaming 接收实时输入的数据流，并对数据进行分批（微批）处理，由 Spark 引擎进行处理，生成最终的批量结果流。 SparkStreaming 是微批处理（批次特别小，足以实现实时处理），不是真正的流处理。 这里也就可以更显著的得到离线批数据和实时数据之间的区别： 离线批数据：bound——有界的数据 实时数据：unbound——无界的数据 4.2 开发第一个SparkStreming案例类比 SparkRDD 开发的流程，我们给出SparkStreaming 开发的流程。 SparkRDD 编程模型： 创建 SparkContext 读取数据源 处理数据 输出结果 关闭 SparkContext SparkStreaming 编程模型： 创建 StreamingContext 读取数据 处理数据 输出结果 启动程序（阻塞） 等待程序关闭 由此，我们开发我们的第一个 SparkStreaming 实例。 在 IDEA 中添加 Maven 依赖 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 TcpStreamingAPP，编写代码 1234567891011121314151617181920212223242526272829303132333435package com.example.x.sparkstreamimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}import org.apache.spark.streaming.{Seconds, StreamingContext}object TcpStreamingAPP { def main(args: Array[String]): Unit = { // 第一步: 创建 StreamingContext val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;TcpStreaming&quot;) val streamingContext = new StreamingContext(conf, Seconds(1)) // 第二步: 读取数据: 接收 hostname:port 发送的数据 // SparkCore SparkContext RDD(弹性分布式数据集)——不可变的、可分区的、可并行计算 // SparkSQL SparkSession DataSet/DataFrame // SparkStreaming StreamingContext DStream val hostname = &quot;bigdata&quot; val port = 9999 var tcpDStream: ReceiverInputDStream[String] = streamingContext.socketTextStream(hostname, port) // 第三步: 处理数据 val wcDStream: DStream[(String, Int)] = tcpDStream.flatMap(_.split(&quot;,&quot;)) .map((_, 1)) .reduceByKey(_ + _) // 默认计算的是当前批次的数据, 不会与之前的批次进行累加操作 // 第四步: 累加结果 wcDStream.print() // 第五步: 启动程序 streamingContext.start() // 第六步: 等待程序关闭 streamingContext.awaitTermination() }} 在 bigdata 中安装和启动 netcat 生成数据 12[subowen@bigdata ~]$ sudo yum -y install nc[subowen@bigdata ~]$ nc –lk 9999 启动程序进行测试，需要注意的是：程序只计算当前时间点发送过来的数据（无状态）。 4.3 SparkStreaming 核心概念 概念 解释 StreamingContext SparkStreaming功能的主要入口点，它提供了用于从各种输入源创建 DStream 的方法。创建和转换DStream后，可以分别使用 start() 和 stop() 启动和停止流计算。awaitTermination() 等待执行停止，允许当前线程通过stop() 手动停止或通过一个异常等待StreamingContext的终止。 DStream DStream 是 SparkStreaming 提供的基本抽象，它表示一个连续的数据流，要么是从Source接收的输入数据流，要么是通过转换输入流生成的处理数据流。在内部，DStream由一系列连续的RDD表示，DStream中的每个RDD都包含一定时间间隔的数据。在DStream上应用的任何操作都转换为在底层 RDD上的操作。DStream中是由每个批次生成的RDD组成的。 Input DStreams 和 Receivers Input DStreams是表示从数据源接收的输入数据流的DStreams。每个Input DStream（文件流除外）都与一个Receiver对象相关联，接收来自源的数据并将其存储在Spark的内存中进行处理。 4.4 数据源数据源其实就是从哪里读取数据，区别就在于一些读取的写法有不同。 4.4.1 基本数据源 Socket套接字数据源：socketTextStream()。 File System文件系统数据源：textFileStream()。 4.4.2 高级数据源Spark Kafka官方文档：Spark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher) - Spark 3.0.1 Documentation (apache.org) 高级数据源：如 kafka 、Kinesis等。 用代码对接 Kafka 123456789101112131415161718192021222324252627282930313233343536373839package com.example.x.sparkstreamimport org.apache.spark.SparkConfimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.codehaus.jackson.map.deser.std.StringDeserializerobject KafkaStreamApp { def main(args: Array[String]): Unit = { // 第一步 // Spark 中遇到的一切序列化问题都需要 KryoSerializer val conf: SparkConf = new SparkConf().setMaster(&quot;Local[2]&quot;).setAppName(&quot;KafkaStreaming&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;subowen&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = &quot;subowen&quot; val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) // 指定订阅的 Topic ) kafkaStream.flatMap(x=&gt;x.value().split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _).print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 4.4.3 维护 Kafka Offset在前面的学习中，我们已经了解了两种消费者的消费策略，分别是 Lastest 和 Earliest，这两种消费策略都是存在问题的。 Lastest 消费策略：当消费者启动之后，从启动后产生的第一条数据开始消费 消费者第一次启动之前，topic中已经存在的数据是不会被消费。 消费者宕机的时间段内，topic中产生的数据不会被消费。 Earliest 消费策略： 如何手动维护Kafka Offset？ 我们可以将 Offset 持久化到一个数据库中，如MySQL、HDFS、ZooKeeper 中。下面将演示如何使用 MySQL 来维护 Kafka Offset。 设计表结构 字段名 约束 类型 备注 k_topic 联合主键 VARCHAR(50) 可以直接从代码中获得 k_groupid 联合主键 VARCHAR(50) 可以直接从代码中获得 k_partition 联合主键 INT - k_offset - BIGINT - 实现表结构 1234567CREATE TABLE IF NOT EXISTS t_offset ( k_topic VARCHAR(50) NOT NULL, -- 设置 Topic k_groupid VARCHAR (50) NOT NULL, -- 设置 groupid k_partition INT NOT NULL, -- 设置 Partition k_offset BIGINT NOT NULL, -- 设置 offset PRIMARY KEY(k_topic, k_groupid, k_partition) -- 设置联合主键); 开发工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.example.x.utilsimport java.sql.{Connection, PreparedStatement, ResultSet}import org.apache.kafka.common.TopicPartitionimport org.apache.spark.streaming.kafka010.OffsetRangeimport scala.collection.mutable.ListBufferobject KafkaOffsetManagerUtils { // 保存 Offset def saveOffset(offsetRanges: Array[OffsetRange], groupId: String) = { val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO t_offset(k_topic, k_groupid, k_partition, k_offset) VALUES(?, ?, ?, ?) |ON DUPLICATE KEY UPDATE k_offset=? |&quot;&quot;&quot;.stripMargin val pst: PreparedStatement = connection.prepareStatement(sql) offsetRanges.map(offsetRanges =&gt; { val topic = offsetRanges.topic val partition = offsetRanges.partition val offset = offsetRanges.untilOffset pst.setString(1, topic) pst.setString(2, groupId) pst.setInt(3, partition) pst.setLong(4, offset) pst.setLong(5, offset) pst.execute() }) ConnectionUtils.closeConnection(connection) } // 读 Offset def readOffset(topic: String, groupId: String): Map[TopicPartition, Long] = { val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;SELECT k_topic, k_groupid, k_partition, k_offset FROM t_offset WHERE k_topic=? AND k_groupid=?&quot; val pst: PreparedStatement = connection.prepareStatement(sql) pst.setString(1, topic) pst.setString(2, groupId) val resultSet: ResultSet = pst.executeQuery() val list = new ListBuffer[(TopicPartition, Long)] while(resultSet.next()) { val partition = resultSet.getInt(&quot;k_partition&quot;) val topicPartition = new TopicPartition(topic, partition) val offset = resultSet.getLong(&quot;k_offset&quot;) list.append((topicPartition, offset)) } list.toMap }} 代码案例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.example.x.quickstartimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.{ConnectionUtils, KafkaOffsetManagerUtils}import org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.{SparkConf, TaskContext}import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.{Seconds, StreamingContext}object KafkaOffset { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;WordCount&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val topics = Array(&quot;KafkaOffset&quot;) val groupId = &quot;szy&quot; val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; groupId, &quot;auto.offset,reset&quot; -&gt; &quot;earliest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) // 先读取数据库的 Offset, 设置到 Subscribe val offset: Map[TopicPartition, Long] = KafkaOffsetManagerUtils.readOffset(topics(0), groupId) println(&quot;&gt;&gt;&gt; [LOGS] The Offset Now Reading: &quot; + offset) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams, offset) // 指定订阅的 Topic ) // 测试数据: // a,a,a,a,a // b,b,b,c // c,c,d kafkaStream.foreachRDD { rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition { iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) /** * @param: o.topic: 从 topics 可以获取到 * @param: o.partition: 分区 * @param: o.fromOffset: 正在执行的 Offset * @param: o.untilOffset: 即将执行的 Offset */ println(s&quot;${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}&quot;) KafkaOffsetManagerUtils.saveOffset(offsetRanges, groupId) } } kafkaStream.print() streamingContext.start() streamingContext.awaitTermination() }} 测试 4.5 转换操作4.6 输出操作将数据输出到指定的数据仓库中，如MySQL、Redis、HBase等。下面通过一个简单的 WordCount 案例快速了解如何将数据写入 MySQL。本案例借助 创建数据库和表 123456CREATE DATABASE travel CHARSET utf8;CREATE TABLE wc( id BIGINT PRIMARY KEY AUTO_INCREMENT, word VARCHAR(50), count BIGINT ); 存放 Druid 的配置文件 1234567891011url = jdbc:mysql://bigdata:3306/travel?useSSL=false&amp;characterEncoding=UTF-8username = rootpassword = rootdriverClassName = com.mysql.jdbc.DriverinitialSize = 5maxActive = 20minIdle = 1maxWait = 60000validationQuery = SELECT 1testOnBorrow = truetestWhileIdle = true 编写数据库连接工具类ConnectionUtils.scala 1234567891011121314151617181920212223242526272829303132333435package com.example.x.utilsimport java.io.InputStreamimport java.sql.Connectionimport java.util.Propertiesimport com.alibaba.druid.pool.DruidDataSourceFactoryimport javax.sql.DataSourceobject ConnectionUtils { // 1. 创建 Druid 的 DataSource 对象 val dataSource: DataSource = { val properties = new Properties() val inputStream: InputStream = getClass.getClassLoader.getResourceAsStream(&quot;druid.properties&quot;) properties.load(inputStream) println(properties) DruidDataSourceFactory.createDataSource(properties) } // 2. 创建获取连接的方法, 向外提供数据库连接 def getConnection(): Connection = { dataSource.getConnection() } // 3. 创建连接回收方法 def closeConnection(connection: Connection): Unit = { if(null != connection) { connection.close() } } def main(args: Array[String]): Unit = { println(dataSource) }} 编写 WordCount 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.example.x.quickstartimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.ConnectionUtilsimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject WordCount { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;WordCount&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;WordCount&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = Array(&quot;WordCount&quot;) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) val wcDStream: DStream[(String, Int)] = kafkaStream.flatMap(x=&gt;x.value().split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _) // 测试数据： // a,b,a,c,a,d,c,c,a,m,e,f // (a, 4) // (b, 1) // (c, 3) // (d, 1) // (e, 1) // (m, 1) // (f, 1) wcDStream.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;INSERT INTO wordcount(word, count) VALUES (?, ?)&quot; // 在执行这个SQL语句之前需要创建 `wordcount` 表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { pst.setString(1, record._1) pst.setLong(2, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) streamingContext.start() streamingContext.awaitTermination() }} 4.7 任务实施 启动 WebServer01 的 Flume 1[subowen@webserver01 config]$ ./start-flume.sh taildir-avro-stream.conf a1 启动 Bigdata 的 Flume 1[subowen@bigdata config]$ ./start-flume.sh taildir-avro-stream.conf a1 启动 WebServer01 的生产者Producer，即logweb-1.0.jar 123456789[subowen@webserver01 serverJar]$ ll总用量 17608drwxrwxr-x. 3 subowen subowen 18 6月 11 12:10 logweb-rw-rw-r--. 1 subowen subowen 18014926 6月 11 12:08 logweb-1.0.jar-rw-------. 1 subowen subowen 5531 6月 12 13:41 nohup.out-rwxrw-r--. 1 subowen subowen 33 6月 11 12:09 start-logweb.sh[subowen@webserver01 serverJar]$ cat ./start-logweb.shnohup java -jar logweb-1.0.jar &amp;[subowen@webserver01 serverJar]$ ./start-logweb.sh 编写消费者程序，进行实时数据处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.example.x.sparkstreamimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject RealtimeScenicPA { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;RealtimeScenic&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;subowen&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = Array(&quot;subowen&quot;) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) // 1. 统计相同经纬度和时间的人流量数据 val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value) .map(line =&gt; { val splitData = line.split(&quot;,&quot;) (s&quot;${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}&quot;, 1) }) .reduceByKey(_ + _) // 2. 基于状态操作统计每个景点每分钟的人流量 val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line =&gt; { val keySplit = line._1.split(&quot;,&quot;) (s&quot;${keySplit(2)},${keySplit(3).substring(0, 12)}&quot;, line._2) }).reduceByKey(_ + _) // 输出结果 peopleCountPerLocationAndTime.print() peopleCountPerMinutePerLocation.print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 在 Windows 平台运行生产者程序ScenicAPP和消费者程序RealtimeScenicPA。结果如下： 结合 MySQL 进行数据持久化，首先建立好数据库和数据表。 12345678910111213141516171819202122-- SQL 建表语句如下CREATE DATABASE IF NOT EXISTS travel CHARSET UTF8;USE travel;CREATE TABLE IF NOT EXISTS people_count_per_location_and_time ( -- id BIGINT PRIMARY KEY AUTO_INCREMENT, -- id longitude DOUBLE, -- 经度 latitude DOUBLE, -- 纬度 scenic VARCHAR(20), -- 景点 sec_moment VARCHAR(15), -- 具体时刻 sec_quantity BIGINT, -- 具体数量 PRIMARY KEY(scenic, sec_moment) -- 设置联合主键);CREATE TABLE IF NOT EXISTS people_count_per_minute_per_location ( -- id BIGINT PRIMARY KEY AUTO_INCREMENT, -- id scenic VARCHAR(20), -- 景点 min_moment VARCHAR(15), -- 具体时刻 min_quantity BIGINT, -- 具体数量 PRIMARY KEY(scenic, min_moment) -- 设置联合主键); 集合之前 WordCount 案例中编写 ConnectionUtils.scala 代码，完成该项目的数据持久化任务。编写RealtimeScenicPA.scala代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.example.x.travelimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.ConnectionUtilsimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.{Seconds, StreamingContext}object RealtimeScenicPA { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;RealtimeScenic&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) streamingContext.checkpoint(&quot;ckpt&quot;) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;subowen&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = Array(&quot;subowen&quot;) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) val updateFunc = (newValues: Seq[Int], state: Option[Int]) =&gt; { val currentCount = newValues.sum val previousCount = state.getOrElse(0) val newCount = currentCount + previousCount Some(newCount) } // 1. 统计相同经纬度和时间的人流量数据 val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value) .map(line =&gt; { val splitData = line.split(&quot;,&quot;) (s&quot;${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}&quot;, 1) }) .updateStateByKey(updateFunc) // 2. 基于状态操作统计每个景点每分钟的人流量 val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line =&gt; { val keySplit = line._1.split(&quot;,&quot;) (s&quot;${keySplit(2)},${keySplit(3).substring(0, 12)}&quot;, line._2) }).reduceByKey(_ + _) // 输出结果 peopleCountPerLocationAndTime.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO people_count_per_location_and_time(longitude, latitude, scenic, sec_moment, sec_quantity) |VALUES (?, ?, ?, ?, ?) |ON DUPLICATE KEY |UPDATE sec_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setDouble(1, splitRecord(0).toDouble) pst.setDouble(2, splitRecord(1).toDouble) pst.setString(3, splitRecord(2)) pst.setString(4, splitRecord(3)) pst.setInt(5, record._2) pst.setInt(6, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) peopleCountPerMinutePerLocation.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO people_count_per_minute_per_location(scenic, min_moment, min_quantity) |VALUES (?, ?, ?) |ON DUPLICATE KEY |UPDATE min_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setString(1, splitRecord(0)) pst.setString(2, splitRecord(1)) pst.setInt(3, record._2) pst.setInt(4, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) // peopleCountPerLocationAndTime.print() peopleCountPerMinutePerLocation.print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 结合 MySQL 对 t_offset 表进行维护。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145package com.example.x.travelimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.{ConnectionUtils, KafkaOffsetManagerUtils}import org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.{SparkConf, TaskContext}import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.{Seconds, StreamingContext}object RealtimeScenicPA { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;RealtimeScenic&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) streamingContext.checkpoint(&quot;ckpt&quot;) val topics = Array(&quot;subowen&quot;) val groupId = &quot;subowen&quot; val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; groupId, &quot;auto.offset,reset&quot; -&gt; &quot;earliest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) // 先读取数据库的 Offset, 设置到 Subscribe val offset: Map[TopicPartition, Long] = KafkaOffsetManagerUtils.readOffset(topics(0), groupId) println(&quot;&gt;&gt;&gt; [LOGS] The Offset Now Reading: &quot; + offset) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams, offset) ) val updateFunc = (newValues: Seq[Int], state: Option[Int]) =&gt; { val currentCount = newValues.sum val previousCount = state.getOrElse(0) val newCount = currentCount + previousCount Some(newCount) } // 1. 统计相同经纬度和时间的人流量数据 val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value) .map(line =&gt; { val splitData = line.split(&quot;,&quot;) (s&quot;${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}&quot;, 1) }) .updateStateByKey(updateFunc) // 2. 基于状态操作统计每个景点每分钟的人流量 val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line =&gt; { val keySplit = line._1.split(&quot;,&quot;) (s&quot;${keySplit(2)},${keySplit(3).substring(0, 12)}&quot;, line._2) }).reduceByKey(_ + _) // 输出结果 peopleCountPerLocationAndTime.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO t_heat(longitude, latitude, scenic, sec_moment, sec_quantity) |VALUES (?, ?, ?, ?, ?) |ON DUPLICATE KEY |UPDATE sec_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setDouble(1, splitRecord(0).toDouble) pst.setDouble(2, splitRecord(1).toDouble) pst.setString(3, splitRecord(2)) pst.setString(4, splitRecord(3)) pst.setInt(5, record._2) pst.setInt(6, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) peopleCountPerMinutePerLocation.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO t_scenic(scenic, min_moment, min_quantity) |VALUES (?, ?, ?) |ON DUPLICATE KEY |UPDATE min_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setString(1, splitRecord(0)) pst.setString(2, splitRecord(1)) pst.setInt(3, record._2) pst.setInt(4, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) // 维护 KafkaStream 的 Offset 表 kafkaStream.foreachRDD { rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition { iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) /** * @param: o.topic: 从 topics 可以获取到 * @param: o.partition: 分区 * @param: o.fromOffset: 正在执行的 Offset * @param: o.untilOffset: 即将执行的 Offset */ println(s&quot;${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}&quot;) KafkaOffsetManagerUtils.saveOffset(offsetRanges, groupId) } } // peopleCountPerLocationAndTime.print() peopleCountPerMinutePerLocation.print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 🎥 5. 数据实时监控系统5.1 软件开发体系架构5.1.1 开发架构 架构 说明 B/S架构（浏览器/服务器） 只要用户安装浏览器（谷歌）就可以访问 C/S架构（客户端/服务器） 需要用户安装和更新客户端部分，比如微信 5.1.2 开发模式及相关技术 开发模式：MVC 模式 组件 说明 Model 模型 View 视图，login.html / index.html Controller 控制器，接受用户的请求，向用户做出响应 开发方式 方式 说明 前后端不分离 JSP/HTML + SSM 前后端分离 前端和后台不在同一个项目中 请求方法 请求方法 说明 常用方式 GET 常用于查询、删除操作 1. 浏览器的地址栏2. &lt;a href=&quot;...&quot;&gt;&lt;/a&gt;3. Ajax4. 异步请求 POST 常用于增加、修改操作 1. Form表单2. Ajax JavaWeb开发相关技术 Java后台技术 SSM：大量的配置文件 xml， SpringBoot+ MyBatis Servlet/JSP、Spring + Spring MVC、SpringBoot、SpringCloud（微服务） 数据库相关框架：MyBatis、Hibernate、Spring JPA 前端技术 HTML/HTML5、CSS/CSS3、JS 前端框架：jQuery、Vue、AngularJS、React、TS 5.2 SpringBoot + MyBatis 框架简介Spring Boot：入门篇 (cnblogs.com) MyBatis中文网 5.3 后台服务开发基本任务说明： 统计今天所有景点的人流量总和 统计当前时间（分钟）所有景点的人流量总和 统计每分钟每个景点的人流量（经纬度）-热力图 5.3.1 项目环境搭建 创建SpringBoot项目 修改必要配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;!-- 这里需要修改版本 --&gt; &lt;version&gt;2.7.15&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example.x.travel&lt;/groupId&gt; &lt;artifactId&gt;travelweb&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;travelweb&lt;/name&gt; &lt;description&gt;Travel Project Web&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.41&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.18&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 修改 application.xml 为 application.yml 并进行配置 123456789101112131415161718192021222324252627282930313233343536# 配置应用服务器端口以及根目录server: port: 9090 servlet: context-path: /travelserver# 配置应用名spring: application: name: travelserver # 配置数据源, 这里是 Druid 的一些配置 datasource: type: com.alibaba.druid.pool.DruidDataSource druid: url: jdbc:mysql://bigdata:3306/travel?useSSL=false&amp;characterEncoding=UTF-8 username: root password: root driver-class-name: com.mysql.jdbc.Driver initial-size: 1 minIdle: 1 maxActive: 5 maxWait: 6000 timeBetweenEvictionRunsMillis: 6000 minEvictableIdleTimeMillis: 30000 validationQuery: SELECT 1 FROM DUAL testWhileIdle: true testOnBorrow: false testOnReturn: false filters: stat,wall,log4j# 配置 MyBatis 的路径mybatis: # 这里是 DAO Mapper的路径, 表示 Resource/Mapper/ 的所有 *.xml 文件 mapper-locations: classpath:mapper/*.xml # 这里定义实体类的位置 type-aliases-package: com.example.x.travel.travelweb.entity configuration: map-underscore-to-camel-case: true 在 IDEA 中添加 Lombok 和 MyBatisX 的插件 配置 MyBatis 的 *.xml 模板 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper&gt; &lt;!-- 这里补全内容 --&gt;&lt;/mapper&gt; 开发 TravelConfig.java 类解决跨域问题 123456789101112131415161718192021// SpringBoot 中很多事情需要通过注解的方式来实现// @... 代表注解// @Configuration 注解表明这是一个 Spring Boot 的配置类// @EnableWebMvc 开启 WebMvc 的功能, 这样 Spring Boot 就能自动处理 Web 相关的配置@Configuration@EnableWebMvcpublic class TravelConfig implements WebMvcConfigurer { /** * @brief 设置服务器跨域访问策略: 所有 GET 和 POST 请求都可以跨域访问, 允许所有来源的请求, 即允许跨域请求 * 因为我们的项目是前后端分离的, 即前端和后端不在一个服务器上, 所以需要处理跨域问题 * @param registry */ @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) // addMapping 方法的参数代表需要进行 CORS 映射的 HTTP方法及其路径, 这里是&quot;/**&quot;, 意味着对所有HTTP方法进行映射 .allowedOriginPatterns(&quot;*&quot;) // allowedOriginPatterns 是一个正则表达式列表, 这里使用 &quot;*&quot; 表示任何来源都可以 .allowedMethods(&quot;GET&quot;, &quot;POST&quot;) // allowedMethods是允许的 HTTP 方法，这里是 &quot;GET,POST&quot; .allowCredentials(true) // 允许携带请求体的请求通过, 这里的 allowCredentials(true) 意味着在请求头中携带用户凭证信息是允许的 .maxAge(3600); // maxAge 设置 CORS 跨域配置的缓存时间, 这里是 3600 秒 }} 本项目（Java开发）中层与层之间的联系 5.3.2 后台开发 com.example.x.travel.travelweb.config.TravelConfig 1234567891011121314151617181920212223242526import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;// SpringBoot 中很多事情需要通过注解的方式来实现// @... 代表注解// @Configuration 注解表明这是一个 Spring Boot 的配置类// @EnableWebMvc 开启 WebMvc 的功能, 这样 Spring Boot 就能自动处理 Web 相关的配置@Configuration@EnableWebMvcpublic class TravelConfig implements WebMvcConfigurer { /** * @brief 设置服务器跨域访问策略: 所有 GET 和 POST 请求都可以跨域访问, 允许所有来源的请求, 即允许跨域请求 * 因为我们的项目是前后端分离的, 即前端和后端不在一个服务器上, 所以需要处理跨域问题 * @param registry */ @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) // addMapping 方法的参数代表需要进行 CORS 映射的 HTTP方法及其路径, 这里是&quot;/**&quot;, 意味着对所有HTTP方法进行映射 .allowedOriginPatterns(&quot;*&quot;) // allowedOriginPatterns 是一个正则表达式列表, 这里使用 &quot;*&quot; 表示任何来源都可以 .allowedMethods(&quot;GET&quot;, &quot;POST&quot;) // allowedMethods是允许的 HTTP 方法，这里是 &quot;GET,POST&quot; .allowCredentials(true) // 允许携带请求体的请求通过, 这里的 allowCredentials(true) 意味着在请求头中携带用户凭证信息是允许的 .maxAge(3600); // maxAge 设置 CORS 跨域配置的缓存时间, 这里是 3600 秒 }} com.example.x.travel.travelweb.controller.TravelController 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import java.util.HashMap;import java.util.List;import java.util.Map;@RestController // 注解: 定义这个类是一个 Controller@RequestMapping(&quot;/scenic&quot;) // 注解: 定义访问这个类的 URL// 访问 URL: http://ip:port/scenicpublic class TravelController { @Autowired private TravelService travelService; // @RequestMapping 表示既可以通过 GET 访问, 也可以通过 POST 访问 // @GetMapping 表示只可以通过 GET 访问 // @PostMapping 表示只可以通过 POST 访问 /** * @brief 表示获取一天访客的总数量 * @url http://localhost:9090/travelserver/scenic/getDaySum * @return */ @RequestMapping(&quot;/getDaySum&quot;) public long getDaySum() { long sum = travelService.getDaySum(); return sum; } @RequestMapping(&quot;/getCurrentSum&quot;) public long getCurrentSum() { long sum = travelService.getCurrentSum(); return sum; } @RequestMapping(&quot;/getHeatData&quot;) public List&lt;Scenic&gt; getHeatData() { return travelService.getHeatData(); } @RequestMapping(&quot;/getScenicMinuteData&quot;) public List&lt;Scenic&gt; getScenicMinuteData() { return travelService.getScenicMinuteData(); } @RequestMapping(&quot;/getScenicTrend&quot;) public HashMap&lt;String, Object&gt; getScenicTrend() { return travelService.getScenicTread(); } @PostMapping(&quot;/selectForm&quot;) @ResponseBody public Object receiveFormData(@RequestBody Map&lt;String, String&gt; formData) { // 在这里可以处理收到的formData // System.out.println(formData); String selectDate = formData.get(&quot;date&quot;); String selectFromTime = formData.get(&quot;fromTime&quot;); String selectToTime = formData.get(&quot;toTime&quot;); String selectTableName = formData.get(&quot;tableName&quot;); // System.out.println(selectDate + &quot; &quot; + selectFromTime + &quot; &quot; + selectToTime + &quot; &quot; + selectTableName); // 执行业务逻辑 if (selectDate.equals(&quot;&quot;)) { return null; } String fromDateAndTime = selectDate.replace(&quot;-&quot;, &quot;&quot;) + selectFromTime.replace(&quot;:&quot;, &quot;&quot;).replace(&quot;：&quot;, &quot;&quot;); String toDateAndTime = selectDate.replace(&quot;-&quot;, &quot;&quot;) + selectToTime.replace(&quot;:&quot;, &quot;&quot;).replace(&quot;：&quot;, &quot;&quot;); System.out.println(&quot;From Date And Time: &quot; + fromDateAndTime + &quot;\\nTo Date And Time: &quot; + toDateAndTime); switch (selectTableName) { case &quot;0&quot;: { System.out.println(&quot;请进行选择&quot;); break; } case &quot;1&quot;: { // 热力图 return travelService.getHeatDataByDateAndTime(fromDateAndTime, toDateAndTime); } case &quot;2&quot;: { // 人流量柱状图 return travelService.getScenicDataByDateAndTime(fromDateAndTime, toDateAndTime); } case &quot;3&quot;: { // 人流量趋势图 return travelService.getScenicTreadByDateAndTime(fromDateAndTime, toDateAndTime); } default: // 未定义图表 break; } return &quot;Data received successfully&quot;; }} com.example.x.travel.travelweb.dao.TravelDao 123456789101112131415161718import org.springframework.stereotype.Repository;import java.util.HashMap;import java.util.List;@Repositorypublic interface TravelDao { long getDaySum(); // 查询当天时间内的所有数据之和 long getCurrentSum(); // 查询当前时间内的所有数据之和 List&lt;Scenic&gt; getHeatData(); // 查询用于创建热力图的数据 List&lt;Scenic&gt; getScenicMinuteData(); // 查询用于获取每分钟景点人流量 List&lt;Scenic&gt; getScenicTread(); // 查询用于获取每分钟景点人流量趋势 List&lt;Scenic&gt; getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime); List&lt;Scenic&gt; getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime); List&lt;Scenic&gt; getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime);} com.example.x.travel.travelweb.entity.LineData 123456789import lombok.Data;import java.util.List;@Datapublic class LineData { private String name; private List&lt;Long&gt; data;} com.example.x.travel.travelweb.entity.Scenic 12345678910import lombok.Data;@Datapublic class Scenic { private double lng; private double lat; private long count; private String time; private String scenic;} com.example.x.travel.travelweb.services.TravelService 1234567891011121314151617181920package com.example.x.travel.travelweb.services;import com.example.x.travel.travelweb.entity.Scenic;import java.util.HashMap;import java.util.List;// 接口: 定义规范的public interface TravelService { long getDaySum(); long getCurrentSum(); List&lt;Scenic&gt; getHeatData(); List&lt;Scenic&gt; getScenicMinuteData(); HashMap&lt;String, Object&gt; getScenicTread(); List&lt;Scenic&gt; getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime); List&lt;Scenic&gt; getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime); HashMap&lt;String, Object&gt; getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime);} com.example.x.travel.travelweb.services.impl.TravelServiceImpl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.HashMap;import java.util.List;@Servicepublic class TravelServiceImpl implements TravelService { // 调用某个对象的方法, 需要先创建对象 @Autowired private TravelDao travelDao; // 相当于 TravelDap travelDao = new TravelDao(); @Override public long getDaySum() { return travelDao.getDaySum() ; } @Override public long getCurrentSum() { return travelDao.getCurrentSum(); } @Override public List&lt;Scenic&gt; getHeatData() { return travelDao.getHeatData(); } @Override public List&lt;Scenic&gt; getScenicMinuteData() { return travelDao.getScenicMinuteData(); } @Override public HashMap&lt;String, Object&gt; getScenicTread() { HashMap&lt;String, Object&gt; maps = new HashMap&lt;&gt;(); List&lt;String&gt; times = new ArrayList&lt;&gt;(); // 时间数组 List&lt;LineData&gt; series = new ArrayList&lt;&gt;(); // Series 数组 List&lt;Scenic&gt; heat = travelDao.getScenicTread(); // 调用 DAO 方法查询数据 HashMap&lt;String, List&lt;Long&gt;&gt; map = new HashMap&lt;&gt;(); // 将景点以及这个景点对应的所有数据整合成一条数据 for(Scenic scenic: heat) { String time = scenic.getTime(); // 获取数据库每条数据的时间 if (!times.contains(time)) { // 这个时间在 times 中是否存在 times.add(time); // 不存在则添加 } String scenic_name = scenic.getScenic(); // 获取景点名称 // 判断在 Map 中是否有 key, 并判断: // 如果有这个 key, 则把数据添加到 value 中 // 如果没有这个 key, 则添加这个 key, 新添加一个集合 map.computeIfAbsent(scenic_name, k-&gt;new ArrayList&lt;&gt;()).add(scenic.getCount()); // 上面一行等同于下面的逻辑 // if (map.containsKey(scienc)){ // map.get(scienc).add(scenic.getCount()); // } else { // List list=new ArrayList&lt;Long&gt;(); // list.add(scenic.getCount()); // map.put(scienc,list); // } } // System.out.println(times); // System.out.println(map); map.forEach((key, value) -&gt; { LineData data = new LineData(); data.setName(key); data.setData(value); series.add(data); }); maps.put(&quot;time&quot;, times); maps.put(&quot;series&quot;, series); return maps; } @Override public List&lt;Scenic&gt; getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime) { return travelDao.getHeatDataByDateAndTime(fromDateAndTime, toDateAndTime); } @Override public List&lt;Scenic&gt; getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime) { return travelDao.getScenicDataByDateAndTime(fromDateAndTime, toDateAndTime); } @Override public HashMap&lt;String, Object&gt; getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime) { return null; }} mapper/TravelMapper.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.inspur.szy.subowen.travel.travelweb.dao.TravelDao&quot;&gt; &lt;select id=&quot;getDaySum&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT IFNULL(SUM(min_quantity), 0) AS cnt FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 8)=DATE_FORMAT(NOW(), &quot;%Y%m%d&quot;) &lt;/select&gt; &lt;select id=&quot;getCurrentSum&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT IFNULL(SUM(min_quantity), 0) AS cnt FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 12)=DATE_FORMAT(NOW(), &quot;%Y%m%d%H%i&quot;) &lt;/select&gt; &lt;select id=&quot;getHeatData&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT longitude AS `lng`, latitude AS `lat`, SUBSTRING(`sec_moment`, 1, 12) AS today, SUM(`sec_quantity`) AS `count` FROM t_heat GROUP BY lng, lat, today HAVING today=DATE_FORMAT(NOW(), '%Y%m%d%H%i') &lt;/select&gt; &lt;select id=&quot;getScenicMinuteData&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT scenic, min_moment AS `time`, min_quantity AS `count` FROM t_scenic WHERE min_moment = (SELECT MAX(min_moment) FROM t_scenic WHERE &lt;![CDATA[min_moment &lt;= NOW()]]&gt;) GROUP BY scenic, min_moment; &lt;/select&gt; &lt;select id=&quot;getScenicTread&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT scenic, min_moment AS `time`, min_quantity AS `count` FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 8)=DATE_FORMAT(NOW(), '%Y%m%d') ORDER BY `time` &lt;/select&gt; &lt;select id=&quot;getHeatDataByDateAndTime&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT longitude AS `lng`, latitude AS `lat`, SUBSTRING(`sec_moment`, 1, 12) AS day, SUM(`sec_quantity`) AS `count` FROM t_heat GROUP BY lng, lat, day HAVING &lt;![CDATA[day &gt;= #{fromDateAndTime} AND day &lt;= #{toDateAndTime}]]&gt; &lt;/select&gt; &lt;select id=&quot;getScenicDataByDateAndTime&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT scenic, SUBSTRING(`min_moment`, 1, 12) AS `time`, min_quantity AS `count` FROM t_scenic WHERE &lt;![CDATA[min_moment &gt;= #{fromDateAndTime} AND min_moment &lt;= #{toDateAndTime}]]&gt; GROUP BY `scenic`, `min_moment`; &lt;/select&gt; &lt;select id=&quot;getScenicTreadByDateAndTime&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; &lt;/select&gt;&lt;/mapper&gt; 5.4 前端开发所有代码在 GitHub 均有开源，我做的也就只是添加 Ajax 请求而已。 ❓ 问题汇总 序号 发生日期 问题描述 是否解决 解决措施 备注 0001 2024-06-13 执行代码：出现编译错误： 是 修改代码：val topics = &quot;...&quot; 为 val topics = Array(&quot;...&quot;)这里报错的原因是出现了无法重载方法的问题。因为错误的传递了所需要的参数，导致Subscribe方法出现了不期待的重载。修改代码到正确的格式即可。 老师协助解决 0002 2024-06-13 使用updateStatusByKey出现数据库数据不断增加的问题问题原因：1. MySQL数据表设计不合理，设置了一个列为 id 并将其设置为主键2. SQL语句有待优化 是 1. 修改MySQL表的结构，设置联合主键 2. 修改SQL语句，使用 ON DUPLICATE KEY 语法 老师协助解决 0003 2024-06-13 执行程序出现错误：ERROR [main] (Logging.scala:94) - Failed to bind SparkUI问题原因：每一个Spark任务都会占用一个SparkUI端口，默认为4040，如果被占用则依次递增端口重试。但是有个默认重试次数，为16次。16次重试都失败后，会放弃该任务的运行。 是 初始化 SparkConf 时，添加conf.set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;)语句；使用 spark-submit提交任务时，在启动命令行中添加–conf spark.port.maxRetries=100 \\该参数设置向后递增100次寻找端口 自主解决 0004 2024-06-17 不是很严重的问题，当return 0 或未产生数据时，使用 Edge 浏览器会出现屏幕上没有输出值的情况 是 替换为Chrome即可解决问题，不过也没啥必要 水 0005 2024-06-18 在 TravelMapper.xml 中编写带 &lt; 和&gt; 的 SQL 代码，导致出现问题：Caused by: org.xml.sax.SAXParseException: 元素内容必须由格式正确的字符数据或标记组成。问题原因：XML文件会将&lt; 或 &gt; 当作是标记，导致错误 是 参考文章使用标记&lt;![CDATA[ ]]&gt;将包含 &lt; 或 &gt; 的语句包含住，如：&lt;![CDATA[min_moment &lt;= NOW()]]&gt; 自主解决 0006 2024-06-19 同学出现问题：生产者数据正常写入、客户端（Windows）和服务器端（Linux）时间正确，MySQL无法通过NOW() 获取到当前时间的数据。 是 MySQL时区问题。 协助同学解决 0007 2024-06-20 同学出现问题：虚拟机掉网卡 是 虚拟机网卡不见了,重新开机虚拟机网卡消失连接不上–虚拟机网卡掉了 协助同学解决","link":"/2024/06/11/OnlineTravelBigdataPlatform/"},{"title":"SparkQuickIN","text":"快速入门 Spark[TOC] ⛳︎ 1. 开始 SparkSpark官网：Apache Spark™ - Unified Engine for large-scale data analytics 1.1 什么是SparkSpark官网的解释：Apache Spark™ is a unified analytics engine for large-scale data processing. Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是加州大学伯克利分校的 AMP实验室 所开源的类 Hadoop MapReduce 的通用并行计算框架，Spark 拥有 Hadoop MapReduce 所具有的优点，但不同于 MapReduce 的是：Job 中间输出结果可以缓存在内存中，从而不再需要读写 HDFS，减少磁盘数据交互，因此 spark 能更好地适用于数据挖掘与机器学习等需要迭代的算法。 Spark是 Scala 编写，方便快速编程。 其特点是：高速、使用简单、通用、可以在多处运行。 1.2 总体技术栈讲解 Spark 提供了 Sparkcore RDD、Spark SQL、Spark Streaming、Spark MLlib、Spark GraphX等技术组件，可以一站式的完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见的任务。这就是Spark一站式开发的特点。 1.3 Spark 和 MapReduce 的区别1.3.1 MapReduce 的原理MapReduce 在运算时需要多次进行磁盘 I/O。下面是一个简单的 MapReduce 过程： 视频链接：https://www.bilibili.com/video/BV1TB4y1i7kk/ 在这个视频中，可以看出MapReduce 过程中需要多次磁盘 I/O，落地到HDFS上。 1.3.2 Spark 是如何做的 可以看到，MapReduce 的多个 Job 之间相互独立，每个 Job 完成后的数据都需要存储到文件系统中。每个 Job 中也可能会存在大量的磁盘 I/O ，这样会使得 MapReduce 的速度很慢。相比于 MapReduce，Spark使用了 DAG 有向无环图。使多个任务串联起来，将结果存储在内存中（当然内存不够还是要将数据缓存在磁盘中）直接进行运算，避免了大量的磁盘I/O。 1.3.3 Spark 和 MapReduce 的一些联系Spark 和 MapReduce 都是分布式计算框架，Spark 计算中间结果基于内存缓存，MapReduce 基于HDFS存储。也正因此，Spark处理数据的能力一般是 MapReduce的三到五倍以上，Spark 中除了基于内存计算这一个计算快的原因，还有DAG(DAG Schedule)有向无环图来切分任务的执行先后顺序。 1.4 Spark APISpark API 有多种语言支持，分别包括：Scala、Java、Python、R、SQL 等。 1.5 Spark 的运行模式 Local：多用于本地测试，如在：Eclipse、IDEA 中编写测试程序等。 Standalone：Spark 自带的资源调度框架，它支持完全分布式。Standalone模式也叫作独立模式，其自带完整的服务，可单独部署到一个集群中，无序依赖任何其他资源管理系统。 从一定程度上来说，该模式是 Local 模式和 Yarn 模式的基础。 Yarn： Hadoop 生态圈里的一种资源调度框架，Spark也是可以基于 Yarn 来计算的。 若要使用 Yarn 来进行资源调度，必须实现ApplicationMaster 接口，Spark 实现了这个接口，所以可以基于 Yarn 来进行资源调度。 Mesos：也是一种资源调度框架（了解即可）。 🥑 2. SparkCore2.1 RDD2.1.1 RDD 的概念RDD(Resilient Distribute Dataset)：弹性分布式数据集。 RDD → 算子 → Other RDD，RDD 经过算子的运算会变成其他的 RDD。 （重点）RDD的特点：① 分区的；② 并行操作的；③ 不可变的。 2.1.2 RDD 的五大特性 每个RDD 由一系列的 Partition 组成。 函数是作用在每一个 Partition (Split) 上的。 RDD 中有一系列的依赖关系，或者说每个RDD都会依赖其他一系列的RDD。 分区器是作用在 &lt;K, V&gt; 格式的 RDD 上，即：&lt;K, V&gt; 的 RDD 可以通过 Partitioner 进行自定义分区。 RDD提供一系列的最佳计算位置。数据在哪里，计算就在哪里，移动数据不如移动计算。 2.1.3 RDD 理解图 Spark 中读取文件是使用 SparkContext 对象调用 textFile 方法，实际上底层和 MapReduce 读取 HDFS 文件的方式是相同的，读取之前先要进行 split 切片。默认情况下 Split 的大小和 Block 块的大小相同。 一些问题： RDD的分布式体现在那些方面？ RDD 由一系列的 Partition 构成，并且 Partition 是分布在不同的节点上的。这就体现了 RDD 的分布式。 哪里体现了 RDD 的弹性？ RDD 由一系列的 Partition 组成，其大小和数量都是可以改变的。默认情况下，Partition 的个数和 Block 块的个数相同。 哪里体现了 RDD 的容错？ RDD 之间存在一系列的依赖关系，子RDD 可以找到对应的父RDD ，然后通过一系列计算得到得出响应的结果，这就是容错的体现。 RDD 提供计算最佳位置，体现了数据本地化，体现了大数据中”移动数据不如移动计算“的理念。 一些注意事项： textFile 方法底层封装的是 MapReduce 读取文件的方式，读取文件之前先进行 Split 切片，默认 Split 大小是一个 Block 的大小。 RDD 实际上不存储数据，但是为了方便理解，可以理解为存储数据。 什么是 &lt;K, V&gt; 格式的 RDD，如果 RDD 里面存储的数据都是二元组对象，那么这个 RDD 我们就叫做 &lt;K, V&gt; 格式的 RDD。 2.1.4 SparkRDD 编程模型 创建 SparkContext 对象 创建 RDD 计算 RDD 输出结果（如控制台打印测试，存储等） 关闭 SparkContext 2.1.5 WordCount 案例123456789101112131415161718192021222324252627object WordCount { def main(args: Array[String]): Unit = { // 第一步: 创建 SparkContext 对象 // 对于每个 Spark 程序来说, 最重要的就是两个对象: SparkConf 和 SparkContext val conf = SparkConf() conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;Local&quot;) val sparkContext = SparkContext(conf) // 创建 RDD val line: RDD[String] = sc.textFile(&quot;files/order.csv&quot;) // 计算 RDD val word: RDD[String] = sc.flatMap(x =&gt; x.split(&quot; &quot;)) val pair: RDD[(String, Int)] = word.map(x =&gt; (x, 1)) // val result: RDD[(String, Int)] = pair.reduceByKey((x, y) =&gt; {x + y}) val result: RDD[(String, Int)] = pair.reduceByKey((x, y) =&gt; { println(x + &quot;:&quot; + y) x + y }) // 输出 RDD result.foreach(println) // 关闭 SparkContext sparkContext.stop() }} 2.2 Spark 任务执行原理从下图中，我们可以看到 Spark 的主要角色： 一些名词的解释： Master Node 主节点 Worker Node 从节点 Driver 驱动程序 Executor 执行节点 Cluster Manager 集群管理者 2.2.1 Spark 架构的类比我们可以简单的将这个架构和 YARN 对比一下：Master 就相当于 YARN 中的 ResourceManager，Worker 就相当于 YARN 中的 NodeManager，Driver 相当于 YARN 中的 Application。 2.2.2 Spark 执行原理详细说明 Master 和 Worker 节点 搭建 Spark 集群的时候我们就已经设置好了 Master 节点和 Worker 节点，一个集群有多个Master节点和多个Worker节点。 Master 节点常驻 Master 守护进程，负责管理 Worker 节点，我们从 Master 节点提交应用。 Worker 节点常驻 Worker 守护进程，与 Master 节点通信，并且管理 Executor 进程。 一台机器可以同时作为 Master 和 Worker 节点（e.g. 有四台机器，可以选择一台设置为 Master节点，然后剩下三台设为 Worker节点，也可以把四台都设为 Worker 节点，这种情况下，有一个机器既是 Master 节点又是 Worker 节点）。 一个 Spark 应用程序分为一个驱动程序 Driver 和多个执行程序 Executors 两种。 Driver 和 Executor 进程Driver 进程就是应用的 main() 函数并且构建 SparkContext 对象，当我们提交了应用之后，便会启动一个对应的 Driver 进程，Driver 本身会根据我们设置的参数占有一定的资源（主要指 CPU Core 和 Memory）。 根据部署模式的不同，Driver 可以运行在 Master 上，也可以运行 Worker上，Driver 与集群节点之间有频繁的通信。上图展示了 Driver 在 Master 上的部署的情况。 如上图所示，Driver首先会向**集群管理者Cluster Manager**，如Standalone、Yarn、Mesos 申请 Spark 应用所需的资源，也就是Executor，然后集群管理者会根据 Spark 应用所设置的参数在各个 Worker 上分配一定数量的 Executor，每个 Executor 都占用一定数量的 CPU和 Memory。 在申请到应用所需的资源以后，Driver 就开始调度和执行我们编写的应用代码了。Driver 进程会将我们编写的 Spark 应用代码拆分成多个 Stage，每个Stage 执行一部分代码片段，并为每个 Stage 创建一批 Tasks，然后将这些 Tasks分配到各个 Executor中执行。这一步即 Driver ---Task--&gt; Worker。 Executor 进程宿主在 Worker 节点上，一个 Worker可以有多个 Executor。每个 Executor 持有一个线程池，每个线程可以执行一个 Task，Executor 执行完 Task 以后将结果返回给 Driver，每个 Executor 执行的 Task 都属于同一个应用。此外 Executor 还有一个功能就是为应用程序中要求缓存的 RDD 提供内存式存储，RDD 是直接缓存在 Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。这一步即 Worker ---Result--&gt; Driver。 Driver 负责任务 Tasks 的分发和结果Results的回收，即任务的调度。如果 Task 的计算结果非常大就不要回收了，会造成OOM（我们在执行程序时可以通过参数指定 Driver 的内存大小，如 1G，如果一个 Worker 的结果是 510M，那么两个接节点上的结果就会超过 1G，导致 OOM）。 2.2 RDD 算子RDD有两种操作算子：分别为转换算子(Transformation) 和 **行动算子(Action)**。算子其实就是函数，只不过在 Scala 中称为算子。 下面表格列出了部分 RDD 算子，完整内容可以查看 [SparkRDD](RDD Programming Guide - Spark 3.0.1 Documentation (apache.org)) 文档。 算子类型 算子方法 算子转换 Transformations map(f: T=>U) RDD[T] => RDD[U] filter(f: T=>Bool) RDD[T] => RDD[T] flatMap(f: T=>Seq[U]) RDD[T] => RDD[U] sample(fraction: Float) RDD[T] => RDD[T](Deterministic sampling) groupByKey() RDD[(K, V)] => RDD[(K, Seq[V])] reduceByKey(f: (V, V)=>V) RDD[(K, V)] => RDD[(K, V)] union() (RDD[T], RDD[T]) => RDD[T] join() (RDD[(K, V)], RDD[(K, W)]) => RDD[(K, (V, W))] cogroup() (RDD[(K, V)], RDD[(K, W)]) => RDD([K, (Seq[V], Seq[W])]) crossProduct() (RDD[T], RDD[U]) => (RDD[(T, U)]) mapValues(f: V=>W) RDD[(K, V)] => RDD[(K, W)](Preserves Partitioning) sort(c: Comparator[K]) RDD[(K, V)] => RDD[(K, V)] partitionBy(p: Partitioner[K]) RDD[(K, V)] => RDD[(K, V)] ... ... Actions count() RDD[T] => Long collect() RDD[T] => Seq[T] reduce(f: (T, T)=>T) RDD[T] => T lookup(k: K) RDD[(K, V)] => Seq[V](On hash/range partitioned RDDs) save(path: String) Outputs RDD to a Storage System, e.g. HDFS foreach(func) - ... ... 2.2.1 Transformation 转换算子点击快速跳转到转换算子列表：Transformations Transformation 转换算子有延迟执行的特点，具有**懒加载(Lazy)**的特性。 下面列出常用的行动算子及用法： map(func) 返回一个新的分布式数据集，由每个原元素经过func函数转换后组成。 12345678910111213141516object MapDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;MapDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(0, 10, 15, 20, 25, 30, 35, 40, 45, 50) val listRDD = sc.parallelize(list, 5) listRDD.foreach(println) println() val retRDD = listRDD.map(num =&gt; num * 7) retRDD.foreach(num =&gt; println(num)) sc.stop() }} mapPartition(func) 将函数用在每个RDD的分区上 1234567891011121314151617181920212223object MapPartitionDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;MapPartitionDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(0, 10, 15, 20, 25, 30, 35, 40, 45, 50) val listRDD = sc.parallelize(list, 3) // 设置三个分区 // 对比 map 和 mapPartition 的区别 listRDD.map( x =&gt; { println(&quot;Map 执行一次&quot;) x + 1 }).foreach(println) println() listRDD.mapPartitions( x =&gt; { println(&quot;MapPartition 执行一次&quot;) x.map(x =&gt; {println(&quot;mapPartition 里的map&quot;); x + 1}) // 这个 map 不是 RDD 里的 map, 而是 Iterator 中的 map }).foreach(println) }} filter(func)返回一个新的数据集，由经过func函数后返回值为true的元素组成。 12345678910111213object FilterDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FilterDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(0, 10, 15, 20, 25, 30, 35, 40, 45, 50) val listRDD = sc.parallelize(list) val retRDD = listRDD.filter(num =&gt; num % 2 == 0) retRDD.foreach(println) sc.stop() }} flatMap(func)类似于map，但是每一个输入元素，会被映射为 0 到多个输出元素（因此，func 函数的返回值是一个 Seq，而不是单一元素）。 12345678910111213object FlatMapDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FlatMapDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(&quot;hello you&quot;, &quot;hello he&quot;, &quot;hello me&quot;) val listRDD = sc.parallelize(list) val wordsRDD = listRDD.flatMap(line =&gt; line.split(&quot; &quot;)) wordsRDD.foreach(println) sc.stop() }} sample(withReplacement, frac, seed)根据给定的随机种子 seed，随机抽样出数量为 frac 的数据。 1 union(otherDataset)返回一个新的数据集，由原数据集和参数联合而成。 1 groupByKey([numTasks])在一个由 &lt;K, V&gt; 对组成的数据集上调用，返回一个 &lt;K, Seq[V]&gt; 对的数据集。注意：默认情况下，使用 8 个并行任务进行分组，你可以传入 numTask 可选参数，根据数据量设置不同数目的 Task。使用该算子可以将相同Key的元素聚集到一起，最终把所有相同Key的元素合并成一个元素，该元素的Key不变，Value则聚集到一个集合中。 1 reduceByKey(func, [numTasks])在一个&lt;K, V&gt;对的数据集上使用，返回一个&lt;K, V&gt;对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和 groupByKey类似，任务的个数是可以通过第二个可选参数来配置的。 1 join(otherDataset, [numTasks])在类型为&lt;K, V&gt;和&lt;K, W&gt;类型的数据集上调用，返回一个&lt;K, &lt;V, W&gt;&gt;对，每个key中的所有元素都在一起的数据集。 sortByKey() 和 sortBy()，sortByKey() 函数需要在类型为 &lt;K, V&gt; 类型的数据集上调用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152object SortByKeyDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SortByKeyDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List( &quot;1,李 磊,22,175&quot;, &quot;2,刘银鹏,23,175&quot;, &quot;3,齐彦鹏,22,180&quot;, &quot;4,杨 柳,22,168&quot;, &quot;5,敦 鹏,20,175&quot; ) val listRDD: RDD[String] = sc.parallelize(list) listRDD.foreach(println) println() // 3,齐彦鹏,22,180 // 1,李磊,22,175 // 2,刘银鹏,23,175 val resultRDD = listRDD.map(x =&gt; { val fields = x.split(&quot;,&quot;) println(fields(0) + &quot; &quot; + fields(1) + &quot; &quot; + fields(2) + &quot; &quot; + fields(3) + &quot; &quot;) (fields(0), fields(1), fields(2), fields(3)) }).map(x =&gt; {(x._3, x._1)}) // 分区数会影响最终打印结果, 设置分区为 2, 所有分区最终会聚合为两个分区, 打印时显示每个分区的排序 // 这就可能导致打印输出时出现不同分区排序数据交叉的情况。 // 所以我们一般设置分区为 1, 表示排序结果聚合到一个分区 resultRDD.sortByKey(true, 2).foreach(println) println() val sortBy_RDD = listRDD.map(x =&gt; { val fields = x.split(&quot;,&quot;) // println(fields(0) + &quot; &quot; + fields(1) + &quot; &quot; + fields(2) + &quot; &quot; + fields(3) + &quot; &quot;) (fields(0), fields(1), fields(2), fields(3)) }) println(&quot;SortBy_001&quot;) sortBy_RDD.sortBy(_._1, true, 1).foreach(println) println(&quot;SortBy_002&quot;) sortBy_RDD.sortBy(_._2, true, 1).foreach(println) println(&quot;SortBy_003&quot;) sortBy_RDD.sortBy(_._3, true, 1).foreach(println) println(&quot;SortBy_004&quot;) sortBy_RDD.sortBy(_._4, true, 1).foreach(println) sc.stop() }} 2.2.2 Action 行动算子Action 行动算子具有触发执行的特点，一个 Application 应用程序有几个 Action 类算子执行，就有几个 Job 运行。 点击快速跳转到 SparkRDD Action 列表： Actions 下面列出常用的行动算子及用法： reduce(func) 通过函数 func 聚集数据集中的所有元素。func 函数接受 2 个参数，返回 1 个值。这个函数必须是关联性的，确保可以被正确的并发执行。关于 reduce 的执行过程，可以对比 Scala 中类似的 reduce函数。 不同于 Transformation 算子，执行后结果是RDD，执行 Action 算子之后，其结果不再是 RDD，而是一个标量。 1234567891011121314151617181920212223242526272829object Action_Reduce { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Action_Reduce&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) // val list = List(0, 5, 15, 20, 25, 30) // 如果用这个 list 输出为: 95 val list = List( &quot;1,李 磊,22,175&quot;, &quot;2,刘银鹏,23,175&quot;, &quot;3,齐彦鹏,22,180&quot;, &quot;4,杨 柳,22,168&quot;, &quot;5,敦 鹏,20,175&quot; ) val listRDD = sc.parallelize(list) listRDD.foreach(println) println() val ret = listRDD.reduce( (v1, v2) =&gt; { println(&quot;[LOGS] v1 &gt; &quot; + v1 + &quot; &quot;) println(&quot;[LOGS] v2 &gt; &quot; + v2 + &quot; &quot;) println(&quot;[LOGS] v1 + v2 &gt; &quot; + (v1 + v2)) v1 + v2 } ) println(&quot;ret: &quot; + ret) sc.stop() }} 1234567891011121314151617181920输出如下：3,齐彦鹏,22,1801,李 磊,22,1754,杨 柳,22,1682,刘银鹏,23,1755,敦 鹏,20,175[LOGS] v1 &gt; 3,齐彦鹏,22,180 [LOGS] v2 &gt; 4,杨 柳,22,168 [LOGS] v1 + v2 &gt; 3,齐彦鹏,22,1804,杨 柳,22,168[LOGS] v1 &gt; 3,齐彦鹏,22,1804,杨 柳,22,168 [LOGS] v2 &gt; 5,敦 鹏,20,175 [LOGS] v1 + v2 &gt; 3,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175[LOGS] v1 &gt; 1,李 磊,22,175 [LOGS] v2 &gt; 2,刘银鹏,23,175 [LOGS] v1 + v2 &gt; 1,李 磊,22,1752,刘银鹏,23,175[LOGS] v1 &gt; 1,李 磊,22,1752,刘银鹏,23,175 [LOGS] v2 &gt; 3,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175 [LOGS] v1 + v2 &gt; 1,李 磊,22,1752,刘银鹏,23,1753,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175ret: 1,李 磊,22,1752,刘银鹏,23,1753,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175 collect 在 Driver 的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用 filter 或者其它操作后，返回一个足够小的数据子集再使用，直接将整个 RDD 集 Collect返回，很可能会让 Driver 程序 OOM，这点尤其需要注意。 1 count 1 take 1 first 1 saveAsTextFile 1 foreach 1 saveAsNewAPIHadoopFile 1 2.2.3 其他算子Spark中还有许多其他种类的算子，体现了 Spark 算子的灵活性，其中包括将数据进行持久化的算子。 cache：懒加载执行的，必须有一个 Action 触发算子触发执行。 persist：懒加载执行的，必须有一个 Action 触发算子触发执行。 checkpoint：算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系。 2.2.3 宽依赖和窄依赖 宽依赖 ① 子 RDD 的每个分区依赖于所有的父 RDD 分区 ② 对单个 RDD 基于 Key进行重组和 Reduce，如 groupByKey、reduceByKey ③ 对两个 RDD 基于 Key 进行 join 和重组，如 join ④ 经过大量 shuffle 生成的 RDD，建议进行缓存。这样避免失败后重新计算带来的开销。 窄依赖 ① 子RDD的每个分区依赖于常数个父分区（与数据规模无关） ② 输入输出一对一的算子，且结果RDD的分区结构不变。主要是map/flatmap ③ 输入输出一对一的算子，但结果RDD的分区结构发生了变化，如union/coalesce ④ 从输入中选择部分元素的算子，如 filter、distinct、substract、sample","link":"/2024/06/12/SparkQuickIN/"},{"title":"CppGamingDEV_PVZ_BASE_EASYX","text":"🎮 从零开始进行 C++ 游戏开发[TOC] 游戏场景概念 什么是场景?如果将要游戏程序比作是一场有玩家参与的盛大演出，那场景就是演出过程中的一幕。在不同的幕中，会有不同的剧本逻辑，也可能会有不同的角色登场，这些角色即游戏开发中常提到的 GameObject 的概念。不论是玩家、敌人还是子弹、道具等，这些从概念上讲都是 GameObject 的范畴。他们接受着不同的场景剧本的指挥，进行着不同逻辑的演出。了解了这些，我们就可以对程序的流程进行宏观的划分，游戏的主菜单是一个场景，玩家角色选择界面也是一个场景，游戏局内的逻辑也需要放置在一个单独的场景中。所以我们就可以定义一个 Scene 场景基类，主菜单、角色选择、局内游戏 作为新类分别继承 Scene 类。 游戏主循环框架概念 什么是游戏主循环框架?游戏程序的主体是一个永不停歇的死循环。在每次循环中，我们读取玩家的操作信息，并根据这些操作，处理玩家数据更新，在最后的绘图阶段将游戏画面根据这些更新后的数据渲染出来。1234567初始化();while(true) { 读取操作(); 处理数据(); 绘制画面();}施放资源; C++ 头文件 为什么需要头文件卫士?如果不使用头文件卫士，可能会导致出现重定义的问题。例如，我们有一个头文件 A.h ，有一个头文件 B.h，头文件 B.h 中使用了 A.h 的内容。当我们在主函数里面使用了 A.h 和 B.h 的时候，使用 #include 时会将头文件中的内容全部复制到 #include 的部分。这样的话如果不加头文件卫士，就会导致重定义的问题，如下： 1234567891011121314151617181920212223// A.hint a;// B.h#include &quot;A.h&quot;int b;// main.cpp#include &quot;A.h&quot; // int a;#include &quot;B.h&quot; // int a; // int b; // 出现重定义错误// 错误信息如下In file included from B.h:1, from main.cpp:2:A.h:1:5: error: redefinition of 'int a' 1 | int a; | ^In file included from main.cpp:1:A.h:1:5: note: 'int a' previously declared here 1 | int a; | ^ 在 MSVC 编译器中，头文件卫士如下： 1#pragma once 在其他的一些编译器中，常写为这样： 1234#ifndef __HEADER_H__#define __HEADER_H__#endif 场景管理器游戏程序是一个巨大的死循环，也是一个巨大的状态机。不同的游戏场景代表着不同的状态，管理着这些状态的“状态机”，在游戏开发中有一个特殊的名字——场景管理器。","link":"/2024/06/23/CppGamingDEV-PVZ-BASE-EASYX/"},{"title":"DataVisualization_USE_R_ECHARTS","text":"📺 数据可视化基础（使用R和ECharts）本文会从 R 和 ECharts 两个方面来展开数据可视化技术的一些使用方法。 [TOC] 📊 1. 数据可视化概述 数据可视化概念 狭义概念：指的是数据用统计图表方式呈现 广义概念：是数据可视化、信息可视化以及科学可视化等等多个领域的统称 数据可视化分类 数据可视化 信息可视化 科学可视化 ⭐*** 数据可视化作用*** 数据表达：通过计算机图形图形技术来更加友好地展示数据信息，以方便人们理解和分析数据。 数据操作：以计算提供的界面、接口和协议等条件为基础完成人与数据的交互需求。 数据分析：通过计算机获得多维、多源、异构和海量数据所隐含信息的核心手段，它是数据存储、数据转换、数据计算和数据可视化的综合应用。 ⭐ 数据可视化的工具 R、ECharts、D3 … Python、pyecharts、Excel … 第三方商业工具：FineBI、FineReport … 学习数据可视化需要了解（具备）什么？ 对色彩的感知：了解原色、间色、复色等，并且了解色彩的视觉感受（心理感受），当然也就是了解即可，色彩这种东西天赋更重要一些，知道三原色是啥就行。 原色：红、黄、蓝。 间色：可存在多个间色，当原色之间 1:1 匹配时则为：橙、绿、紫。 复色：可存在多个复色（一定包含三原色），两个间色或一种原色和其对应的间色（黄 + 紫、蓝 + 橙）相混合得到复色。 色彩感受：轻重感、冷暖感、前后感、收缩感 … 数据可视化的基本流程 数据采集 数据处理 可视化映射 用户感知 数据可视化的一些设计技巧 设计原则：注重用户的视觉体验 设计技巧： ① （颜色）建立视觉层次，用醒目的颜色突出数据，淡化其他元素 ② （内容）高亮显示重点内容 ③ （跨度）提升不同区域的色阶跨度 ④ （场景）借助场景来表现数据指标 ⑤ （转换）将抽象的不易理解的数字转换为容易被人感知的图表 ⑥ （简洁）尽量让图表简洁 数据可视化的图表类型 在统计图表中每一种类型的图表中都可包含不同的数据可视化图形，如：柱状图、饼图、气泡图、热力图、趋势图、直方图、雷达图、色块图、漏斗图、和弦图、仪表盘、面积图、折线图、密度图 以及 K线图等。 🎫 2. R - R语言快速入门参考文献：数据科学中的 R 语言 (bookdown.org) 2.1 R 语言概述R语言是为数学研究工作者设计的一种数学编程语言，主要用于统计分析、绘图、数据挖掘数据可视化。 目前主流的数据分析语言有 R、Python、Matlab 等。 R Python Matlab 语言学习难易程度 入门难度低 入门难度一般 入门难度一般 使用场景 数据分析、数据挖掘、机器学习、数据可视化等。 数据分析、机器学习、矩阵运算、科学数据可视化、数字图像处理、Web应用、网络爬虫、系统运维等。 矩阵计算、数值分析、科学数据可视化、机器学习、符号计算、数字图像处理、数字信号处理、仿真模拟等。 第三方支持 拥有大量的 Packages，能够调用 C、C++、Fortran、Java等其他程序语言。 拥有大量的第三方库，能够简便地调用C、C++、Fortran、Java等其他程序语言。 拥有大量专业的工具箱，在新版本中加入了对C、C++，Java的支持。 流行领域 工业界 ≈ 学术界 工业界 ＞ 学术界 工业界 ≤ 学术界 软件成本 开源免费 开源免费 商业收费 2.2 R 语言中包的概念和使用 软件包和软件库 软件包：R 中的包是 R 函数、数据、预编译代码以一种定义完善的格式组成的集合。 软件库：软件库指的是一个包含了若干个包的目录。你可以拥有一个系统级别的软件库，也可以为每个用户单独设立一个软件库。 R 自带了一系列默认包（包括 base、datasets、utils、grDevices、graphics、stats以及methods），它们提供了种类繁多的默认函数和数据集。 base：包含基础的 R 函数 database：自带的数据包，里面的数据结构包括矩阵、向量、数据框等 utils：工具函数包 grDevices：基础绘图工具包，提供调整图形颜色和字体的功能，可用于配色 graphics：基础绘图工具包 stats：统计函数的扩展包 methods：R 对象和其他编程工具定义方法和类比的扩展包 包的相关操作 查看包的安装路径：.libPaths() 12&gt; .libPaths()[1] &quot;D:/R/R-4.3.3/library&quot; 查看已安装的包：library() 查看编译环境下已载入的包的列表：search() 123&gt; search() [1] &quot;.GlobalEnv&quot; &quot;tools:rstudio&quot; &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; [6] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; 下载包： 通过 CRAN (The Comprehensive R Archive Network) 下载。CRAN 是 R 语言的综合档案网络。用户可以从 CRAN 上下载并安装各种第三方包来扩展 R 语言的功能。 ① 指定包名进行安装：install.packages(&quot;PackageName&quot;) ② 使用图形界面安装：install.packages() 手动安装：https://cran.r-project.org/web/packages/XML/index.html 使用 RStudio 安装 包的卸载：remove.packages(&quot;PackageName&quot;) 包的载入：library(&quot;PackageName&quot;) 12345&gt; library(&quot;XML&quot;)&gt; search() # 可以看到 &quot;package:XML&quot; 已经被加载进来了 [1] &quot;.GlobalEnv&quot; &quot;package:XML&quot; &quot;tools:rstudio&quot; &quot;package:stats&quot; &quot;package:graphics&quot; [6] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; [11] &quot;package:base&quot; 包的取消载入：detach(&quot;package:PackageName&quot;, unload = True) 1234&gt; detach(&quot;package:XML&quot;, unload = TRUE)&gt; search() [1] &quot;.GlobalEnv&quot; &quot;tools:rstudio&quot; &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; [6] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; 2.3 R 语言的工作空间的概念和使用 概念：工作空间(Work Space) 就是当前 R 的工作环境，它储存着所有用户定义的对象（向量、矩阵、函数、数据框、列表）。在一个 R 会话结束时，用户可以将当前工作空间保存到一个镜像中，并在下次启动 R 时自动载入它。 设置工作目录：setwd() 或使用 RStudio 可视化窗口进行设置（Session → Set Working Directory） 获取工作目录：getwd() 2.4 R 语言的六大原子数据类型 数字型(Numeric) 逻辑型(Logical) 整型(Integer) 字符型(Character) 复合型(Complex) 原子型(Raw) 对于不同原子数据类型可以按照如下方式定义： 1234567891011121314151617181920212223242526272829&gt; # 数值型(Numeric)&gt; varNumeric = 12.5&gt; varNumeric[1] 12.5&gt; # 逻辑型(Logical)&gt; varLogical = TRUE&gt; varLogical[1] TRUE&gt; # 整型(Integer)&gt; varInteger = 25L&gt; varInteger[1] 25&gt; # 字符型(Character)&gt; varCharacter = &quot;Hello World&quot;&gt; varCharacter[1] &quot;Hello World&quot;&gt; # 复合型(Complex)&gt; varComplex = 2 + 3i&gt; varComplex[1] 2+3i&gt; # 原子型(Raw)&gt; varRaw = charToRaw('Hello')&gt; varRaw[1] 48 65 6c 6c 6f 2.5 R 语言变量使用 定义变量并赋值：使用 &lt;-、=、-&gt; 123456789101112131415&gt; var1 = 12.5 # 定义 var1, 并赋值为 12.5&gt; print(var1) # 输出 var1[1] 12.5&gt; var2 &lt;- 13 # 定义 var2, 并赋值为 13(由右向左)&gt; print(var2) # 输出 var2[1] 13&gt; 14 -&gt; var3 # 定义 var3, 并赋值为 14(由左向右)&gt; print(var3) # 输出 var3[1] 14&gt; var2 -&gt; var3&gt; print(var3)[1] 13 变量的打印：print() 或 cat() print() 和 cat() 都可以向控制台输出文字，区别在于 print() 有返回值，其会返回输出的内容，无法使用转义字符，当使用转义字符时，会直接输出转义字符；而 cat() 没有返回值，可以使用转义字符。 1234567891011121314151617&gt; varX = &quot;Hello World&quot;&gt; varY = &quot;Hello World\\n&quot;&gt; printX &lt;- print(varX)[1] &quot;Hello World&quot;&gt; catX &lt;- cat(varX, &quot;\\n&quot;) # 如果不添加转义字符会无法换行Hello World &gt; printY &lt;- print(varY)[1] &quot;Hello World\\n&quot; # 使用 print() 直接将转义字符进行了输出&gt; catY &lt;- cat(varY)Hello World # 使用 cat() 没有将转义字符直接输出, 而是转化成了响应的格式, 如&quot;换行&quot;&gt; print(printX) # print() 有返回值[1] &quot;Hello World&quot;&gt; print(catX) # cat() 没有返回值NULL 变量的查看：ls() 1234567891011121314151617&gt; ls() # 输出所有变量 [1] &quot;catX&quot; &quot;catY&quot; &quot;cyl.f&quot; &quot;printX&quot; &quot;printY&quot; &quot;score&quot; &quot;tempdens&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot;[11] &quot;varX&quot; &quot;varY&quot; &gt; ls(pattern = &quot;var&quot;) # 模式匹配部分变量, 包含 &quot;var&quot; 的[1] &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; &quot;varX&quot; &quot;varY&quot;&gt; ls(pattern = &quot;*&quot;) # 模式匹配部分变量, 使用通配符 [1] &quot;catX&quot; &quot;catY&quot; &quot;cyl.f&quot; &quot;printX&quot; &quot;printY&quot; &quot;score&quot; &quot;tempdens&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; [11] &quot;varX&quot; &quot;varY&quot; &gt; ls(pattern = &quot;2&quot;) # 模式匹配部分变量, 包含 &quot;2&quot; 的[1] &quot;var2&quot;&gt; ls(all.names = TRUE) # [1] &quot;.Random.seed&quot; &quot;catX&quot; &quot;catY&quot; &quot;cyl.f&quot; &quot;printX&quot; &quot;printY&quot; &quot;score&quot; [8] &quot;tempdens&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; &quot;varX&quot; &quot;varY&quot; 变量的删除：rm() 12345678&gt; rm(&quot;var1&quot;, &quot;var2&quot;) # 删除 &quot;var1&quot; 和 &quot;var2&quot;&gt; rm(list = ls(pattern = &quot;var&quot;)) # 删除 匹配到 &quot;var&quot; 的变量, list 是一个可以指定的参数&gt; rmlist = ls(pattern = &quot;X&quot;)&gt; rmlist[1] &quot;catX&quot; &quot;printX&quot;&gt; rm(list = rmlist) [⭐] 变量的数据类型判别和转换 判别：is.XXX()，返回 True/False 123456789101112131415161718192021222324252627282930313233&gt; # 数值型(Numeric)&gt; varNumeric = 12.5&gt; varNumeric[1] 12.5&gt; is.numeric(varNumeric)[1] TRUE&gt; is.logical(varNumeric)[1] FALSE&gt; is.integer(varNumeric)[1] FALSE&gt; is.character(varNumeric)[1] FALSE&gt; is.complex(varNumeric)[1] FALSE&gt; is.raw(varNumeric)[1] FALSE&gt; # 逻辑型(Logical)&gt; varLogical = TRUE&gt; varLogical[1] TRUE&gt; is.numeric(varLogical)[1] FALSE&gt; is.logical(varLogical)[1] TRUE&gt; is.integer(varLogical)[1] FALSE&gt; is.character(varLogical)[1] FALSE&gt; is.complex(varLogical)[1] FALSE&gt; is.raw(varLogical)[1] FALSE 转换：as.XXX() 12345678910111213141516&gt; # 变量类型转换&gt; varNumToChar &lt;- as.character(varNumeric)&gt; varNumToChar[1] &quot;12.5&quot;&gt; is.numeric(varNumToChar)[1] FALSE&gt; is.logical(varNumToChar)[1] FALSE&gt; is.integer(varNumToChar)[1] FALSE&gt; is.character(varNumToChar)[1] TRUE &lt;-------------------- 可以看到已经转换成了字符型&gt; is.complex(varNumToChar)[1] FALSE&gt; is.raw(varNumToChar)[1] FALSE [⭐] 变量的数据类型查看 mode()：查看数据元素类型 typeof()：查看数据元素类型，基本等同于 mode() ，比 mode() 更为详细 class()：查看数据结构，vector、matrix、array、dataframe、list 123456789101112131415161718192021222324252627282930313233343536373839404142&gt; # 查看变量类型&gt; mode(varNumeric)[1] &quot;numeric&quot;&gt; typeof(varNumeric)[1] &quot;double&quot;&gt; class(varNumeric)[1] &quot;numeric&quot;&gt; mode(varLogical)[1] &quot;logical&quot;&gt; typeof(varLogical)[1] &quot;logical&quot;&gt; class(varLogical)[1] &quot;logical&quot;&gt; mode(varInteger)[1] &quot;numeric&quot;&gt; typeof(varInteger)[1] &quot;integer&quot;&gt; class(varInteger)[1] &quot;integer&quot;&gt; mode(varCharacter)[1] &quot;character&quot;&gt; typeof(varCharacter)[1] &quot;character&quot;&gt; class(varCharacter)[1] &quot;character&quot;&gt; mode(varComplex)[1] &quot;complex&quot;&gt; typeof(varComplex)[1] &quot;complex&quot;&gt; class(varComplex)[1] &quot;complex&quot;&gt; mode(varRaw)[1] &quot;raw&quot;&gt; typeof(varRaw)[1] &quot;raw&quot;&gt; class(varRaw)[1] &quot;raw&quot; [⭐] 字符串的应用 R语言中的文本，或者原子数据类型中 Character。在 R 语言中的单引号' ' 或 双引号&quot; &quot; 中写入的任何值都被视为字符串。在字符串构造中应用的规则： 在字符串的开头和结尾的引号应该是两个双引号或两个单引号。它们不能被混合。 双引号可以插入到以单引号开头和结尾的字符串中 单引号可以插入以双引号开头和结尾的字符串。 双引号不能插入以双引号开头和结尾的字符串。 单引号不能插入以单引号开头和结尾的字符串。 2.7 R 语言的运算符的概念和使用 运算符的概念 算数运算符 +加、-减、*乘、/除 %% 整除取余 %/% 整除 1234567891011&gt; # 相除&gt; 6 / 4[1] 1.5&gt; # %% 整除取余&gt; 6 %% 4[1] 2&gt; # %/% 整除&gt; 6 %/% 4[1] 1 关系运算符 &lt; 小于、&gt; 大于、= 等于 &lt;=小于等于、&gt;= 大于等于 != 不等于 逻辑运算符 &amp; | &amp;&amp; || &amp;&amp;和||为值逻辑，&amp;和|为位逻辑说人话就是，&amp;&amp; 和 ||是讲两个操作目的值做逻辑运算，无论操作对象是向量还是标量，返回值都是一个逻辑值，（NOW）&amp;&amp; 和 || 运算符只接受长度为 1 的逻辑值作为参数；而 &amp; 和 | 是讲两个对象按位比较，其返回值的长度与对象是标量还是向量有关。 12345678910111213141516171819&gt; # 逻辑运算符&gt; v &lt;- c(3, 1, TRUE, 2+3i)&gt; t &lt;- c(4, 1, FALSE, 2+3i)&gt; print(v&amp;t)[1] TRUE TRUE FALSE TRUE&gt; v &lt;- c(3, 0, TRUE, 2+2i)&gt; t &lt;- c(1, 3, TRUE, 2+3i)&gt; print(v &amp;&amp; t)Error in v &amp;&amp; t : 'length = 4' in coercion to 'logical(1)'&gt; TRUE &amp;&amp; TRUE[1] TRUE&gt; TRUE &amp;&amp; FALSE[1] FALSE&gt; FALSE &amp;&amp; TRUE[1] FALSE&gt; FALSE &amp;&amp; FALSE[1] FALSE 赋值运算符 &lt;- = -&gt; &lt;&lt;- -&gt;&gt; &lt;− 或 = 或 &lt;&lt;− 运算符： 称为左赋值。 1234567891011121314&gt; # 赋值运算符&gt; # 左赋值&gt; v1 &lt;- c(3,1,TRUE,2+3i)&gt; v2 &lt;&lt;- c(3,1,TRUE,2+3i)&gt; v3 = c(3,1,TRUE,2+3i)&gt; print(v1)[1] 3+0i 1+0i 1+0i 2+3i&gt; print(v2)[1] 3+0i 1+0i 1+0i 2+3i&gt; print(v3)[1] 3+0i 1+0i 1+0i 2+3i -&gt; 或 -&gt;&gt; 运算符：称为右赋值。 123456789&gt; # 右赋值&gt; c(3,1,TRUE,2+3i) -&gt; v1&gt; c(3,1,TRUE,2+3i) -&gt;&gt; v2 &gt; print(v1)[1] 3+0i 1+0i 1+0i 2+3i&gt; print(v2)[1] 3+0i 1+0i 1+0i 2+3i 其他运算符 : 为向量创建数字序列。 123&gt; v &lt;- 2:8&gt; print(v)[1] 2 3 4 5 6 7 8 %in% 用于确定元素是否属于向量。 1234567&gt; v1 &lt;- 8&gt; v2 &lt;- 12&gt; t &lt;- 1:10&gt; print(v1 %in% t) # 8 是否再 1~10 中[1] TRUE&gt; print(v2 %in% t) # 12 是否再 1~10 中[1] FALSE %*% 用于求两个向量的内积，也称为点乘 1234567891011121314151617181920212223242526272829&gt; M = matrix( c(2,6,5,1,10,4), nrow = 2, ncol = 3, byrow = TRUE)# M2, 6, 51, 10, 4# t(M)2, 16, 105, 4# M %*% t(M)2*2 + 6*6 + 5*5, 1*2 + 10* 6 + 4*52*1 + 10*6 + 4*5, 1*1 + 10*10 + 4*4&gt; t = M %*% t(M)&gt; print(t) [,1] [,2][1,] 65 82[2,] 82 117&gt; #===================================================================#&gt; M = matrix(c(2, 6, 5, 1, 10, 4), nrow = 2,ncol = 3, byrow = TRUE)&gt; N = matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2, byrow = TRUE)&gt; t = M %*% N&gt; print(t) [,1] [,2][1,] 45 58[2,] 51 66 2.8 R 语言的六大数据对象R 语言有六种基本的数据结构（数据对象）：向量vector、列表list、矩阵matrix、数组array、数据框data.frame 和 因子factor 类型。 2.8.1 向量 vector 概念：向量是最基本的 R 语言数据对象，向量的元素支持六种原子数据类型，即逻辑，整数，双精度，复合，字符和原型。 特征：一个向量的所有元素都必须属于相同的类型。如果不是，R将强制执行类型转换。 创建向量：c()、seq()、rep() c() 这里的c就是 combine 或 concatenate 的意思，它要求元素之间用英文的逗号分隔，且元素的数据类型是统一的，比如都是数值。c() 函数把一组数据聚合到了一起，就构成了一个向量。 12345678910111213141516171819&gt; # 创建向量&gt; # 使用 c()&gt; low &lt;- c(1, 2, 3)&gt; high &lt;- c(4, 5, 6)&gt; sequence &lt;- c(low, high)&gt; sequence[1] 1 2 3 4 5 6&gt; # 给变量命名&gt; x &lt;- c('a' = 5, 'b' = 6, 'c' = 7, 'd' = 8)&gt; xa b c d 5 6 7 8 &gt; x &lt;- c(5, 6, 7, 8)&gt; names(x) &lt;- c('a', 'b', 'c', 'd')&gt; xa b c d 5 6 7 8 如果向量元素很多，用手工一个个去输入，那就成了体力活，不现实。在特定情况下，有几种偷懒方法: seq() 函数可以生成等差数列，from 参数指定数列的起始值，to 参数指定数列的终止值，by 参数指定数值的间距。 1234&gt; # 使用 seq()&gt; s1 &lt;- seq(from = 0, to = 10, by = 0.5)&gt; s1 [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 rep() 是 repeat（重复）的意思，可以用于产生重复出现的数字序列：x 用于重复的向量，times 参数可以指定要生成的个数，each 参数可以指定每个元素重复的次数。 1234567891011&gt; s2 &lt;- rep(x = c(0, 1), times = 3)&gt; s2[1] 0 1 0 1 0 1&gt; s3 &lt;- rep(x = c(0, 1), each = 3)&gt; s3[1] 0 0 0 1 1 1&gt; s4 &lt;- rep(x = c(0, 1), time = 3, each = 3)&gt; s4[1] 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 向量元素访问 12345&gt; # 定义一个向量&gt; t &lt;- seq(from = 10, to = 20, by = 0.4)&gt; t [1] 10.0 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8[24] 19.2 19.6 20.0 t[index]：使用这种方式进行访问，index 默认从 1 开始。 12345&gt; # 方式 1&gt; t[1][1] 10&gt; t[2][1] 10.4 t[Logic Index]： TRUE 表示读取，FALSE 为不读取。 1234567&gt; # 方式 2&gt; t[c(TRUE, TRUE, FALSE, TRUE)] [1] 10.0 10.4 11.2 11.6 12.0 12.8 13.2 13.6 14.4 14.8 15.2 16.0 16.4 16.8 17.6 18.0 18.4 19.2 19.6 20.0&gt; t[c(1, 0, 2, 0, 3, 0, 1)][1] 10.0 10.4 10.8 10.0&gt; t[c(6, 0, 0, 0, 1, 0, 1)][1] 12 10 10 t[name]：通过 name 进行访问。 12345678910&gt; # 方式 3&gt; names(t) &lt;- c(&quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;, &quot;v4&quot;, &quot;v5&quot;, &quot;v6&quot;, &quot;v7&quot;, &quot;v8&quot;, &quot;v9&quot;, &quot;v10&quot;)&gt; t v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.0 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 19.2 19.6 20.0 &gt; t[&quot;v1&quot;]v1 10 t[-index]：索引为负数，则会删除该位置的元素。 1234567891011121314&gt; # 方式 4&gt; t v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.0 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 19.2 19.6 20.0 &gt; t[-1] v2 v3 v4 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 19.2 &lt;NA&gt; &lt;NA&gt; 19.6 20.0 &gt; t[c(-2: -4)] v1 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.0 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 19.2 19.6 20.0 向量元素运算 长度相同的向量 1234567891011121314151617&gt; # 长度不同的向量&gt; v3 = c(10, 20)&gt; v4 = c(10, 20, 10, 20, 10, 20)&gt; add.result = v1 + v3&gt; add.result = v1 + v4&gt; # 向量的运算&gt; # 长度相同的向量&gt; v1 = c(1, 2, 3, 4, 5, 6)&gt; v2 = c(6, 7, 8, 9, 10, 11)&gt; add.result = v1 + v2&gt; sub.result = v1 - v2&gt; add.result[1] 7 9 11 13 15 17&gt; sub.result[1] -5 -5 -5 -5 -5 -5 长度不同的向量 如果长度不同，较短的向量会循环补充到与较长的向量长度相同，在进行运算。但是前提是：长的向量的长度必须为短的向量的整数倍。 123456789&gt; # 长度不同的向量&gt; v3 = c(10, 20)&gt; v4 = c(10, 20, 10, 20, 10, 20)&gt; add.result = v1 + v3&gt; add.result[1] 11 22 13 24 15 26&gt; add.result = v1 + v4&gt; add.result[1] 11 22 13 24 15 26 2.8.2 列表 list 概念：列表是 R 语言的对象集合，可以用来保存不同类型的数据，可以是数字、字符串、向量、另一个列表等，当然还可以包含矩阵和函数。R语言创建列表使用 list() 函数。 列表的创建 1234567891011121314151617181920&gt; # 列表的创建&gt; list_data = list(&quot;Hello&quot;, &quot;World&quot;, c(1, 10), TRUE, 51.23, 119L)&gt; list_data[[1]][1] &quot;Hello&quot;[[2]][1] &quot;World&quot;[[3]][1] 1 10[[4]][1] TRUE[[5]][1] 51.23[[6]][1] 119 列表元素的重命名(names 函数) 12345678910111213141516171819202122232425262728293031323334&gt; # 列表的重命名&gt; list_data &lt;- list(c(&quot;Hello&quot;, &quot;World&quot;), matrix(c(1, 2, 3, 4, 5, 6), nrow=2), list(&quot;Happy&quot;, 1314L))&gt; list_data[[1]][1] &quot;Hello&quot; &quot;World&quot;[[2]] [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6[[3]][[3]][[1]][1] &quot;Happy&quot;[[3]][[2]][1] 1314&gt; names(list_data) &lt;- c(&quot;HelloString&quot;, &quot;MatrixElem&quot;, &quot;ListElem&quot;)&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314 列表元素的访问——增删改查 查 1234567891011121314151617181920212223242526272829303132333435363738394041&gt; # (查)&gt; # 根据索引访问&gt; list_data[0]named list()&gt; list_data[3]$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314&gt; # 根据元素名称访问&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;&gt; list_data$ListElem[[1]][1] &quot;Happy&quot;[[2]][1] 1314&gt; list_data[4]&lt;-&quot;Hello&quot;&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314[[4]][1] &quot;Hello&quot; 增 123456789101112131415161718192021&gt; # (增)&gt; list_data[4]&lt;-&quot;Hello&quot;&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314[[4]][1] &quot;Hello&quot; 改 1234567891011121314151617181920&gt; # (改)&gt; list_data[4] &lt;- 12&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314[[4]][1] 12 删 1234567891011121314151617&gt; # (删)&gt; list_data[4] &lt;- NULL&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314 列表的合并：使用 c 可以将多个列表合并为一个列表 12345678910111213141516171819202122232425262728&gt; # 合并列表&gt; list1 &lt;- list(1, 2, 3)&gt; list2 &lt;- list(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)&gt; class(list1)[1] &quot;list&quot;&gt; class(list2)[1] &quot;list&quot;&gt; merged.list &lt;- c(list1, list2)&gt; merged.list[[1]][1] 1[[2]][1] 2[[3]][1] 3[[4]][1] &quot;A&quot;[[5]][1] &quot;B&quot;[[6]][1] &quot;C&quot;&gt; class(merged.list)[1] &quot;list&quot; 列表和向量之间的转换 1234567891011121314151617181920212223242526272829303132333435&gt; # 列表转换为向量&gt; list1 &lt;- list(1 : 5)&gt; list2 &lt;- list(10: 14)&gt; list3 &lt;- list(&quot;hello&quot;, matrix(c(1, 2, 3, 4), nrow=2), 3)&gt; list1[[1]][1] 1 2 3 4 5&gt; list2[[1]][1] 10 11 12 13 14&gt; list3[[1]][1] &quot;hello&quot;[[2]] [,1] [,2][1,] 1 3[2,] 2 4[[3]][1] 3&gt; v1 &lt;- unlist(list1)&gt; v2 &lt;- unlist(list2)&gt; v3 &lt;- unlist(list3)&gt; class(v1)[1] &quot;integer&quot;&gt; class(v2)[1] &quot;integer&quot;&gt; class(v3)[1] &quot;character&quot;&gt; v3[1] &quot;hello&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;3&quot; 2.8.3 矩阵 matrix 概念：R 语言为线性代数的研究提供了矩阵类型，这种数据结构很类似于其它语言中的二维数组。 矩阵的创建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&gt; # 矩阵的创建&gt; m1 = matrix(data=c(3: 14))&gt; m1 [,1] [1,] 3 [2,] 4 [3,] 5 [4,] 6 [5,] 7 [6,] 8 [7,] 9 [8,] 10 [9,] 11[10,] 12[11,] 13[12,] 14&gt; m2 = matrix(data=c(3: 14), nrow=4)&gt; m2 [,1] [,2] [,3][1,] 3 7 11[2,] 4 8 12[3,] 5 9 13[4,] 6 10 14&gt; m2 = matrix(data=c(3: 14), ncol=4)&gt; m2 [,1] [,2] [,3] [,4][1,] 3 6 9 12[2,] 4 7 10 13[3,] 5 8 11 14&gt; m3 &lt;- matrix(data=c(3, 14), nrow=4, byrow=TRUE) # 默认为 TRUE&gt; m3 [,1][1,] 3[2,] 14[3,] 3[4,] 14&gt; rownames &lt;- c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;, &quot;row4&quot;) &gt; colnames &lt;- c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;)&gt; m4 &lt;- matrix(data=c(3: 14), nrow=4, byrow=TRUE, dimnames=list(rownames, colnames))&gt; m4 col1 col2 col3row1 3 4 5row2 6 7 8row3 9 10 11row4 12 13 14 矩阵转置 1234567&gt; # 矩阵转置 t()&gt; # R 语言矩阵提供了 t() 函数, 可以实现矩阵的行列转换&gt; t(m4) row1 row2 row3 row4col1 3 6 9 12col2 4 7 10 13col3 5 8 11 14 矩阵元素访问 1234&gt; m4[c(&quot;row1&quot;, &quot;row4&quot;), c(&quot;col1&quot;, &quot;col3&quot;)] col1 col3row1 3 5row4 12 14 矩阵相关操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt; # 矩阵元素操作&gt; # 矩阵计算&gt; m1 &lt;- matrix(data=c(3: 14), nrow=4)&gt; m1 [,1] [,2] [,3][1,] 3 7 11[2,] 4 8 12[3,] 5 9 13[4,] 6 10 14&gt; m2 &lt;- matrix(data=c(1: 12), nrow=4)&gt; m2 [,1] [,2] [,3][1,] 1 5 9[2,] 2 6 10[3,] 3 7 11[4,] 4 8 12&gt; m1 + m2 [,1] [,2] [,3][1,] 4 12 20[2,] 6 14 22[3,] 8 16 24[4,] 10 18 26&gt; # 矩阵转换 &gt; # 矩阵 -&gt; 向量&gt; is.matrix(m1)[1] TRUE&gt; class(m1)[1] &quot;matrix&quot; &quot;array&quot; &gt; v1 &lt;- as.vector(m1)&gt; v1 [1] 3 4 5 6 7 8 9 10 11 12 13 14&gt; class(v1)[1] &quot;integer&quot;&gt; # 矩阵组合&gt; # cbind() 把其自变量横向拼成一个大矩阵, 横向组合, 行数一致&gt; # rbind() 把其自变量纵向拼成一个大矩阵, 纵向组合, 列数一致&gt; v1 = c(1, 2, 3, 4, 5)&gt; v2 = c(6, 7, 8, 9,10)&gt; y1 = cbind(v1, v2)&gt; y2 = rbind(v1, v2)&gt; y1 v1 v2[1,] 1 6[2,] 2 7[3,] 3 8[4,] 4 9[5,] 5 10&gt; y2 [,1] [,2] [,3] [,4] [,5]v1 1 2 3 4 5v2 6 7 8 9 10&gt; 2.8.4 数组 array 概念：数组也是 R 语言的对象，R 语言可以创建一维或多维数组。 数组的创建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt; # 创建数组&gt; # 一维数组&gt; v1 &lt;- c(5, 9, 1)&gt; a1 &lt;- array(v1)&gt; a1[1] 5 9 1&gt; is.array(a1)[1] TRUE&gt; a &lt;- array(10: 15, dim=9)&gt; a[1] 10 11 12 13 14 15 10 11 12&gt; a &lt;- array(10: 15, dim=c(2))&gt; a[1] 10 11&gt; &gt; # 二维数组&gt; a &lt;- array(10: 15, dim=c(2, 3)) # 10, 11, 12, 13, 14, 15&gt; a [,1] [,2] [,3][1,] 10 12 14[2,] 11 13 15&gt; is.array(a)[1] TRUE&gt; &gt; # 三维数组&gt; a &lt;- array(10: 15, dim=c(2, 3, 2))&gt; a, , 1 [,1] [,2] [,3][1,] 10 12 14[2,] 11 13 15, , 2 [,1] [,2] [,3][1,] 10 12 14[2,] 11 13 15&gt; column.names &lt;- c(&quot;COL1&quot;, &quot;COL2&quot;, &quot;COL3&quot;)&gt; row.names &lt;- c(&quot;ROW1&quot;, &quot;ROW2&quot;, &quot;ROW3&quot;)&gt; matrix.names &lt;- c(&quot;Matrix1&quot;, &quot;Matrix2&quot;)&gt; &gt; vector1 &lt;- c(5, 9, 3)&gt; vector2 &lt;- c(10, 11, 12, 13, 14, 15)&gt; a &lt;- array(c(vector1, vector2), dim=c(3, 3, 2), dimnames = list(row.names, column.names, matrix.names))&gt; a, , Matrix1 COL1 COL2 COL3ROW1 5 10 13ROW2 9 11 14ROW3 3 12 15, , Matrix2 COL1 COL2 COL3ROW1 5 10 13ROW2 9 11 14ROW3 3 12 15 数组元素访问 123456789101112131415161718192021&gt; # 数组访问&gt; a[1] # 一维数组的访问[1] 5&gt; a[1: 6] # 一维数组的访问——向量访问[1] 5 9 3 10 11 12&gt; &gt; dim2_a &lt;- array(c(vector1, vector2), dim=c(3, 3))&gt; dim2_a [,1] [,2] [,3][1,] 5 10 13[2,] 9 11 14[3,] 3 12 15&gt; dim2_a[1: 1] # 二维数组的访问, 访问的必须是二维数组[1] 5&gt; dim2_a[1: 2, 1: 2] # 二维数组的访问——矩阵访问 [,1] [,2][1,] 5 10[2,] 9 11&gt; &gt; a[3, 3, 1] # 三维数组[1] 15 2.8.5 数据框 data.frame 概念：数据框 DataFrame 可以理解成我们常说的表格，数据框是 R 语言的数据结构，是特殊的二维列表。数据框每一列都有一个唯一的列名，长度都是相等的，同一列的数据类型需要一致，不同列的数据类型可以不一样。数据框的特点： 列名称应为非空。 行名称应该是唯一的。 存储在数据框中的数据可以是数字，字符型等。 每个列应包含相同数量的数据项。 数据框的创建 12345678910111213&gt; # 数据框&gt; # 创建数据框对象&gt; df1 &lt;- data.frame(+ # tag = value+ 姓名 = c(&quot;张三&quot;, &quot;李四&quot;, &quot;王五&quot;),+ 工号 = c(&quot;001&quot;, &quot;002&quot;, &quot;003&quot;),+ 月薪 = c(1000, 2000, 3000)+ )&gt; df1 姓名 工号 月薪1 张三 001 10002 李四 002 20003 王五 003 3000 查看数据框结构 123456789101112131415&gt; # 查看数据框结构(Structure)&gt; str(df1)'data.frame': 3 obs. of 3 variables: $ 姓名: chr &quot;张三&quot; &quot;李四&quot; &quot;王五&quot; $ 工号: chr &quot;001&quot; &quot;002&quot; &quot;003&quot; $ 月薪: num 1000 2000 3000&gt; # 查看数据框描述性统计量&gt; summary(df1) 姓名 工号 月薪 Length:3 Length:3 Min. :1000 Class :character Class :character 1st Qu.:1500 Mode :character Mode :character Median :2000 Mean :2000 3rd Qu.:2500 Max. :3000 数据框访问、新增列 123456789101112131415161718192021222324252627282930313233343536373839&gt; # 提取数据框中的列&gt; df1$姓名[1] &quot;张三&quot; &quot;李四&quot; &quot;王五&quot;&gt; df2 &lt;- data.frame(df1$姓名, df1$月薪)&gt; df2 df1.姓名 df1.月薪1 张三 10002 李四 20003 王五 3000&gt; &gt; # 提取数据框中的行&gt; # 提取前两行&gt; result &lt;- df1[1:2, ]&gt; result 姓名 工号 月薪1 张三 001 10002 李四 002 2000&gt; &gt; # 提取前两行, 前两列&gt; result &lt;- df1[1:2, 1:3]&gt; result 姓名 工号 月薪1 张三 001 10002 李四 002 2000&gt; &gt; # 提取第1、3行, 第1、2列的数据&gt; result &lt;- df1[c(1, 3), c(1, 2)]&gt; result 姓名 工号1 张三 0013 王五 003&gt; &gt; # 新增列&gt; df1$部门 &lt;- c(&quot;运营&quot;, &quot;技术&quot;, &quot;运营&quot;)&gt; df1 姓名 工号 月薪 部门1 张三 001 1000 运营2 李四 002 2000 技术3 王五 003 3000 运营 数据框合并 12# 数据框合并# cbind()/rbind() 数据框筛选 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt; # 数据框筛选&gt; student &lt;- data.frame(+ sno = c(1, 2, 3),+ sname = c(&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;),+ sage = c(10, 12, 14)+ )&gt; student sno sname sage1 1 zhangsan 102 2 lisi 123 3 wangwu 14&gt; &gt; student[1] # 第一个字段 sno1 12 23 3&gt; student[1, 2] # 第一行, 第二列[1] &quot;zhangsan&quot;&gt; student$sage&gt;12 # FALSE FALSE TRUE[1] FALSE FALSE TRUE&gt; &gt; # [...] 里面一个参数取字段, 两个参数取行列&gt; student[student$sage&gt;11] # 执行判断, 得到结果 c(FALSE, TRUE, TRUE) sname sage1 zhangsan 102 lisi 123 wangwu 14&gt; student[c(FALSE, TRUE, TRUE)] # 使用 c(FALSE, TRUE, TRUE) 进行运算 sname sage1 zhangsan 102 lisi 123 wangwu 14&gt; student[student$sage&gt;11, ] # 选择年龄大于 11 的, 并取所有字段 sno sname sage2 2 lisi 123 3 wangwu 14&gt; student[c(FALSE, TRUE, TRUE), ] # 选择年龄大于 11 的行, 并取所有字段 sno sname sage2 2 lisi 123 3 wangwu 14&gt; student[student$sage&gt;11, 2] # 选择年龄大于 11 的, 并取第 2 个字段[1] &quot;lisi&quot; &quot;wangwu&quot;&gt; student[student$sage&gt;11, c(1, 2)] # 选择年龄大于 11 的, 并取第 1、2 个字段 sno sname2 2 lisi3 3 wangwu&gt; &gt; subset(student, select = c(&quot;sname&quot;, &quot;sage&quot;)) # 可以理解为: SELECT sname, sage FROM student; sname sage1 zhangsan 102 lisi 123 wangwu 14&gt; subset(student, select = c(&quot;sname&quot;, &quot;sage&quot;), sage&gt;11 &amp; sno == 2) # 可以理解问: SELECT sname, sage FROM student WHERE sage&gt;11 AND sno=2; sname sage2 lisi 12 2.8.6 因子 factor 概念：因子用于存储不同类别的数据类型，例如：人的性别有 男 和 女 两个类别，年龄来分可以有 未成年人 和 成年人 。 因子创建 123456789101112&gt; # 因子创建&gt; # 例 1:男 2:女&gt; x &lt;- c(1, 2, 1, 2, 2, 1, 3)&gt; factor(x)[1] 1 2 1 2 2 1 3Levels: 1 2 3&gt; factor(x, levels = c(1, 2))[1] 1 2 1 2 2 1 &lt;NA&gt;Levels: 1 2&gt; factor(x, levels = c(1, 2), labels = c(&quot;男&quot;, &quot;女&quot;))[1] 男 女 男 女 女 男 &lt;NA&gt;Levels: 男 女 2.9 R 语言判断、控制、循环2.9.1 判断语句 if 语句 1234567891011&gt; # 控制语句&gt; x &lt;- TRUE&gt; if (x) {+ print(&quot;x 为真&quot;)+ }[1] &quot;x 为真&quot;&gt; &gt; if (is.logical(x)) {+ print(&quot;x 是逻辑型数据&quot;)+ }[1] &quot;x 是逻辑型数据&quot; if...else... 语句 和 if...else if... 语句 12345678910111213141516171819&gt; x &lt;- 10L&gt; if (is.logical(x)) {+ print(&quot;x 是逻辑型数据&quot;)+ } else if(is.numeric(x)) {+ print(&quot;x 是数值型数据&quot;)+ } else {+ print(&quot;x 不是逻辑型数据, 也不是数值型数据&quot;)+ }[1] &quot;x 是数值型数据&quot;&gt; &gt; y &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;e&quot;)&gt; if(&quot;e&quot; %in% y) {+ print(&quot;y 中包含 `e`&quot;)+ } else if (&quot;a&quot; %in% y) {+ print(&quot;y 中包含 `a`&quot;)+ } else {+ print(&quot;不存在&quot;)+ }[1] &quot;y 中包含 `e`&quot; switch 语句 12345678910111213141516171819202122232425262728293031323334&gt; # switch语句&gt; # x &lt;- switch (object,&gt; # case = action&gt; # )&gt; &gt; # expression(object) 是一个常量表达式, 可以是整数或字符串&gt; # 如果是整数, 则返回对应 case 的位置值, 如果整数不在范围内则返回 NULL&gt; x &lt;- switch (1,+ &quot;a&quot;,+ &quot;b&quot;,+ &quot;c&quot;,+ &quot;d&quot;+ )&gt; x[1] &quot;a&quot;&gt; &gt; x &lt;- switch (1,+ &quot;a&quot; = &quot;a1&quot;,+ &quot;b&quot; = &quot;b1&quot;,+ &quot;c&quot; = &quot;c1&quot;,+ &quot;d&quot; = &quot;d1&quot;+ )&gt; x[1] &quot;a1&quot;&gt; &gt; # expression 如果是字符串, 则对应的是 case 中变量名对应的值, 没有匹配则没有返回值&gt; x &lt;- switch (&quot;c&quot;,+ &quot;a&quot; = 1,+ &quot;b&quot; = 2,+ &quot;c&quot; = &quot;cn&quot;+ )&gt; &gt; x[1] &quot;cn&quot; 2.9.2 循环语句 repeat + if(...) { break } 12345678910111213&gt; # 循环语句&gt; # repeat + if(...) { break }&gt; v &lt;- c(&quot;Google&quot;, &quot;Micrsoft&quot;, &quot;OpenAI&quot;, &quot;LinkIn&quot;)&gt; cnt &lt;- 3&gt; repeat {+ print(v[cnt])+ cnt &lt;- cnt + 1+ if(cnt &gt; 6) { break; }+ }[1] &quot;OpenAI&quot;[1] &quot;LinkIn&quot;[1] NA[1] NA while() 1234567891011&gt; &gt; # while 循环&gt; v &lt;- c(&quot;Google&quot;, &quot;Micrsoft&quot;, &quot;OpenAI&quot;, &quot;LinkIn&quot;)&gt; cnt &lt;- 3&gt; while(cnt &lt;= 5) {+ print(v[cnt])+ cnt &lt;- cnt + 1+ }[1] &quot;OpenAI&quot;[1] &quot;LinkIn&quot;[1] NA for() 12345678910111213141516&gt; # for 循环&gt; letters [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot;&gt; LETTERS [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; &quot;M&quot; &quot;N&quot; &quot;O&quot; &quot;P&quot; &quot;Q&quot; &quot;R&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot;&gt; LETTERS[1: 6][1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot;&gt; for (i in LETTERS[1: 6]) {+ print(i)+ }[1] &quot;A&quot;[1] &quot;B&quot;[1] &quot;C&quot;[1] &quot;D&quot;[1] &quot;E&quot;[1] &quot;F&quot; 2.9.3 控制语句 break 12345678910&gt; # break语句&gt; for (i in LETTERS[1: 6]) {+ if (i == &quot;D&quot;) {+ break;+ }+ print(i)+ }[1] &quot;A&quot;[1] &quot;B&quot;[1] &quot;C&quot; next 123456789101112&gt; # next语句&gt; for (i in LETTERS[1: 6]) {+ if (i == &quot;D&quot;) {+ next;+ }+ print(i)+ }[1] &quot;A&quot;[1] &quot;B&quot;[1] &quot;C&quot;[1] &quot;E&quot;[1] &quot;F&quot; 2.10 R 语言函数概念：函数是一组一起执行一个任务的语句。 2.10.1 内置函数： print() cat() list() array() 2.10.2 自定义函数R 语言函数通常由以下几个部分组成： 函数名 参数 函数体 返回值 具体格式如下： 12345function_name &lt;- function(arg_1, arg_2, ...) { # 函数体 # 执行的代码块 return(output) # 返回值} 下面由一些例子来演示： 只演示几种情况，其他情况（如有参函数、有默认参数值的函数都和其他语言类似）。 123456789101112131415&gt; # R 语言函数&gt; new.function &lt;- function() {+ # 函数体+ for(i in 1: 6){+ print(i^2)+ }+ }&gt; &gt; new.function()[1] 1[1] 4[1] 9[1] 16[1] 25[1] 36 2.11 R语言读写数据文件、数据库2.11.1 读写数据文件 读写类型 读写方法 备注 从键盘输入数据 edit() 使用方法见下方代码 读写 .txt 格式文件 读 read.table 写 write.table - 读写 .csv 格式文件 读 read.csv()写 write.csv() - 读写 .xlsx 格式文件 读 read.xlsx()写 write.xlsx() 使用之前需要下载并加载 R语言的 xlsx 包 从键盘中输入数据 1234567891011&gt; # 从键盘读入数据, edit() 和 fix() 可以可视化的输入数据&gt; mydata &lt;- data.frame(age=numeric(0), gender=character(0), weight=numeric(0))&gt; mydata[1] age gender weight&lt;0 行&gt; (或0-长度的row.names)&gt; edit(mydata) age gender weight1 12 2 1202 13 2 NA&gt; &gt; fix(mydata) 读写 .txt 格式文件 12345stutxt &lt;- read.table(&quot;/path/to/file.txt&quot;, header=TRUE, sep=&quot;,&quot;)stutxtclass(stutxt)write.table(stutxt, &quot;/path/to/write.txt&quot;, sep=&quot;,&quot;) 读写 .csv 格式文件 1234csvfile &lt;- read.csv(&quot;/path/to/file.csv&quot;)csvfilewrite.csv(csvfile, &quot;/path/to/write.csv&quot;) 读写 .xlsx 格式文件 123456789101112install.packages(&quot;xlsx&quot;)library()search()library(&quot;x1sx&quot;)student &lt;- read.xlsx(&quot;/path/to/file.xlsx&quot;, sheetIndex=1, encoding = &quot;UTF-8&quot;)studentcity&lt;-read.xlsx(&quot;/path/to/file.xlsx&quot;, sheetName = &quot;city&quot;,encoding = &quot;UTF-8&quot;)citywrite.xlsx(city, &quot;/path/to/write.xlsx&quot;, sheetName = &quot;city&quot;) 2.11.2 读写数据库非重点，有需要可自行了解 2.12 R 语言基础绘图2.12.1 R 语言绘制barplot() 条形图、堆叠条形图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394####### 1. 绘制条形图 barplot()# 1.1 height: 高度, 通过这个参数可以指定要画多少个柱子, 以及每个柱子的高度, 其值有两种——向量、矩阵# 1.1.1 向量 vector, 此时会根据向量的长度确定图中有多少个柱子, 向量中的每个值就是高度barplot(height = 1: 5)# 1.1.2 矩阵 matrix, 此时用于画堆积柱状图data &lt;- cbind(a = 1: 4, b = 1: 4)dataclass(data)barplot(height = data)# 1.2 width: 宽度, 控制每个柱子的宽度, 默认值为 1, 值得注意的是, 这个参数的值是可以循环使用的barplot(height = 1: 5)barplot(height = 1: 5, width=1)barplot(height = 1: 5, width=c(1, 1, 1, 1, 1))barplot(height = 1: 5, width=1: 5)data &lt;- cbind(a = 1: 4, b = 1:4)barplot(height = data)barplot(height = data, width=1)barplot(height = data, width=1:2)# 1.3 space: 间隔, 指定每个柱子左边的空白区域的宽度, 这个值为一个百分比, 默认值为 0.2barplot(height=1: 5, space=1)barplot(height=1: 5, space=5: 1)barplot(height=data, space=1)barplot(height=data, space=1: 2)# 1.4 names.arg: 每个柱子下面的标记, 当 height 为 vector 时, 默认的标记为向量的 names 属性barplot(height=1:3, names.arg=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))v &lt;- 1:3names(v) &lt;- c(&quot;c&quot; ,&quot;b&quot; ,&quot;a&quot;)vbarplot(height=v)# 1.5 horiz: 逻辑值, 默认为 FALSE, 当值为 TRUE 时, 将 X 轴和 Y 轴转置barplot(height=1:3, horiz = FALSE)barplot(height=1:3, horiz = TRUE)databarplot(height=data, horiz= FALSE)barplot(height=data, horiz = TRUE)# 1.6 col: 柱子的填充色, 默认为灰色# 1.7 border: 柱子边框的颜色, 默认为 black, 当 border为 NA时, 表示没有边框# 1.8 main: 设置图形标题c &lt;- colors()cc[2: 5]barplot(1: 4, col=c[2:5], border = NA, main=&quot;图形标题&quot;)barplot(1: 4, col=rainbow(4))barplot(1: 4, col=c(&quot;red&quot;, &quot;green&quot;, &quot;antiquewhite&quot;, &quot;white&quot;))barplot(data, col=rainbow(4))# 1.9 &amp; 1.10 density 设置柱子用线条填充, density 控制线条的密度，angle 控制线条的角度par(mfrow=c(1, 3))parbarplot(rep(1, 4), density = 1, angle = 30, main = &quot;density = 1&quot;)barplot(rep(1, 4), density = 2, angle = 90, main = &quot;density = 2&quot;)barplot(rep(1, 4), density = 3, angle = 180, main = &quot;density = 3&quot;)# 1.11 axes: 逻辑值, 控制是否显示轴线(如 y 轴线)barplot(data, axes = F)barplot(data, axes = T)# 1.12 axisnames: 控制是否显示柱子的标签barplot(data, axisnames = F)barplot(data, axisnames = T)# 1.13 beside: 当输入参数为矩阵时, 默认是堆积柱状图, beside 默认值为 FALSE;# beside = FALSE 时, 条形图的高度是矩阵的数值, 矩形条是水平堆叠的# beside = TRUE 时, 条形图的高度是矩阵的数值, 矩形条是并列的databarplot(data, beside = F)barplot(data, beside = T)# 1.14 legend.text: 图例的文字说明# 只有当 height 参数的值是 matrix 时, 才起作用# 默认显示的是 matrix 的 rownames 属性# 其值有两种指定形式:# 第一种: 逻辑值, 是否需要显示图例, 默认为FALSEpar(mfrow=c(1, 1))datarownames(data) &lt;- c(&quot;A1&quot;, &quot;A2&quot;, &quot;A3&quot;, &quot;A4&quot;)databarplot(data, legend.text = TRUE)# 第二种, 指定图例中的文字内容# 相当于修改了 matrix 的 rownames 属性barplot(data, legend.text = c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;)) 2.12.2 R 语言绘制 饼图、3D饼图绘制12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455####### 2. 绘制饼图# 2.1 x: 数值向量, 表示每个扇形的面积x &lt;- c(21, 62, 10, 53)pie(x)# 2.2 labels: 字符型向量, 表示各扇形面积标签x&lt;-c(21, 62, 10, 53)names(x) &lt;- c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;)xpie(x)x&lt;-c(21, 62, 10, 53)pie(x)pie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;))# 2.3 radius: 饼图的半径x&lt;-c(21, 62, 10, 53)pie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;), radius = 1)# 2.4 main: 饼图的标题pie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;), radius =1, main=&quot;饼图&quot;)# 2.5 clockwise: 一个逻辑值, 用来指定饼图各个切片是否按顺时针做出分割pie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;), radius =1, main=&quot;饼图&quot;, clockwise = TRUE)# 2.6 &amp; 2.7# angle: 设置底纹的斜率# density: 底纹的密度, 默认值为NULLpie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;), radius =1, main=&quot;饼图&quot;, clockwise = TRUE, density = 10, angle = 20)# 2.8 col: 每个扇面的颜色, 相当于调色板pie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;), radius =1, main=&quot;饼图&quot;, clockwise = TRUE, density = 10, angle = 20, col = rainbow(length(x)))pie(x, labels = c(&quot;London&quot;, &quot;New York&quot;, &quot;singapore&quot;, &quot;Mumbai&quot;), radius =1, main=&quot;饼图&quot;, clockwise = TRUE, col = rainbow(length(x)))# 2.9 设置百分数piepercent &lt;- round(x/sum(x) * 100, 1)pie(x, piepercent, radius=0.5, main=&quot;饼图&quot;, clockwise= TRUE, col=rainbow(length(x)))# 2.10 设置图例legend(&quot;topright&quot;, c(&quot;London&quot;, &quot;New York&quot;, &quot;Singapore&quot;, &quot;Mumbai&quot;), fill = rainbow(length(x)), cex = 0.8)legend(&quot;topright&quot;, c(&quot;London&quot;, &quot;New York&quot;, &quot;Singapore&quot;, &quot;Mumbai&quot;), fill = rainbow(length(x)), horiz = TRUE, cex = 0.5)# 2.11 绘制 3D 饼图install.packages(&quot;plotrix&quot;)library(&quot;plotrix&quot;)lbl &lt;- c(&quot;London&quot;, &quot;New York&quot;, &quot;Singapore&quot;, &quot;Mumbai&quot;)pie3D(x, labels = lbl, explode = 0.1, main = &quot;城市 3D 饼图&quot;) 2.12.3 R 语言散点图、折线图、散点矩阵图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879####### 3. 绘制散点图、折线图、散点矩阵图# 3.1 绘制散点图 plot()# x: 横坐标, x轴的数据集合# y: 纵坐标, y轴的数据集合# type: 绘图的类型, type=&quot;p&quot; 为点(默认); # type=&quot;l&quot; 在图形中数据显示为线;# type=&quot;b&quot; 在图形中数据显示为点和连接线;# type=&quot;o&quot; 在图形中数据点覆盖在线上;# type=&quot;h&quot; 在图形中数据显示为从点到x轴的垂直线; # type=&quot;s&quot; 在图形中数据显示为阶梯图;# type=&quot;n&quot; 在图形中数据不显示# main: 图表标题# xlab/ylab: x轴/y轴的标签名称# xlim/ylim: x轴/y轴的范围# axes: 布尔值, 是否绘制两个坐标轴# pch: 点的样式由 pch 的取值决定# cex: 表示相对于默认大小缩放倍数的数值。默认大小为1, 1.5表示放大为默认值的1.5倍; 0.5表示做小为默认大小的0.5倍;cars # 加载车辆时速-刹车距离数据集cars$speed # 查看车辆时速cars$dist # 查看车辆刹车距离plot(cars$speed, cars$dist)plot(cars[, 1], cars[, 2])plot(cars)plot(cars, main=&quot;车辆时速-刹车距离散点图&quot;, xlab=&quot;车速&quot;, ylab=&quot;刹车距离&quot;, col=&quot;red&quot;, pch=4, cex=1)with(classData, plot(height, weight))# 3.2 绘制折线图 plot()# type: 绘图的类型, type=&quot;p&quot; 为点(默认); # type=&quot;l&quot; 在图形中数据显示为线;# type=&quot;b&quot; 在图形中数据显示为点和连接线;# type=&quot;o&quot; 在图形中数据点覆盖在线上;# type=&quot;h&quot; 在图形中数据显示为从点到 x 轴的垂直线(直线条形图);# type=&quot;s&quot; 在图形中数据显示为阶梯图;# type=&quot;c&quot; 在图形中数据虚线图;# type=&quot;n&quot; 在图形中数据不显示plot(cars$speed, cars$dist, type=&quot;l&quot;)plot(cars$speed, cars$dist, type=&quot;b&quot;)plot(cars$speed, cars$dist, type=&quot;o&quot;)# plot 可以配合lines() 函数绘制出多条折线图# lines() 函数必须与 plot() 函数配合才能使用, 先用 plot() 函数画出一个图形# round(x, digits = 0) 舍入函数, x 为数值向量, digits表示有效位数, 可以为负数, 负数表示舍入十位(-1)、百位(-2)...等x &lt;- 1: 10beijing &lt;- round(rnorm(10, mean = 20, sd = 2), 2)beijingshanghai &lt;- round(rnorm(10, mean = 23, sd = 3), 2)shanghaiguangzhou &lt;- round(rnorm(10, mean = 18, sd = 1), 2)guangzhouplot(x, beijing, type = &quot;l&quot;, ylim = c(16, 30), lwd=2, main = &quot;北上广近十天气温变化趋势图&quot;)lines(x, shanghai, type = &quot;b&quot;, col=&quot;blue&quot;, lwd=2)lines(x, guangzhou, type = &quot;l&quot;, col=&quot;red&quot;, lwd=2)# 用 plot() 会单独绘制一张新图plot(x, shanghai, type = &quot;l&quot;, ylim = c(16, 30), lwd=2, main = &quot;北上广近十天气温变化趋势图&quot;)plot(x, guangzhou, type = &quot;l&quot;, ylim = c(16, 30), lwd=2, main = &quot;北上广近十天气温变化趋势图&quot;)# 3.3 绘制散点矩阵图mtcars # 加载 mtcars 数据集, 数据来自1974年美国汽车趋势杂志, 包括32辆汽车(1973-74款) 的油耗和10个方面的汽车设计和性能 # mpg(Miles Per Gallon): 油耗(每加仑英里(美国)), 功能更强大, 更重的汽车往往消耗更多的燃油 # cyl(cylinders): 汽缸数, 功率更大的汽车通常具有更多的汽缸 # disp(displacement, in cubic inches): 排量(立方英寸), 发动机气缸的总容积 # hp(horsepower): 总马力：这是汽车产生的功率的量度 # drat：driveshaft ratio:后轴比率：这描述了驱动轴的转动与车轮的转动如何对应。较高的值会降低燃油效率。 # wt(weight): 重量(1000磅) # qsec(1/4 mile time; a measure of acceleration): 1/4英里时间：汽车的速度和加速度 # vs(‘V’ or straight - engine shape): 发动机缸体, 表示车辆的发动机形状是&quot;V&quot; 形还是更常见的直形。 # am(transmission; auto or manual): 变速箱, 这表示汽车的变速箱是自动(0)还是手动(1)。 # gear(abbreviation of gears): 前进挡的数量, 跑车往往具有更多的挡位 # carb(abbreviation of carburetors): 化油器数量, 与更强大的发动机相关pairs(mtcars[, c(&quot;wt&quot;, &quot;mpg&quot;, &quot;disp&quot;, &quot;cyl&quot;)])pairs(vs~wt + mpg + disp + cyl, data=mtcars) 2.12.4 R 语言绘制直方图1 2.12.5 R 语言绘制核密度图1 2.12.6 R 语言绘制箱线图1 2.12.7 R 语言小提琴图12345678910111213141516171819202122232425####### 7. 小提琴图 # vioplot 包中的 vioplot 函数用于绘制小提琴图, 小提琴图是核密度图与箱线图的结合# install.packages(&quot;vioplot&quot;)# install.packages(&quot;vioplot&quot;)library(&quot;vioplot&quot;)# `==` 意为比较等号两端是否相等mpg4 &lt;- mtcars$mpg[mtcars$cyl == 4]mpg4mpg6 &lt;- mtcars$mpg[mtcars$cyl == 6]mpg6mpg8 &lt;- mtcars$mpg[mtcars$cyl == 8]mpg8vioplot(mpg4, mpg6, mpg8, names=c(&quot;4-cyl&quot;, &quot;6-cyl&quot;, &quot;8-cyl&quot;), col=&quot;gold&quot;)title(&quot;四/六/八缸 车辆的油耗 (英里/加仑)&quot;, ylab=&quot;Miles Per Gallon&quot;, xlab=&quot;Num Of Cylinders&quot;)# 图片的保存pdf(file=&quot;fig-vioplot.pdf&quot;, height = 10, width = 10, family = &quot;GB1&quot;)vioplot(mpg~cyl, data=mtcars, xlab=&quot;气缸数&quot;, ylab=&quot;加仑每英里&quot;, main=&quot;里程数据&quot;, names=c(&quot;4-cyl&quot;, &quot;6-cyl&quot;, &quot;8-cyl&quot;))dev.off # 关闭图像设备, 我们经常使用图像设备来创建和保存图形输出 # 当我们完成图形绘制并且不再需要将图形输出到设备上时, 我们需要关闭图像设备以释放系统资源并确保图形正确保存 # 在 R 中, 我们可以使用 dev.off() 函数来关闭当前活动的图像设备 2.13 R 绘图常用拓展库 ggplot2 绘图简介：ggplot2是一款强大的图形可视化R包，其作图方式易于理解，且生成的图形精美，定制化程度也很高，是 R 语言中很流行的可视化工具之一。 ggplot2 包是 R 的一个作图用的扩展包，它实现了 图形的语法，将一个作图任务分成若干子任务，只需要完成各个子任务就可以完成作图。在绘制常用图形时，只需要两个步骤： 将图形所展现的数据输入到 ggplot() 函数中 调用某个 geom_xxx() 函数，指定图形类型，如散点图、面积图、曲线图、盒型图等。geom_xxx() 提供了各种基本图形，如geom_blank() 不绘制图形、geom_point() 每个观测为一个散点、geom_hline(), geom_vline(), geom_abline() 绘制线等。 ggplot2 的作图的一般步骤为： 准备数据：一般为数据框，且一般为长表，即每个观测时间占一行，每个观测变量占一列 输入数据：将数据输入到 ggplot() 函数中，并指定参与作图的每个变量分别映射到哪些图形特性 选择图形类型：选择一个合适的图形类型，函数名以 geom_ 开头，如 geom_point() 表示散点图。 设定标题和图例位置等，如 labs()，仍用加号连接。 模板如下： 12345678p &lt;- ggplot(data = &lt;输入数据框&gt;, mapping = aes(&lt;维度&gt;=&lt;变量名&gt;, &lt;维度&gt;=&lt;变量名&gt;, &lt;...&gt;=&lt;...&gt;))p + geom_&lt;图形类型&gt;(&lt;...&gt;) + scala_&lt;映射&gt;_&lt;类型&gt;(&lt;...&gt;) + coord_&lt;类型&gt;(&lt;...&gt;) + labs(&lt;...&gt;) 2.13.1 ggplot2 绘制散点图12345678910111213141516171819202122####### ggplot2 绘图# install.packages(&quot;ggplot2&quot;) # 安装 ggplot2 绘图库# install.packages(&quot;gapminder&quot;) # 安装 gapminder 数据集library(&quot;ggplot2&quot;) # 绘图包library(&quot;gapminder&quot;) # 数据集包gapminderdev.new() # 打开绘图设备p &lt;- ggplot(data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp))p &lt;- ggplot(data = gapminder, aes(gdpPercap, lifeExp))p + geom_point() # 散点图p + geom_smooth() # 拟合曲线图p + geom_point() + geom_smooth()dev.off() # 关闭绘图设备 2.13.2 ggplot2 绘制面积图1234567891011121314151617# 面积图# 定义数据框 data.framedf &lt;- data.frame( year=c(1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716), sunspots=c( 5.0, 11.0, 16.0, 23.0, 36.0, 58.0, 29.0, 20.0, 10.0, 8.0, 3.0, 0.0, 0.0, 2.0, 11.0, 27.0, 47.0))dfp &lt;- ggplot(data = df, mapping = aes(year, sunspots))p + geom_area(fill = &quot;blue&quot;, alpha = 0.2, col = &quot;red&quot;) 2.13.3 ggplot2 绘制堆叠面积图1 🎫 3. ECharts - ECharts的简单使用3.1 ECharts 简介 ECharts 概念：一个基于 JavaScript 的开源可视化图表库（别人写好了 .js，我直接使用），可以流畅的运行在 PC 和移动设备上，兼容当前绝大部分浏览器（IE 9/10/11，Chrome，Firefox，Safari 等），底层依赖矢量图形库 *ZRender*，提供直观，交互丰富，可高度个性化定制的数据可视化图表。 官网：ECharts 特性：ECharts Feature 获取 Apache ECharts 通过 GitHub 获取 Apache/ECharts 项目的 release 页面可以找到各个版本的链接。点击下载页面下方 Assets 中的 Source code，解压后即为包含完整 ECharts 功能的文件。 echarts-5.4.0/echarts-5.4.0/dist/echarts.common.js 与 echarts-5.4.0/echarts-5.4.0/dist/echarts.common.min.js：体积适中，常用版，支持常见的图表和组件 echarts-5.4.0/echarts-5.4.0/dist/echarts.js 与 echarts-5.4.0/echarts-5.4.0/dist/echarts.min.js：体积最大，完整版，包含所有支持的图表和组件。 echarts-5.4.0/echarts-5.4.0/dist/echarts.simple.js 与 echarts-5.4.0/echarts-5.4.0/dist/echarts.simple.min.js：体积较小，精简版，包含最常用的图表和组件。 从 npm 获取 npm install echarts 从 CDN 获取 &lt;script src=&quot;https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; 在线定制 3.2 第一个 ECharts 程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;第一个 ECharts 程序&lt;/title&gt; &lt;!-- 1. 引入 ECharts.js 类库文件 --&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; window.onload=function() { // 3. 实例化 ECharts 对象 var dom = document.getElementById(&quot;main&quot;); var myChart = echarts.init(dom); // 4. 指定图表的配置项和数据 var option = { title: { text: &quot;ECharts 入门案例&quot; }, tooltip: { }, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '帽子', '高跟鞋', '袜子'] }, yAxis: { }, series: [{ name: '销量', type: 'bar', data: [5, 20, 26, 10, 20, 8] }] }; // 5. 使用刚指定的配置项和数据显示图表 myChart.setOption(option); } &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- 2. 准备一个放图表的容器 --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 3.3 ECharts 绘图流程 新建一个 HTML 文件，并在网页头部 &lt;head&gt; ... &lt;/head&gt; 部分引入在线的 ECharts 类库或者本地已经下载好的类库。 123456&lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;!-- 引入在线的 ECharts 类库或者本地已经下载好的类库 --&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt; 在网页创建一个容器，为 ECharts 准备一个具备大小（宽高）的 DOM （Document Object Model, 文件对象模型 ) 。 1234&lt;body&gt; &lt;!-- 为 ECharts 准备一个具备大小(宽高)的 DOM --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px; height: 400px&quot;&gt;&lt;/div&gt;&lt;/body&gt; 当页面加载时，获取 DOM，并初始化 ECharts 对象 123456&lt;script&gt; window.onload=function() { var dom = document.getElementById(&quot;main&quot;); var myChart = echarts.init(dom); }&lt;/script&gt; 指定配置项信息 123456789101112131415161718192021&lt;script&gt; window.onload=function() { // 省略上一步的操作 var options = { title: { text: &quot;ECharts 入门案例&quot; }, tooltip: { }, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { }, series: [ {name:'销量', type: 'bar', data: [5, 20, 36, 10, 10, 20]} ] } }&lt;/script&gt; 用刚指定的配置项和数据显示图表 123456&lt;script&gt; window.onload=function() { // 省略上一步的操作... myChart.setOption(option); }&lt;/script&gt; 3.4 ECharts 常用配置项详解完成简单柱状图的绘制需要包含的组件包括： xAxis：X 轴组件 yAxis：Y 轴组件 series：系列组件（我更喜欢称其序列组件） 完善图形补充组件： title：标题组件 legend：图例组件 tooltip：提示框组件 toolbox：工具栏配置项 datazoom：数据区域缩放 3.5 ECharts 样式设置3.5.1 ECharts 颜色主题(Theme) 内置颜色主题（dark、light） 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;样式设置&lt;/title&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;height=900px; width:400px&quot;&gt;&lt;/div&gt; &lt;script&gt; window.onload=function() { var dom = document.getElementById(&quot;main&quot;); // 内置颜色主题 // var myCharts = echarts.init(dom, &quot;light&quot;); var myCharts = echarts.init(dom, &quot;dark&quot;); var options = { title: { text: &quot;ECharts 入门案例&quot; }, tooltip: { }, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { }, series: [ {name:'销量', type: 'bar', data: [5, 20, 36, 10, 10, 20]} ] } myCharts.setOption(options); } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 自定义颜色主题 通过 JavaScript 定义颜色主题 1 通过 Json + jQuery 定义颜色主题 1 3.5.2 ECharts 调色盘 全局调色盘 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;ECharts 全局调色盘&lt;/title&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;width: 900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;script&gt; window.onload=function() { var dom = document.getElementById('main'); // 获取到 DOM var myCharts = echarts.init(dom); // 初始化 ECharts对象 // 定义选项 var options = { // 颜色组件——全局调色盘 color: [&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;], xAxis: { show: true, position: &quot;bottom&quot;, type: 'category', data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { type: 'value', max: 50 }, // 系列组件 series: [ { type: &quot;bar&quot;, name: &quot;销量1&quot;, data: [5, 20, 36, 10, 10, 20] }, { type: &quot;line&quot;, name: &quot;销量2&quot;, data: [4, 30, 26, 14, 8, 15] } ] } myCharts.setOption(options); } &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 系列调色盘 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;ECharts 系列调色盘&lt;/title&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;width: 900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;script&gt; window.onload=function() { var dom = document.getElementById('main'); // 获取到 DOM var myCharts = echarts.init(dom); // 初始化 ECharts对象 // 定义选项 var options = { // 颜色组件——全局调色盘 color: [&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;], xAxis: { show: true, position: &quot;bottom&quot;, type: 'category', data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { type: 'value', max: 50 }, // 系列组件 series: [ { type: &quot;bar&quot;, name: &quot;销量1&quot;, data: [5, 20, 36, 10, 10, 20], color: [&quot;red&quot;, &quot;green&quot;] }, { type: &quot;line&quot;, name: &quot;销量2&quot;, data: [4, 30, 26, 14, 8, 15] } ] } myCharts.setOption(options); } &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 系列调色盘与全局调色盘同时出现： 当系列调色盘与全局调色盘同时出现时，图表使用系列调色盘中配置的颜色。 3.5.3 ECharts 直接样式设置直接样式设置（itemStyle、lineStyle、areaStyle、label、…） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;ECharts 全局调色盘&lt;/title&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;width:900px; height: 400px;&quot;&gt;&lt;/div&gt; &lt;script&gt; window.onload=function() { var dom = document.getElementById(&quot;main&quot;); var myChart = echarts.init(dom); var options = { xAxis: { show: true, position: 'bottom', type: 'category', data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { type: 'value', max: 50 }, series: [ { type: 'scatter', name: '销量1', data: [5, 20, 36, 10, 10, 20], symbol: 'triangle', symbolSize:20, itemStyle: { color: 'red', borderWidth: 2, borderColor: 'blue', }, emphasis: { itemStyle: { color: 'green' }, label: { show: true, formmatter: &quot;{a}\\n{b}{c}&quot;, // 这是一个 label position: 'right' } } },{ type: 'scatter', name: '销量2', data: [4, 30, 26, 14, 8, 15] } ], title: { show: true, text: '主标题-点击访问百度', link: 'https//www.baidu.com/', target: 'blank', // 或者 self 也可以 subtext: '副标题-点击访问个人网站', sublink: 'http://hello-nilera.com/', subtarget textStyle: { color: 'red', fontSize: 30, fontStyle: 'italic', fontWeight: 'normal' } } } myChart.setOption(options); } &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 3.5.4 ECharts 视觉映射(Visual Map)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;样式设置——视觉映射组件&lt;/title&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;width: 900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;script&gt; window.onload=function() { var dom = document.getElementById('main'); var myChart = echarts.init(dom); var options = { // 连续型视觉映射组件 /* visualMap: [ { type: 'continuous', min: 0, max: 40, range: [4, 15], calculable: true } ], */ // 分段型视觉映射组件 visualMap: [{ type: 'piecewise', min: 0, max: 40 }], title: { text: 'ECharts 视觉映射组件' }, tooltip: { }, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { }, series: [ { name: '销量 1', type: 'bar', data: [5, 20, 36, 10, 10, 20] },{ name: '销量 2', type: 'bar', data: [8, 12, 30, 15, 18, 26] } ] } myChart.setOption(options); } &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 3.6 ECharts 中的事件和行为概念：在 Apache ECharts 的图表中用户的操作将会触发相应的事件。开发者可以监听这些事件，然后通过回调函数做相应的处理。 事件分类： 在 ECharts 中事件分为两种类型，一种是用户鼠标操作点击，或者 hover 图表的图形时触发的事件，还有一种是用户在使用可以交互的组件后触发的行为事件，例如在切换图例开关时触发的 legendselectchanged 事件（这里需要注意切换图例开关是不会触发事件的），数据区域缩放时触发的 datazoom 事件等等。 3.7 ECharts 实例3.7.1 柱状图绘制简单的柱状图，一些配置项： xAxis/yAxis type：category 类目轴；value 数值轴；time 时间；log 对数轴 data：设置类目的数据（数组） series type：设置系列的类型（设置绘制什么图表） name：设置系列名称，会用在 tooltip 和 legend 中 data：系列中的数据内容数组，定义当前这个系列中每个柱子的高度 label：设置图形上的文本标签（把数值显示在每个柱子上) itemStyle：设置图形样式 showBackground：设置是否显示背景色（默认不显示） backgroundStyle：设置背景色 3.7.2 折线图、堆叠折线图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;width: 900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;script&gt; window.onload=function() { var dom = document.getElementById(&quot;main&quot;); var myCharts = echarts.init(dom); var options = { xAxis: { type: 'category', data: ['周一', '周二', '周三', '周四', '周五'] }, yAxis: { type: 'value' }, series: [ { type: 'line', data: [10, 20, 30, 20, 10]} ], toolbox: { feature: { show: true, dataZoom: { // 数据区域缩放 // 框选型数据区域缩放组件(dataZoomSelect): 提供一个选框进行区域缩放。 // 即 toolbox.feature.dataZoom yAxisIndex: 'none' }, dataView: { readOnly: true }, magicType: { type: ['line', 'bar'] }, restore: { // 配置项还原 }, saveAsImage: { type: 'jpg' } } } } myCharts.setOption(options); } &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 3.7.3 面积图和堆叠面积图3.7.4 雷达图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.min.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;main&quot; class=&quot;main&quot; style=&quot;width:900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;script&gt; var dom = document.getElementById(&quot;main&quot;); var myCharts = echarts.init(dom); var options = { radar: { indicator: [ { name: '身高' }, { name: '体重' }, { name: '年龄' }, { name: '跑步速度' } ], }, series: [ { type: 'radar', data: [ { name: '训练前', value: [178, 60, 20, 50] }, { name: '训练后', value: [178, 58, 20, 52] }, { name: '训练一年后', value: [180, 55, 21, 55]} ] }, ], tooltip: { }, toolbox: { show:true, feature:{ saveAsImage:{ show:true } } }, } myCharts.setOption(options); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt;","link":"/2024/06/22/DataVisualization-USE-R-ECHARTS/"},{"title":"HBaseExamReview","text":"HBase 考察知识点复习知识点（选择） Region的分裂 在HBase中，Region 是数据管理的基本单位，类似于关系型数据库中的分区。它是 HBase 数据存储和组织的核心概念之一。Region 的概念和特点：Region 是 HBase 数据管理的基本单位，它负责存储一定范围内的数据。每个 Region 由一个起始 RowKey 和 终止 RowKey 定义，负责存储该范围内的数据。与关系型数据库中的分区类似，HBase 中的 Region 可以根据数据量的大小进行动态调整。当一个Region 的数据量过大时，它会被 分裂 成两个新的 Region。相反，当数据量较小时，两个或多个 Region 可能会合并成一个新的Region。 当 MemStore 的数据超过阈值时，将数据溢写磁盘，生成一个 StoreFile 文件。当 Region 中最大Store 的大小超过阈值时，Region 分裂，等分成两个 Region，实现数据访问的负载均衡。新的 Region 的位置由 HMaster 来确定在哪个RegionServer 中。 查看表结构的命令 12345678910111213141516171819202122232425262728293031323334[subowen@bigdata ~]$ hbase shell2024-07-05 23:04:35,655 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase ShellUse &quot;help&quot; to get list of supported commands.Use &quot;exit&quot; to quit this interactive shell.For Reference, please visit: http://hbase.apache.org/2.0/book.html#shellVersion 2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Took 0.0028 secondshbase(main):001:0&gt; listTABLElogslogs_20240606logs_20240607usersdutcm:bigdata5 row(s)Took 1.2756 seconds=&gt; [&quot;logs&quot;, &quot;logs_20240606&quot;, &quot;logs_20240607&quot;, &quot;user&quot;, &quot;sdutcm:bigdata&quot;]hbase(main):002:0&gt; describe 'sdutcm:bigdata'Table sdutcm:bigdata is ENABLEDsdutcm:bigdataCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'info', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'msg', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}2 row(s)Took 0.3826 secondshbase(main):003:0&gt; scan 'sdutcm:bigdata'ROW COLUMN+CELL 2001 column=info:name, timestamp=1719026353219, value=wang1 row(s)Took 0.2295 secondshbase(main):004:0&gt; 启动和关闭 HBase Shell 的命令 启动 HBase Shell 之前需要保证 ZooKeeper、Hadoop HDFS 和 HBase 已经启动。可以使用 hbase shell 进入 HBase Shell，在 HBase Shell 中可以使用 exit 或 quit 退出。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[subowen@bigdata ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /home/subowen/apps/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[subowen@bigdata ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /home/subowen/apps/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: standalone[subowen@bigdata ~]$ start-dfs.shStarting namenodes on [bigdata]bigdata: starting namenode, logging to /home/subowen/apps/hadoop-2.7.6/logs/hadoop-subowen-namenode-bigdata.outbigdata: starting datanode, logging to /home/subowen/apps/hadoop-2.7.6/logs/hadoop-subowen-datanode-bigdata.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/subowen/apps/hadoop-2.7.6/logs/hadoop-subowen-secondarynamenode-bigdata.out[subowen@bigdata ~]$ start-hbase.shrunning master, logging to /home/subowen/apps/hbase-2.1.8/logs/hbase-subowen-master-bigdata.outbigdata: running regionserver, logging to /home/subowen/apps/hbase-2.1.8/logs/hbase-subowen-regionserver-bigdata.out[subowen@bigdata ~]$ jps1746 NameNode1572 QuorumPeerMain2071 SecondaryNameNode2874 Jps2379 HRegionServer1852 DataNode2285 HMaster[subowen@bigdata ~]$ hbase shell2024-07-05 22:47:57,331 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase ShellUse &quot;help&quot; to get list of supported commands.Use &quot;exit&quot; to quit this interactive shell.For Reference, please visit: http://hbase.apache.org/2.0/book.html#shellVersion 2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Took 0.0051 secondshbase(main):001:0&gt; exit[subowen@bigdata ~]$ hbase shell2024-07-05 22:52:46,337 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase ShellUse &quot;help&quot; to get list of supported commands.Use &quot;exit&quot; to quit this interactive shell.For Reference, please visit: http://hbase.apache.org/2.0/book.html#shellVersion 2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Took 0.0025 secondshbase(main):001:0&gt; quit[subowen@bigdata ~]$ 启动ZooKeeper和查看 ZooKeeper 的运行状态的命令 123456789[subowen@bigdata ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /home/subowen/apps/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[subowen@bigdata ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /home/subowen/apps/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: standalone 修改表结构的命令 使用 alter 命令可以对表的结构进行修改： 表名创建时写的所有和列族相关的信息，都可以后续通过alter修改，包括增加、删除列族。 ① 增加列族和修改信息都使用覆盖的方法 ​ 修改列族的版本，VERSIONS =&gt; 6： 12345hbase(main):001:0&gt; alter 'bigdata:person', NAME =&gt; 'name', VERSIONS =&gt; 6Updating all regions with the new schema...1/1 regions updated.Done.Took 4.0145 seconds ​ 添加列族 tel： 12345hbase(main):002:0&gt; alter 'bigdata:person', NAME =&gt; 'tel', VERSIONS =&gt; 6Updating all regions with the new schema...1/1 regions updated.Done.Took 2.4498 seconds ​ 查看修改后的数据： 1234567891011121314151617hbase(main):003:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'msg', VERSIONS =&gt; '6', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'tel', VERSIONS =&gt; '6', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}3 row(s)Took 0.0795 seconds ② 删除列族 ​ 删除列族可以用以下两种方式： 12345hbase(main):001:0&gt; alter 'bigdata:person', NAME =&gt; 'tel', METHOD =&gt; 'delete'Updating all regions with the new schema...1/1 regions updated.Done.Took 2.1046 seconds 12345hbase(main):002:0&gt; alter 'bigdata:person', 'delete' =&gt; 'msg'Updating all regions with the new schema...1/1 regions updated.Done.Took 2.9721 seconds HBase 的 Java API中，数据表描述的对象是什么？TableDescriptor 1TableDescriptor ZooKeeper 的访问端口号，HBase 的Web访问端口号 ZooKeeper 的访问端口号为 2181 HBase 的 Web 访问端口号为 16010 修改表中列族的Version版本的命令 1alter 'table_name', {NAME =&gt; 'column_family', VERSIONS =&gt; number_of_versions} 删除表中数据的命令 1delete 'ns:tb', 'rk', 'cf:col' 启动和关闭ZooKeeper的命令 12345# 启动 ZooKeeperzkServer.sh start# 关闭 ZooKeeperzkServer.sh stop 更新表数据的命令 1alter 'ns:tb', {NAME =&gt; 'cf:col', VERSIONS =&gt; 5} HBase安装时，配置文件有那几个？ hbase-env.sh hbase-site.xml regionservers .bashrc 关于NoSQL数据库有哪些？ HBase Cassandra BigTable 删除表操作 12disable 'ns:tb'drop 'ns:tb' HBase进程名，ZooKeeper 的进程名等相关进程 HMaster HRegionServer QuorumPeerMain HBase 依托于哪个文件存储系统？ HBase 主要依托于 Hadoop HDFS。 创建表的语法结构 1create 'ns:tb', {NAME =&gt; 'cf', VERSIONS=5} 查询表数据的Shell语法 123get 'r1', {COLUMN =&gt; 'cf:col'}scan 'ns:tb' 什么是列族COLUMN FAMILY HBase的数据类型 数据类型 基本数据类型 字符串(String)：用于存储文本数据，如名称、描述等。 整数(Int)：用于存储整数数据，如计数、编号等。 浮点数(Float)：用于存储小数数据，如金额、比率等。 布尔值(Boolean)：用于存储逻辑值，如是否、有效等。 复合数据类型 列族(Column Family)：列族是HBase表中数据的组织方式，用于存储一组相关的列。列族是HBase表中数据的基本组织单位，每个列族对应一个数据节点。 列(Column)：列是HBase表中数据的基本单位，用于存储一组相关的单元格。列可以包含多个单元格，每个单元格对应一个数据值。 单元格(Cell)：单元格是HBase表中数据的基本单位，用于存储一组相关的数据值。单元格包含一个键(Row Key)、一个列(Column)和一个值(Value)。 数据模型 NameSpace：命名空间，类似于关系型数据库的 Database 概念，每个命名空间下有多个表。HBase 两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表，default表是用户默认使用的命名空间。 Table：类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需要声明具体的列。因为数据存储是稀疏的，所以往 HBase 写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase 能够轻松应对字段变更的场景。 Row：HBase 表中的每行数据都由一个 RowKey 和多个 Column（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重要。 Column：HBase 中的每个列都由 Column Family(列族)和 Column Qualifier（列限定符）进行限定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。 Time Stamp：用于标识数据的不同版本Version，每条数据写入时，系统会自动为其加上该字段，其值为写入 HBase 的时间。 Cell：由 {rowkey, column Family:Column Qualifier, timestamp} 唯一确定的单元。Cell 中的数据全部是字节码形式存贮。 判断题 HBase 本身是用 C++ 语言编写的，因此具有很高的执行效率。（ × ） HBase 本身是由 Java 语言编写的，尽管使用 Java，其执行效率还是较高的。 HBase 优化目前主要工作集中于 Rowkey 优化。（ × ） HBase 的优化目前主要工作集中在 RowKey 的设计、参数优化、JVM调优等方面。 可以想象 HBase 是通过**”三维”**的表来存储数据的，通过时间戳保存数据的历史版本。（ √ ） 安装 HBase 完全分布式时，需要确保 SSH 和 JDK 是可以正常使用的。（ √ ） 在安装 HBase 的完全分布式部署时，确保 SSH 和 JDK 可以正常使用是非常重要的。具体来说： SSH：在分布式部署中，HBase 的各个节点之间需要进行通信和协作，而 SSH 是用于在不同节点之间建立安全连接的工具。确保节点之间可以互相访问、通信和传输文件是部署 HBase 所必需的。通常需要设置免密登录，以便在节点之间进行信息交换而无需手动输入密码。 JDK：HBase 是基于 Java 编程语言开发的，因此必须确保在每个节点上安装并配置了适当的 JDK。HBase 运行需要 Java 环境，且为了确保 HBase 能够正常工作，必须保证每个节点都能够使用正确的 JDK 版本。 Hadoop 中引入 HBase 的其中一个原因是：HDFS 在处理连续数据时比较吃力。（ × ）· HDFS 的设计目标之一就是为了有效地处理大规模数据，包括连续数据（如大文件、数据流等）和离散数据（如小文件、批量数据等）。Hadoop 中之所以引入 HBase，是需要 HBase 提供实时访问数据的能力，使得 Hadoop 生态系统更全面（此外还有高可伸缩性、高容错、多版本策略等）。 利用 HBase 技术可在廉价 PC Server 上搭建起大规模结构化存储集群。（ √ ） HBase 是 Hadoop 生态系统的一个组成部分。（ √ ） HBase 是继 Google Bigtable 之后创建的分布式、开源、版本化、非关系型数据库。它是 Hadoop 生态系统的重要组件，利用 HDFS 的容错功能，提供对数据的实时读写访问。 HBase 尽管是数据库，但也可以称为数据存储系统，因为它不提供触发器、查询语言和二级索引等 RDBMS 功能。 使用 HBase 时，无需启动服务就可以直接在开发环境中调用 HBase 的相关功能。（ × ） HBase 是基于 Hadoop 的，在使用 HBase 之前需要启动 ZooKeeper 和 Hadoop 才能使用 HBase。 配置 HBase 分布式部署的过程中，可以通过 cp 命令把文件复制到运行在不同机器上的节点中。（ × ） 可以使用 scp 命令、rsync命令，或者基于 rsync 编写工具 xsync 进行传输。 填空题 判断表是否存在，禁用表，删除表，创建表，查看命名空间等 Shell 命令。 判断表是否存在 ：exists 'namespace_name:table_name' 禁用/启用表：disable 'namespace_name:table_name' / enable namespace_name:table_name 删除表：删除表之前需要禁用表，disable 'namespace_name:table_name'，然后再删除表，drop 'namespace_name:tablename' 创建表：create 'namespace_name:table_name', {NAME =&gt; 'cf1', VERSION=5}, {NAME =&gt; 'cf2', VERSION=5} 查看命名空间：list_namespace HBase 的进程有哪些：关于这些进程的详细说明见 QuickPassHBase 或 简答题 T4。 HBase 的主要进程：HMaster、HRegionServer HBase 所依赖的两个外部的服务：ZooKeeper、HDFS 行键是什么：rowkey 在 HBase 中，行键唯一标识了 HBase 表的一行，可以通过单个行键或行键区间的方式访问表。行键保存为 字节数组 ，以字典序 排序存储。 HBase 是一个什么样的分布式存储系统。（非关系型） Apache HBase™ 是以 HDFS 为数据存储的，一种分布式、可扩展的 NoSQL 数据库。HBase 是一款面向列存储，用于存储处理海量数据的 NoSQL 数据库。 HBase 的理论原型是 Google 的 BigTable 论文。他是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。 简答题 HBase 的基础核心组件有哪些？分别什么作用？ HBase-Client客户端，用来访问 HBase 集群。可以和 Hbase 交互，也可以和 HRegionServer 交互。通过 HBase RPC 来访问对应的接口。 这里的客户端模式有多种，可以是 Thrift、Avro、Rest 等。 另外，hbase-client 自身会缓存 region 的一些信息。 ZooKeeper作用： HMaster 的高可用 存储 ROOT 表的地址、HMaster 的地址 存储所有 HRegionServer 的状态，监控 HRegionServer 的上下限 存储 HBase 的一些 Schema 和 Table 的元数据 HMaster HMaster可以启动多个，通过选举机制来保证只有一个 HMaster 正常运行并提供服务，其他的 HMaster 作为 standby 来保证高可用。HMaster 主要负责 表 和 Region 的管理工作。如： 用户对表的增删改查 管理 RegionServer 的负载均衡，调整 Region 的分布 在 RegionServer 宕机或下线后，负责迁移 RegionServer 上的 Region 到其他的 RegionServer 上 Region 在分裂后，负责分配新的 Region HRegionServerHRegionServer 是 HBase 中真正的工作节点，主要负责响应用户的 I/O 请求，向 HDFS 文件系统读写数据，以及 Region 的数据文件的合并和拆分等，是 HBase 中最核心的模块。 在 HBase 中，一张表由多个的 HRegion 组成，一个 HRegionServer 中管理着多个 HRegion 对象。而一个 HRegion 由多个HStore 组成，每个HStore对象都对应着表的一个列族 （Column Family）。之后，一个HStore又由一个MemStore和多个StoreFile组成。这些 StoreFile 就是hbase 存储在 HDFS上的数据文件，MemStore 表示还在内存中未刷新到文件上的那些数据。 HBase 的写流程， 读流程？ 写流程 写流程顺序正如 API 编写顺序，首先创建 HBase 的重量级连接。 ① 首先访问 ZooKeeper，获取 hbase:meta 表位于哪个 Region Server ② 访问对应的 Region Server，获取 hbase:meta 表，将其缓存到连接中，作为连接属性 MetaCache，由于 meta 表具有一定的数据量，导致了创建连接比较慢； 之后使用创建的连接获取 Table，这是一个轻量级的连接，只有在第一次创建的时候会检查表格是否存在访问RegionServer，之后在获取 Table 时不会访问 RegionServer ③ 调用 Table 的 put 方法写入数据，此时还需要解析 RowKey，对照缓存的 MetaCache，查看具体写入的位置有哪个 RegionServer ④ 将数据顺序写入（追加）到 WAL，此处写入是直接落盘的，并设置专门的线程控制 WAL 预写日志的滚动（类似 Flume）； ⑤ 根据写入命令的 RowKey 和 ColumnFamily 查看具体写入到哪个 MemStore，并且在 MemStore 中排序 ⑥ 向客户端发送 ACK ⑦ 等达到 MemStore 的刷写时机后，将数据刷写到对应的 Store 中。 读流程 读流程创建连接的方式同写流程。创建完连接后： ① 创建 Table 对象发送 get 请求。 ② 优先访问 Block Cache（读缓存），查找是否之前读取过，并且可以读取 HFile 的索引信息和布隆过滤器。 ③ 不管读缓存中是否已经有数据了（可能已经过期了），都需要再次读取写缓存和 store 中的文件。 ④ 最终将所有读取到的数据合并版本，按照 get 的要求返回即可。 HBase 的特点有哪些？优点或缺点？ HBase的特点：(强 / 自动 / 高 / 海集 / 并 / 列 / 多 / 块 / 运行 / 稀) 强一致性的读/写：HBase不是”最终一致性”数据库，它非常适合于诸如高速计数器聚合等任务。 自动分片：HBase 中的表通过 Region 分布在集群上，而且 Region 会随着数据的增长自动拆分和重新分布。 高可靠性：自动 RegionServer 故障转移，WAL 机制保证了数据写入时不会因集群异常而导致写入数据丢失，Replication 机制保证了在集群出现严重的问题时，数据不会发生丢失或损坏。而且 HBase 底层使用 HDFS，HDFS 本身也有备份。 Hadoop/HDFS 集成&amp;海量存储：，HBase支持 HDFS开箱即用作为其分布式文件系统。HBase 作为一个开源的分布式 Key-Value 数据库，其主要作用是面向 PB 级别数据的实时入库和快速随机访问。这主要源于上述易扩展的特点，使得 HBase 通过扩展来存储海量的数据。 并行处理：HBase 通过 MapReduce 支持大规模并行处理，将 HBase 用作源和接收器。 列式存储：HBase 是根据列族来存储数据的。列族下面可以有非常多的列。列式存储的最大好处就是，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段时，能大大减少读取的数据量。（面向列） 多种语言的 API：HBase 支持使用 Java 的 API 来编程进行数据的存取，还支持使用 Thrift 语言和 REST 语言的 API 来编程进行数据的存取。 块缓存和布隆过滤器：HBase支持 Block Cache 和 Bloom过滤器进行大容量查询优化。 运行管理：HBase 为业务洞察和 JMX 度量提供内置网页。 稀疏性：为空的列可以不占存储空间，表可以设计的非常稀疏。 HBase 的优点和缺点 HBase的优点：(动节/海存/负载/并拓) 动态增加&amp;节省空间：在传统的关系数据库中，如果数据结构发生了变化，就需要停机维护，而且需要修改表结构，而在 HBase 中数据表内的列可以做到动态增加，并且列为空的时候不存储数据，从而节省存储空间。 海量数据存储：HBase 适合存储 PB 数量级的海量数据，PB 级的数据在只采用廉价 PC 来存储的情况下，也可以在几十到一百毫秒内返回数据。这与 HBase 的极易扩展息息相关，正因如此，HBase 为海量数据的存储提供了便利。 负载均衡：传统的通用关系数据库无法应对在数据规模剧增时导致的系统扩展性问题和性能问题。HBase 可以做到自动切分数据，并且会随着数据的增长自动地拆分和重新分布。 高并发：HBase 可以提供高并发的读写操作，而且可以利用廉价的计算机来处理超过 10 亿行的表数据 高拓展性：HBase 具有可伸缩性，如果当前集群的处理能力明显下降，可以增加集群的服务器数量来维持甚至提高处理能力。 HBase 的缺点：（条查/复杂/JOIN/ACID/SQL） 不支持条件查询：不能支持条件查询，只支持按照 RowKey（行键）来查询，也就是只能按照主键来查询。这样在设计 RowKey 时，就需要完美的方案以设计出符合业务的查询。 架构设计复杂：架构设计复杂，且使用 HDFS 作为分布式存储，因此只是存储少量数据，它也不会很快。在大数据量时，它慢的不会很明显。 不支持 Join 操作：HBase 不支持表的关联操作，因此数据分析是 HBase 的弱项。常见的 group by 或 order by 只能通过编写 MapReduce 来实现。 不支持ACID：HBase部分支持了 ACID 不支持SQL语句查询：查询 HBase 时不支持通过 SQL 语句进行查询。 Master 和 RegionServer 的作用是什么？ HBase 包含一个 Master 和许多个 RegionServer 。 Master：实现类为 HMaster，负责监控集群中所有的 RegionServer 实例。主要作用如下： 管理元数据表格 hbase:meta，接收用户对表格创建修改删除的命令并执行 监控 Region 是否需要进行负载均衡，故障转移和 Region 的拆分。 RegionServer：实现类为 HRegionServer，主要作用如下: 负责数据 Cell 的处理，例如写入数据 put，查询数据 get 等。 拆分合并 Region 的实际执行者，有 master 监控，有 RegionServer 执行。 编程题Shell 创建命名空间的 Shell 1create_namespace 'namespace_name' 创建表的 Shell 1create 'namespace_name:table_name', {NAME =&gt; 'cf1', VERSION =&gt; 5} 修改表的 Shell 1alter 'namespace_name:table_name', NAME =&gt; 'cf1', METHOD =&gt; 'delete' 插入数据的 Shell 1put 'ns:tb', 'rk', 'cf:col', 'value' 查询数据的 Shell get 最大范围是一行数据，也可以进行列的过滤，读取的结果为多行 Cell。 Cell 的格式如下：{rowkey, column Familycolumn Qualifier, time Stamp}。 1get 'ns:tb', 'rk' , {COLUMN =&gt; 'cf:col'} scan 用于扫描数据，能够读取多行数据，不建议扫描过多的数据，推荐使用 startRow 和 stopRow 来控制读取的数据，默认范围左闭右开。 1scan 'namespace_name:table_name', {STARTROW =&gt; '1001',STOPROW =&gt; '1002'} API读写操作时 加载配置信息 获取 HBase 的链接对象 获取 Admin 对象 构造 TableName 对象 判断表是否存在 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131public class Test { public static Connection connection = null; static { Configuration conf = new Configuration(); // 加载配置信息 conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;bigdata&quot;); try { connection = ConnectionFactory.createConnection(conf); } catch (IOException e) { e.printStackTrace(); } } public static void closeConntction() throw IOException { if (connection != null) { connection.close(); } } /** * @brief 判断表格是否存在 * @param namespace 命名空间名 * @param tableName 表名 * @return true 存在; false 不存在 */ public static boolean isTableExists(String namespace, String tableName) throws IOException { // 1. 获得 admin Admin admin = HBaseConnection.connection.getAdmin(); // 2. 使用对应的方法, 判断表格是否存在 boolean result = false; try { result = admin.tableExists(TableName.valueOf(namespace, tableName)); } catch (IOException e) { e.printStackTrace(); } // 3. 关闭 admin admin.close(); // 返回结果 return result; } /** * @brief 向表格中插入数据 * @param namespace 命名空间名 * @param tableName 表名 * @param rowKey 行键 * @param columnFamily 列族名 * @param columnName 列名 * @param value 插入值 */ public static void putCell(String namespace, String tableName, String rowKey, String columnFamily, String columnName, String value) throws IOException { // 1. 获取 Table Table table = connection.getTable(TableName.valueOf(namespace, tableName)); // 2. 调用相关方法插入数据 // 2.1 创建 Put 对象 Put put = new Put(Bytes.toBytes(rowKey)); // 2.2. 给 Put 对象添加数据 put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName), Bytes.toBytes(value)); // 2.3 将对象写入对应的方法 try { table.put(put); } catch (IOException e) { e.printStackTrace(); } // 3. 关闭 Table table.close(); } /** * @brief 读取数据 读取对应一行的某一列 * @param namespace 命名空间名 * @param tableName 表名 * @param rowKey 行键 * @param columnFamily 列族名 * @param columnName 列名 */ public static void getCells(String namespace, String tableName, String rowKey, String columnFamily, String columnName) throws IOException { // 1. 获取 Table Table table = connection.getTable(TableName.valueOf(namespace, tableName)); // 2. 调用相关方法插入数据 // 2.1 创建 Get 对象 Get get = new Get(Bytes.toBytes(rowKey)); // 如果此时不使用 addColumn 进行参数的添加, 此时则读取一整行的数据 // 2.2 如果想读取某一列的数据, 需要添加对应的参数 get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName)); // 2.3 也可以设置读取数据的版本 get.readAllVersions(); try { // 3. 读取数据, 写入 Result 对象 Result result = table.get(get); // 4. 处理数据 Cell[] cells = result.rawCells(); // 5. 测试方法: 直接把读取到的数据打印到控制台 // 如果是实际开发, 需要再额外写方法, 对应处理数据 for (Cell cell : cells) { // Cell 存储数据比较底层, 需要进一步处理 String value = new String(CellUtil.cloneValue(cell)); System.out.println(value); } } catch (IOException e) { e.printStackTrace(); } // 6. 关闭 Table table.close(); } private static main(String[] args) throw IOException { putCell(&quot;sdutcm&quot;, &quot;bigdata&quot;, &quot;2001&quot;, &quot;info&quot;, &quot;name&quot;, &quot;zhangsan&quot;); connection.close(); }} 参考文献Hadoop 综合揭秘——HBase的原理与应用 - 风尘浪子 - 博客园 (cnblogs.com) HBase的组件","link":"/2024/06/22/HBaseExamReview/"},{"title":"DockerQuickIN","text":"Docker 快速入门[TOC] 宝藏视频：https://www.bilibili.com/video/BV14s4y1i7Vf 如果你遇到过： 应用程序的部署和环境配置过于复杂 在开发环境好用，但是到了测试和生产环境都不好用的情况 作为新人加入项目组，需要花费大量的时间来配置开发环境 花费一整天甚至更长时间，一步一步按照配置部署文档来配置环境，但是最终却卡在中间某个步骤上，再也无法前进 那么，Docker 可以帮我们完美解决上面这些问题。 🐳 1. Docker 的基本概念1.1 Docker 的简介Docker 是一个用于构建（Build）、运行（Run）、传送（Share）应用程序的平台。 有了 Docker，我们就可以将应用程序和运行他所需要的各种依赖包、第三方软件库、配置文件等打包在一起。以便在任何环境中都能正确的运行。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口，更重要的是容器性能开销极低。 Docker 从 17.03 版本之后分为 社区版 CE(Community Edition) 和 企业版 EE(Enterprise Edition)。 1.2 为什么 要使用 Docker为什么要使用 Docker 呢？ 举个简单的例子：我们写了一个网站，用到了现在不叫流行的前后端分离架构，前端使用 Vue 框架来构建网站的界面，后端使用 Java 的 SpringBoot 框架来提供各种服务和接口，并且使用 MySQL 数据库来存储数据。 如果没有 Docker，我们的步骤大概是这样的： Step-01：安装 NodeJS 环境。 Step-02：安装各种 npm 依赖。 Step-03：安装 Java 运行时环境。 Step-04：安装各种第三方依赖包。 Step-05：安装 MySQL 数据库。 Step-06：配置各种环境变量、启动脚本。 Step-07：配置 Redis。 Step-08：配置 Nginx 负载均衡。 Step-09：其他配置项… 这仅仅是开发环境所需要进行的操作，当我们需要将我们的网站部署到测试环境或生产环境上，那么这些所有的步骤都需要重新再来一遍。 但是如果我们有了 Docker，我们就可以将他们打包成一个个集装箱，只要我们在开发环境中运行成功了，那么在测试环境中一定也是可以运行成功的。 1.3 Docker 和 虚拟机的区别我们可能了解或使用过一些虚拟机软件，如：VMware、Virtalbox、Parallels Desktop，或者 Window 的 WSL、Microsoft Hyper-V 等功能。 他们都是完整的操作系统，和实际上使用操作系统一样，可以在实际的操作系统中运行应用程序，这是通过一种叫做虚拟化（Hypervisor）的技术实现的。虚拟化技术是一种将物理资源虚拟为多个逻辑资源的技术，他可以将一台物理服务器虚拟成多个逻辑服务器，每个逻辑服务器都有自己的操作系统、CPU、内存、硬盘和网络接口等。不同的逻辑服务器之间是完全隔离的，可以独立运行。 虚拟机在一定程度上实现了资源的整合，可以将要一台服务器的计算能力、存储能力、网络资源分配给多个逻辑服务器。实现多台服务器的功能。但是其缺点也同样明显，每台虚拟机需要占用大量的资源，比如CPU、内存、硬盘、网络等，而且启动速度非常慢。但是其实我们的每台服务器上只需要运行一个主要对外提供服务的应用程序就可以了。并不需要一个完整的操作系统所提供的所有功能。 也许我们需要的只是一个Web服务器，但是使用虚拟机却需要启动一个完整的操作系统，包括操作系统的内核、各种系统服务、各种工具甚至图形界面等，这些我们并不需要的服务占用了大量的资源，导致了资源的浪费和启动速度慢的问题。 了解了虚拟机后，我们再来看一下 Docker 是如何解决的。需要注意，Docker 中***容器(Container)的概念。需要注意的是，Docker 和 容器是两个不同的概念，但是因为 Docker 的流行性，以至于很多人将 Docker 和 容器 混为一谈，实际上 Docker只是容器的一种实现，是一个容器化的解决方案和平台。而容器是一种虚拟化技术，和虚拟机类似*，也是一个独立的环境，可以在这个环境中运行应用程序。和虚拟机不同的是，他并不需要在容器中运行一个完整的操作系统，而是使用宿主机的操作系统，所以启动速度非常快，通常只需要几秒钟。同时，因为需要的资源更少，所以可以在一台物理服务器上运行更多的容器，以便于更加充分的利用服务器的资源，减少资源的闲置和浪费。 正因如此，一台物理服务器上也许可以运行几个虚拟机，但是却可以运行上百个容器。 1.4 Docker 的基本原理和主要概念 镜像 Image：镜像是一个只读的模板，它可以用来创建容器。 容器 Container：容器是 Docker 的运行实例，他提供了一个独立的、可移植的环境，可以在这个环境中运行应用程序。 仓库 Registry：Docker 的仓库是一个用于存储 Docker镜像的地方。最流行、最常用的仓库就是 Docker Hub。 类似于 Java 和 C++，镜像类似于类，而容器则就是类的实例，可以有一到多个容器。 Docker 使用的是 Client-Server 架构模式，Docker Client 和 Docker Daemon 之间通过 socket 或 RESTful API 进行通信。Docker Daemon 就是 Docker 服务端的守护进程，他负责管理 Docker 的各种资源。Docker Client 负责向 Docker Daemon 发送请求，Docker Daemon 接收到请求后进行处理，然后将要结果返回给 Docker Client。Docker Daemon 是一个后台进程，用于接收并处理来自 Docker 客户端的请求，然后将结果返回给客户端。因此我们在终端中输入的各种 Docker 命令，其实都是通过 Docker Client 发送给 Docker Daemon 的，Docker Daemon 进行处理，最后将处理结果返回给 Docker Client，然后就可以再终端中看到执行结果。 🐳 2. Docker 的安装配置Docker 官网（目前国内无法正常访问）：https://www.docker.com/ 可以找一些镜像网站来进行下载安装。 下载完成后，需要运行 Docker，才可以使用 Docker 的一系列命令。 Windows 系统下需要开启 Hyper-V 功能。开启方式如下： 在设置中搜索 启用或关闭 Windows 功能。 勾选 Hyper-V 选项，按照提示重启系统即可。 启动之后，即可在终端使用 Docker 的各种命令了。我们可以使用一条简单的命令来进行测试： 123456789101112131415161718192021222324252627282930C:\\Users\\NilEra&gt;docker versionClient: Cloud integration: v1.0.28 Version: 20.10.17 API version: 1.41 Go version: go1.17.11 Git commit: 100c701 Built: Mon Jun 6 23:09:02 2022 OS/Arch: windows/amd64 Context: default Experimental: trueServer: Docker Desktop 4.11.1 (84025) Engine: Version: 20.10.17 API version: 1.41 (minimum version 1.12) Go version: go1.17.11 Git commit: a89b842 Built: Mon Jun 6 23:01:23 2022 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.6.6 GitCommit: 10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 runc: Version: 1.1.2 GitCommit: v1.1.2-0-ga916309 docker-init: Version: 0.19.0 GitCommit: de40ad0 我们可以看到 Client 和 Server 的信息，如果只能看到 Client，则表示 Docker 没有启动，需要启动 Docker 之后才可以看到 Server。 🐳 3. Docker 的常用命令3.1 Docker Desktop 的使用3.1.1 Docker Containers &amp; Images 容器和镜像Docker Desktop 是 Docker 的图形化界面，Docker Desktop 封装了容器日常使用和管理的各种常用功能，打开控制台面板后，可以看到 Containers 容器和**Images镜像**，这两个菜单项分别用于查看本机的容器列表和镜像列表。 对于菜单项中的内容，我们可以进行一系列操作。 比如，我们可以点击 RUN 按钮运行一个镜像，等同于执行了 docker run 命令。点击三个小圆点，有几个选项 Inspect、Pull、Push to Hub、Remove，分别用于查看镜像的详细信息、拉取镜像、推送镜像、移除镜像。 3.1.2 Docker Volumes 逻辑卷此外，还有 Volumes 逻辑卷 菜单项，在这个菜单项列表中可以看到容器的逻辑卷，逻辑卷是 Docker 中用来存储的。 Docker 容器有一个特点，就是容器中的数据是不会持久化的，当我们创建一个容器的时候，它通常以一个干净的文件系统开始，容器启动之后，我们可以在文件中创建文件、修改文件，但是当容器停止之后，容器中的所有数据都会丢失掉。 如果我们想要对于 Docker 中的数据进行容器的持久化应该怎么做？ 逻辑卷就是用来解决这个问题的，它可以将容器中的目录或者指定路径映射到宿主机的某一目录或者位置上，这样就可以将数据保存到宿主机的磁盘上，实现了数据的持久化。 3.1.3 Docker Dev Environments 开发环境用于管理开发环境，简单来说就是可以创建一个开发环境，然后通过一些代码配置这个开发环境，这样就可以将这个开发环境共享给项目中的其他开发人员，让每个人都在一个相同的环境下进行开发，避免因为环境的不一致导致的各种问题。 🐳 4. 如何从零开始构建一个 Docker 镜像4.1 容器化和 Dockerfile**容器化(Containerization)**：就是将应用程序打包成容器，然后在容器中运行应用程序的过程。 容器化的过程可以分为以下几个步骤： 创建一个 Dockerfile：来告诉 Docker 构建应用程序镜像所需要的步骤和配置。 使用 Dockerfile 构建镜像。 使用镜像创建和运行容器。 Dockerfile 是一个文本文件，里面包含了一条条指令，用于告诉 Docker 如何构建镜像，这个镜像中包括了我们应用程序执行的所有命令，如各种依赖、配置环境和运行应用程序所需要的所有内容，一般来说包括：精简版的操作系统（如Alpine）、应用程序的运行时环境（如NodeJS、Python、Java）、应用程序（如打包好的 jar 包）、应用程序的第三方依赖库或包、应用程序的配置文件、环境变量等。 一般来说，我们会在项目的根目录下创建一个叫 Dockerfile 的文件，在这个文件中写入构建镜像所需要的各种指令之后，Docker 就会根据这个 Dockerfile 文件来构建一个镜像，有了这个镜像后，我们就可以根据这个镜像来创建容器，然后在容器中运行应用程序。 4.2 编写 Dockerfile 新建一个名为 HelloDocker 的文件夹，使用 VSCode 打开该文件夹 在文件夹中创建一个 index.js 文件，简单的输入一些代码内容，比如输出一段内容到控制台 1console.log(&quot;Hello Docker!&quot;); 我们可以使用 NodeJS 来测试这个 JavaScript 文件，NodeJS 是一个运行时环境，它可以让我们在浏览器之外的环境运行 JavaScript 代码。JavaScript 和 NodeJS 的关系就像 Java 和 JRE 的关系一样，如果想在浏览器之外的环境中运行 JavaScript 代码就需要 NodeJS。可以使用下面的命令运行 index.js。 1PS D:\\HelloDocker&gt; node index.js 所以，当我们想要在另一个环境中运行这个应用程序时，都需要执行那些步骤呢？ 安装操作系统 安装 JavaScript 的运行环境 复制应用程序、依赖包、配置文件 执行启动命令运行程序 而有了 Docker 后，我们可以将上述的步骤写入到 Dockerfile 中，剩下的工作交给 Docker 自动完成。 创建 Dockerfile 文件 在 Dockerfile 中，我们需要先指定一个基础镜像，镜像是按层次结构来构建的，每一层都是基于上一层的，所以我们需要先指定一个基础镜像，然后在此基础上添加我们的应用程序。 注意 Dockerfile 中不需要注释，这里只是为了便于理解所以添加了注释 123456789# 指定基础镜像FROM node:14-alpine# 复制 本地路径的 JS 文件到镜像的根目录下COPY index.js /# 使用 CMD 命令运行该程序 CMD [&quot;arg1&quot;, &quot;arg2&quot;, &quot;arg...&quot;], 其中 arg1 表示可执行程序的名字, arg2 及以后的参数表示可执行程序的参数# 可以写成这种形式: CMD node /index.jsCMD [&quot;node&quot;, &quot;/index.js&quot;] 4.3 创建镜像 构建 Docker 镜像 执行 docker build 命令，hello-docker 是镜像名称；. 表示当前目录，即 Dockerfile 所在目录。 1PS D:\\HelloDocker&gt; docker build -t hello-docker . 执行完成后，我们可以使用命令docker images 或 docker image ls 查看所有镜像。 123PS D:\\HelloDocker&gt; docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-docker latest 31429fa0e1e7 22 seconds ago 119MB 4.4 启动容器 使用 docker run 命令运行容器。 12PS D:\\HelloDocker&gt; docker run hello-dockerHello Docker! 4.5 在其他设备上测试容器 为了测试我们的容器，我们可以将我们的镜像上传到 Docker Hub 或者 Harbor 镜像仓库中，上传完成之后，可以使用 Play with Docker 进行测试。 上传镜像可以使用 docker push 命令。需要注意的是，推送镜像可能会出现问题：docker：denied: requested access to the resource is denied，遇到这个问题时，可以参考解决docker：denied: requested access to the resource is denied_docker login - CSDN。 ① 确保自己登录了 Docker ② 将镜像打 TAG，使用 docker tag hello-docker:latest nilerak/hello-docker:1.0.0 ③ 推送镜像，使用 docker push nilerak/hello-docker:1.0.0 使用 docker pull 命令下载镜像文件。 1docker pull nilerak/hello-docker:1.0.0 🐳 5. 如何运行一个 Docker 容器🐳 6. Docker Compose 和 Kubernetes6.1 Docker ComposeDocker Compose 是 Docker 官方的开源项目，是一个用于定义和运行多容器 Docker 应用程序的工具，使用 Yaml 文件配置应用程序的服务，只需运行一条命令即可创建并启动所有的服务。 例如我们的项目可能需要前端（Vue）、后端（SpringBoot）、数据库（MySQL）、缓存（Redis）、负载均衡（Nginx）等多个服务器，这些服务相互独立，但是又存在一定的关联，需要相互配合来工作，前端需要连接后端、后端需要连接数据库。这些服务之间的关联关系，就是 Docker Compose 要解决的问题。它通过一个单独的 docker-compose.yaml 文件来将这一组互相关联的容器组合在一起，形成一个项目，然后使用一条命令 docker compose up 即可启动、停止、重建这些服务，这样就可以非常方便的管理这些服务了。 参考文献Docker超详细基础教程 - CSDN博客","link":"/2024/07/22/DockerQuickIN/"},{"title":"NginxQuickIN","text":"ENGINE X - Nginx 快速上手指南[TOC] 🌐 1. Nginx 基本概念Nginx 是目前最流行的 Web 服务器软件，也是互联网公司和网站的首选。一开始是由一个名为 Igor Sysoev 的俄罗斯程序员开发的，目的是用于解决 C10K 问题，C10K 问题是指服务器同时支持成千上万个并发客户端的问题。这个问题在当时是非常严重的，因为当时的服务器软件都是单线程的，所以在高并发的情况下，服务器的性能会变得非常的差。 Nginx 的出现很好的解决了这个问题，在 Nginx 官方的测试结果中，Nginx 可以支持 50000 个并发连接，在后来的发展中，Nginx 也逐渐成为了最流行的 Web 服务器软件。 Nginx 具有 epoll I/O多路复用、高并发、高性能、低内存消耗、热部署等优势。 🌐 2. Nginx 安装配置2.1 通过包管理工具安装 Nginx① 在 Linux 环境下： 执行 sudo apt update 更新 apt 包，并执行 sudo apt install nginx 安装 Nginx。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889nilera@nilera-virtual-machine:~$ sudo apt update[sudo] password for nilera: Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB] Hit:3 https://packages.microsoft.com/repos/edge stable InRelease Hit:1 https://packages.microsoft.com/repos/code stable InRelease Hit:4 https://ppa.launchpadcontent.net/christian-boxdoerfer/fsearch-stable/ubuntu jammy InReleaseHit:5 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy InRelease Hit:6 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy InRelease Hit:7 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates InReleaseHit:8 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-backports InReleaseFetched 129 kB in 4s (29.4 kB/s)Reading package lists... DoneBuilding dependency tree... DoneReading state information... Done271 packages can be upgraded. Run 'apt list --upgradable' to see them.nilera@nilera-virtual-machine:~$ sudo apt install nginx Reading package lists... DoneBuilding dependency tree... DoneReading state information... DoneThe following additional packages will be installed: libnginx-mod-http-geoip2 libnginx-mod-http-image-filter libnginx-mod-http-xslt-filter libnginx-mod-mail libnginx-mod-stream libnginx-mod-stream-geoip2 nginx-common nginx-coreSuggested packages: fcgiwrap nginx-docThe following NEW packages will be installed: libnginx-mod-http-geoip2 libnginx-mod-http-image-filter libnginx-mod-http-xslt-filter libnginx-mod-mail libnginx-mod-stream libnginx-mod-stream-geoip2 nginx nginx-common nginx-core0 upgraded, 9 newly installed, 0 to remove and 271 not upgraded.Need to get 697 kB of archives.After this operation, 2,395 kB of additional disk space will be used.Do you want to continue? [Y/n] YGet:1 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 nginx-common all 1.18.0-6ubuntu14.4 [40.0 kB]Get:2 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 libnginx-mod-http-geoip2 amd64 1.18.0-6ubuntu14.4 [11.9 kB]Get:3 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 libnginx-mod-http-image-filter amd64 1.18.0-6ubuntu14.4 [15.4 kB]Get:4 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 libnginx-mod-http-xslt-filter amd64 1.18.0-6ubuntu14.4 [13.7 kB]Get:5 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 libnginx-mod-mail amd64 1.18.0-6ubuntu14.4 [45.7 kB]Get:6 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 libnginx-mod-stream amd64 1.18.0-6ubuntu14.4 [72.9 kB]Get:7 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 libnginx-mod-stream-geoip2 amd64 1.18.0-6ubuntu14.4 [10.1 kB]Get:8 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 nginx-core amd64 1.18.0-6ubuntu14.4 [484 kB]Get:9 http://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 nginx amd64 1.18.0-6ubuntu14.4 [3,872 B]Fetched 697 kB in 5s (128 kB/s) debconf: unable to initialize frontend: Dialogdebconf: (Dialog frontend requires a screen at least 13 lines tall and 31 columns wide.)debconf: falling back to frontend: ReadlinePreconfiguring packages ...Selecting previously unselected package nginx-common.(Reading database ... 229379 files and directories currently installed.)Preparing to unpack .../0-nginx-common_1.18.0-6ubuntu14.4_all.deb ...Unpacking nginx-common (1.18.0-6ubuntu14.4) ...Selecting previously unselected package libnginx-mod-http-geoip2.Preparing to unpack .../1-libnginx-mod-http-geoip2_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking libnginx-mod-http-geoip2 (1.18.0-6ubuntu14.4) ...Selecting previously unselected package libnginx-mod-http-image-filter.Preparing to unpack .../2-libnginx-mod-http-image-filter_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking libnginx-mod-http-image-filter (1.18.0-6ubuntu14.4) ...Selecting previously unselected package libnginx-mod-http-xslt-filter.Preparing to unpack .../3-libnginx-mod-http-xslt-filter_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking libnginx-mod-http-xslt-filter (1.18.0-6ubuntu14.4) ...Selecting previously unselected package libnginx-mod-mail.Preparing to unpack .../4-libnginx-mod-mail_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking libnginx-mod-mail (1.18.0-6ubuntu14.4) ...Selecting previously unselected package libnginx-mod-stream.Preparing to unpack .../5-libnginx-mod-stream_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking libnginx-mod-stream (1.18.0-6ubuntu14.4) ...Selecting previously unselected package libnginx-mod-stream-geoip2.Preparing to unpack .../6-libnginx-mod-stream-geoip2_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking libnginx-mod-stream-geoip2 (1.18.0-6ubuntu14.4) ...Selecting previously unselected package nginx-core.Preparing to unpack .../7-nginx-core_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking nginx-core (1.18.0-6ubuntu14.4) ...Selecting previously unselected package nginx.Preparing to unpack .../8-nginx_1.18.0-6ubuntu14.4_amd64.deb ...Unpacking nginx (1.18.0-6ubuntu14.4) ...Setting up nginx-common (1.18.0-6ubuntu14.4) ...debconf: unable to initialize frontend: Dialogdebconf: (Dialog frontend requires a screen at least 13 lines tall and 31 columns wide.)debconf: falling back to frontend: ReadlineCreated symlink /etc/systemd/system/multi-user.target.wants/nginx.service → /lib/systemd/system/nginx.service.Setting up libnginx-mod-http-xslt-filter (1.18.0-6ubuntu14.4) ...Setting up libnginx-mod-http-geoip2 (1.18.0-6ubuntu14.4) ...Setting up libnginx-mod-mail (1.18.0-6ubuntu14.4) ...Setting up libnginx-mod-http-image-filter (1.18.0-6ubuntu14.4) ...Setting up libnginx-mod-stream (1.18.0-6ubuntu14.4) ...Setting up libnginx-mod-stream-geoip2 (1.18.0-6ubuntu14.4) ...Setting up nginx-core (1.18.0-6ubuntu14.4) ... * Upgrading binary nginx [ OK ] Setting up nginx (1.18.0-6ubuntu14.4) ...Processing triggers for man-db (2.10.2-1) ...Processing triggers for ufw (0.36.1-4ubuntu0.1) ... ② 在 Mac OS 环境下： 1brew install nginx ③ 在 Windows 环境下： 1scoop install nginx 或者使用： 1choco install nginx 2.2 通过源码编译的方式安装 NginxNginx 是使用 C 语言开发的，因此可以像其他 C 语言的项目一样，下载 Nginx 的源码到自己的服务器上，然后执行预编译、编译和安装的过程。 ① 预编译 1234567./configure--sbin-path=/usr/local/nginx/nginx \\--conf-path=/usr/local/nginx/nginx.conf \\--pid-path=/usr/local/nginx/nginx/pid \\--with-http_ssl_module \\--with-pcre=../pcre2-10.39 \\--with-zlib=../zlib-1.2.11 ② 编译 1make ③ 安装 1make install 2.3 使用 Docker 来安装 Nginx1docker pull nginx 🌐 3. Nginx 常用命令3.1 服务的启动和停止3.1.1 服务的启动安装完成后，可以直接在命令行输入 nginx 启动服务，若是没有出现信息则说明启动成功；若是启动失败，则会输出启动失败的原因，此时需要根据错误原因进行排查。 1nginx 可能会遇到的问题： 问题描述 问题原因 解决方案 运行 nginx 命令出现错误：nginx: [alert] could not open error log file: open() &quot;/var/log/nginx/error.log&quot; failed (13: Permission denied) 权限问题导致无法正常启动 Nginx。 使用 sudo nginx 命令提高用户执行的权限。 运行 sudo nginx 命令出现错误：nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Unknown error) ... nginx: [emerg] still could not bind() 大概率是 80 端口已经被绑定了，或者是之前已经启动了 Nginx 导致端口冲突。可以查看 80 端口的占用情况进行进一步操作。 使用 kill 命令杀死占用端口的程序，然后重新运行 sudo nginx 命令。 访问 &lt;your ipaddr&gt;:8080 出现以下欢迎界面则表示 Nginx 启动成功。 3.1.2 查看 Nginx 的进程使用 ps -ef|grep nginx 可以查看 Nginx 进程。这个其实并不是 Nginx 的命令，而是 Linux 自带的命令。 1234567nilera@nilera-virtual-machine:~$ ps -ef|grep nginxroot 14461 1 0 10:43 ? 00:00:00 nginx: master process nginxwww-data 14462 14461 0 10:43 ? 00:00:00 nginx: worker processwww-data 14463 14461 0 10:43 ? 00:00:00 nginx: worker processwww-data 14464 14461 0 10:43 ? 00:00:00 nginx: worker processwww-data 14465 14461 0 10:43 ? 00:00:00 nginx: worker processnilera 14542 11143 0 10:43 pts/0 00:00:00 grep --color=auto nginx 这里可以看到 Nginx 的进程包括 Master 进程 和 Worker 进程。 Master 进程就是Nginx 的主进程，他主要负责读取和验证配置文件，并管理 Worker进程，将工作分配给 Worker进程。 Worker 进程就是Nginx 的工作进程，其主要负责实际的请求，完成具体的工作。 Master进程只有一个，而**Worker进程**可以有很多个，Worker 进程的数量可以通过配置文件来调整。 或者我们可以使用 lsof -i:80 来查看 80 端口的占用情况，需要注意：lsof 命令并不适用于所有的 Linux 发行版，有些发行版可能需要安装该命令。 12345678910111213nilera@nilera-virtual-machine:~$ sudo lsof -i:80[sudo] password for nilera: COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnginx 14461 root 6u IPv4 141914 0t0 TCP *:http (LISTEN)nginx 14461 root 7u IPv6 141915 0t0 TCP *:http (LISTEN)nginx 14462 www-data 6u IPv4 141914 0t0 TCP *:http (LISTEN)nginx 14462 www-data 7u IPv6 141915 0t0 TCP *:http (LISTEN)nginx 14463 www-data 6u IPv4 141914 0t0 TCP *:http (LISTEN)nginx 14463 www-data 7u IPv6 141915 0t0 TCP *:http (LISTEN)nginx 14464 www-data 6u IPv4 141914 0t0 TCP *:http (LISTEN)nginx 14464 www-data 7u IPv6 141915 0t0 TCP *:http (LISTEN)nginx 14465 www-data 6u IPv4 141914 0t0 TCP *:http (LISTEN)nginx 14465 www-data 7u IPv6 141915 0t0 TCP *:http (LISTEN) 3.1.3 服务的停止我们可以使用 nginx -s &lt;signal&gt; 来控制 Nginx 的停止或重启。&lt;signal&gt; 可以是 quit优雅退出、**stop立即停止、reload重载配置文件、reopen重新打开日志文件**中的一个。 123nilera@nilera-virtual-machine:~$ sudo nginx -s quitnilera@nilera-virtual-machine:~$ ps -ef|grep nginxnilera 15527 11143 0 11:16 pts/0 00:00:00 grep --color=auto nginx 3.2 查看 Nginx 的版本、路径等信息12345nilera@nilera-virtual-machine:~$ nginx -Vnginx version: nginx/1.18.0 (Ubuntu)built with OpenSSL 3.0.2 15 Mar 2022TLS SNI support enabledconfigure arguments: --with-cc-opt='-g -O2 -ffile-prefix-map=/build/nginx-zctdR4/nginx-1.18.0=. -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-compat --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --add-dynamic-module=/build/nginx-zctdR4/nginx-1.18.0/debian/modules/http-geoip2 --with-http_addition_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_sub_module 其中 --conf-path 包含了 Nginx 的配置文件的位置。在这里我们可以看到路径为：--conf-path=/etc/nginx/nginx.conf。 或者可以使用 nginx -t 来快速定位到 配置文件的位置。nginx -t 同时也可以用于检测配置文件是否存在问题，如果配置文件出现错误，执行 nginx -t 会提示错误位置和错误信息。 123nilera@nilera-virtual-machine:~$ sudo nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful 在Ubuntu 22.04 中，安装的 Nginx 版本为 nginx/1.18.0(Ubuntu)。在该版本中的配置文件和一些最新的版本有一些区别，体现在如下几个方面： 主配置文件：/etc/nginx/nginx.conf 站点配置文件目录：/etc/nginx/sites-available 和 /etc/nginx/sites-enabled 在该版本通常将站点配置文件放在 sites-available 目录中，并通过创建符号链接到 sites-enabled 目录来启用它们。例如，可以通过以下命令创建一个简单的站点配置文件： 1sudo nano /etc/nginx/sites-available/my_site 在 nginx/1.25.1 版本中，站点的配置文件和主配置文件是在写在一起的。 3.3 Nginx 的配置文件Nginx 的配置文件可以帮助我们进行 Nginx 的一些设置，如 worker 进程数等。 我们通过以下步骤修改 Nginx 的配置文件。 ① 编辑 Nginx 配置文件 1vim /etc/nginx/nginx.conf ② 修改相应的配置项 Nginx 的配置项包括全局块、events 块、http 块。 全局块 主要是一些全局配置，比如 Worker 进程数、指定运行服务的用户等。 events 块 主要是服务器和客户端之间网络连接的一些配置等，比如指定每个 Worker 进程可以同时接收多少个网络连接、网络I/O模型等。 http 块 http 块是 Nginx 修改最频繁的部分，如虚拟主机、反向代理、负载均衡等，都是在 http 块中进行配置的。 server 块 http 块下具有多个 server 块，也叫虚拟主机。 我们可以在文件下面 include servers/*，这样就可以将 servers 目录下所有的配置文件都包含进来，这样就可以将每个虚拟主机都放在一个单独的文件中。从而让主配置文件看起来更加的清晰。 在 1.18.0(Ubuntu) 版本中，可以看到主配置文件中包含 include /etc/nginx/sites-enabled/*;，而在 /etc/nginx/site-enabled 中，可以看到 default 软连接到 /etc/nginx/sites-available/default 中。 include 命令还可以包含其他的配置文件，如 mime.types。 下面是 Ubuntu 版本的配置文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283user www-data;worker_processes auto; -- 这里可以用于修改 Work 进程的进程数pid /run/nginx.pid;include /etc/nginx/modules-enabled/*.conf;events { worker_connections 768; # multi_accept on;}http { ## # Basic Settings ## sendfile on; tcp_nopush on; types_hash_max_size 2048; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types; default_type application/octet-stream; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## # Gzip Settings ## gzip on; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*;}#mail {# # See sample authentication script at:# # http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript## # auth_http localhost/auth.php;# # pop3_capabilities &quot;TOP&quot; &quot;USER&quot;;# # imap_capabilities &quot;IMAP4rev1&quot; &quot;UIDPLUS&quot;;## server {# listen localhost:110;# protocol pop3;# proxy on;# }## server {# listen localhost:143;# protocol imap;# proxy on;# }#} ③ 使用 nginx -t 命令检查配置文件是否存在问题。 ④ 使用 nginx -s reload 修改配置文件后需要重新加载配置文件。 🌐 4. Nginx 常用操作4.1 静态站点部署我们可以使用 Hexo 来生成一个简单的静态博客网站，并将其部署到 Nginx 中。 Hexo 是一个静态博客站点生成工具，是一个基于 Node.js 的博客框架。它可以将 Markdown 格式的文档转化成静态页面，非常适合用于做个人技术博客。 下面先了解以下如何安装和启动 Hexo。 ① 前置要求：安装 Node.js 和 Git 1234nilera@nilera-virtual-machine:~$ sudo apt updatenilera@nilera-virtual-machine:~$ sudo apt install -y nodejs npmnilera@nilera-virtual-machine:~$ sudo apt install git ② 检测 Node.js、npm 和 Git 的版本 12345678nilera@nilera-virtual-machine:~$ nodejs -vv12.22.9nilera@nilera-virtual-machine:~$ npm -v8.5.1nilera@nilera-virtual-machine:~$ git -vgit version 2.43.0 ③ 安装 Hexo 1nilera@nilera-virtual-machine:~$ npm install hexo-cli -g ④ 安装 Hexo 有可能会出现安装速度过慢的情况，这时我们进行 npm 换源操作。 查看源 12nilera@nilera-virtual-machine:~$ npm config get registryhttps://registry.npmjs.org/ 更换淘宝源 这里需要注意，https://registry.npm.taobao.org/ 已经于 2022年05月31日 废弃，现在需要更换为新的淘宝镜像源 https://registry.npmmirror.com/。换源的配置会持久化保存到 ~/.npmrc 文件中，可以通过手动修改该文件来修改配置。 此外我们可以安装 nrm 包来简化这一操作，nrm 其实就是 NPM Registry Manager，是一个用于管理 NPM 源的简单命令行工具。具体操作可以参考文章：NPM 如何换源 ? 12nilera@nilera-virtual-machine:~$ npm config set registry https://registry.npm.taobao.org/nilera@nilera-virtual-machine:~$ npm config set registry https://registry.npmmirror.com/ 恢复官方源 1nilera@nilera-virtual-machine:~$ npm config set registry https://registry.npmjs.org 删除注册表 1nilera@nilera-virtual-machine:~$ npm config delete registry ⑤ 初始化 执行完 hexo init blog-demo 后，会在执行该目录下生成 blog-demo 目录。 1234nilera@nilera-virtual-machine:~$ hexo init blog-demoINFO Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO Install dependenciesINFO Start blogging with Hexo! ⑥ 安装依赖 12nilera@nilera-virtual-machine:~$ cd blog-demonilera@nilera-virtual-machine:~$ npm install ⑦ 使用 hexo g 生成网站静态页面 该命令的主要作用是将 Markdown 格式的文章，转化成静态页面，并放到 public 目录中。第一次执行该命令，会生成 public 目录。 12345678910111213141516nilera@nilera-virtual-machine:~/blog-demo$ hexo gINFO Validating configINFO Start processingINFO Files loaded in 1.18 sINFO Generated: archives/2024/index.htmlINFO Generated: archives/index.htmlINFO Generated: archives/2024/07/index.htmlINFO Generated: index.htmlINFO Generated: fancybox/jquery.fancybox.min.cssINFO Generated: js/script.jsINFO Generated: css/style.cssINFO Generated: css/images/banner.jpgINFO Generated: 2024/07/28/hello-world/index.htmlINFO Generated: fancybox/jquery.fancybox.min.jsINFO Generated: js/jquery-3.6.4.min.jsINFO 11 files generated in 7.46 s ⑦ 使用 hexo s 或 hexo server 本地运行（启动本地服务器） 12345nilera@nilera-virtual-machine:~/blog-demo$ hexo servernilera@nilera-virtual-machine:~/blog-demo$ hexo sINFO Validating configINFO Start processingINFO Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop. ⑧ 实际生产环境中，我们会将页面部署到 Nginx 上，我们只需要将 blog-demo 中的 public 目录复制到 Nginx 用于存放静态页面的目录下即可。 1nilera@nilera-virtual-machine:~/blog-demo$ sudo cp -rf public/* /var/www/html/ ⑨ 访问 Nginx 页面，即可进行博客网站的访问。 4.2 反向代理反向代理是相对于正向代理来说的，简单来说，正向代理就是代理客户端，反向代理就是代理服务端。 例如，当我们想要访问一个国外的网站（如：Google）时，可能由于网络的原因无法访问到相关内容。这个时候我们就可以使用 VPN 这种代理服务器，让代理服务器作为客户端，代理我们去访问这些网站，再将访问的结果返回给我们。这就是一个典型的正向代理的例子，这里的代理服务器代理的是客户端，而且这个过程客户端是知道的，但是对于服务端来说是透明的。 而反向代理是代理服务端的，比如当我们准备访问一个网站（如：Google）的时候，他后面可能有着成千上万台服务器，但是对外暴露的只有一个域名，我们也只能通过这个域名来进行访问，然后我们的请求会被转发到后面的服务器上。这种方式隐藏了真实的服务器IP地址、端口等信息。这就是反向代理的典型例子，这里的代理服务器代理的是服务端，而且这个过程服务端是知道的，但是对于客户端来说是透明的。 4.3 负载均衡4.4 HTTPS 配置HTTPS 协议是 HTTP 协议的安全版本，它通过对传输的数据进行加密来保证数据的安全性。HTTP 协议的默认端口号是 80 ，而 HTTPS 协议的默认端口号是 443。 HTTPS 协议需要使用到 SSL 证书，再主流的云平台上都可以免费申请到 SSL 证书。证书申请完成之后，会得到密钥文件和证书文件。如果没有云平台也没有关系，我们可以通过 OpenSSL 命令来生成一个自签名的 SSL 证书，步骤如下： ① 生成私钥文件（Private Key） 1openssl genrsa -out private.key 2048 ② 根据私钥生成证书签名请求文件（Certificate Signing Request, CSR） 1openssl req -new -key private.key -out cert.csr ③ 使用私钥对证书申请进行签名从而生成证书文件（pem） 1openssl x509 -req -in cert.csr -out cacert.pem -signkey private.key 4.5 虚拟主机","link":"/2024/07/24/NginxQuickIN/"},{"title":"Cpp_Muti_Threaded","text":"🔱 C++ 多线程编程[TOC] 🧶 1. 进程和线程的基本概念1.1.1 进程和线程的概念进程是系统中正在运行的一个程序，程序一旦运行就是进程。进程可以看成程序执行的一个实例。进程是系统资源分配的独立实体，每个进程都拥有独立的地址空间。一个进程无法访问另一个进程的变量和数据结构，如果想让一个进程访问另一个进程的资源，需要使用进程间通信，比如管道、文件、套接字等。 线程是进程的进程。一个进程可以拥有多个线程，每个线程使用其所属进程的栈空间。线程的最大数量取决于CPU的核心数。 线程与进程的一个主要区别是，同一进程内的多个线程会共享部分状态，多个线程可以读写同一块内存（一个进程无法直接访问另一进程的内存）。同时，每个线程还拥有自己的寄存器和栈，其他线程可以读写这些栈内存。 线程是进程的一个实体，是进程的一条执行路径。 线程是进程的一个特定执行路径。当一个线程修改了进程的资源，它的兄弟线程可以立即看到这种变化。 1.1.2 并发和并行的概念并发（Concurrent）：并发指在同一时间段内，多个任务都在进行中，但并不一定同时执行。在单核 CPU 上，多个任务占用不同的 CPU 时间片，物理上还是串行执行的。但是由于每个任务所占用的 CPU 时间片非常短（如：10ms），看起来就像是多个任务在共同执行一样，这样的场景就叫做并发。 并发概念图 并行（Parallel）：并行是指在同一时刻，多个任务同时执行。并行需要多核 CPU 或者多处理器系统，让不同的任务在不同的核心上同时运行。但是这并不意味着每个核心上只运行一个任务，每个核心上的运行策略其实和单核 CPU 没有太大区别（即也是并发的）。 并行概念图 🧶 2. 多线程的设计开发一个任务时，要不要实现成并发程序呢？多线程一定更好吗？ 在某些情况下多线程具有一定的优势，但是在某些情况下多线程并不具有优势，这需要根据当前程序的类型进行分析和判断。 程序的类型有以下两种：一种是 I/O密集型；一种是 CPU 密集型。 2.1 I/O 密集型程序I/O密集型，即程序里面指令的执行，涉及一些 I/O 操作，比如设备、文件、网络操作（如等待客户端的连接）等，I/O操作是可以将进程阻塞住的，如果我们再给这样的程序分配时间片，其实就相当于 CPU 空闲下来了。 在 I/O 密集型的程序在执行的时候，在 I/O 操作没有准备好时，程序是会被放在阻塞队列中的，在阻塞队列中是不受操作系统调度的。 正因如此，I/O 密集型程序更适合设计成多线程程序。 I/O 密集型程序不论是在单核还是多核的情况下，都是适合设计成多线程程序的，因为他不会造成 CPU 资源的浪费。 2.2 CPU 密集型程序CPU 密集型程序，即程序里面的指令都是用来做计算用的，例如大量的加减乘除、深度学习都是在进行大量的计算。 CPU密集型程序，在单核情况下是不适合设计成多线程程序的，因为线程的调度有额外的花费：线程的上下文切换（当前线程调度完了，该调度下一个线程）。在这种情况下，相当于只有一个计算器，单线程是一个人一直计算，而多线程是多个人一人算一段。但是传递计算器的过程会产生一定的花销。而单线程进行上下文切换时，要获取之前计算到的信息，这也是一笔开销。 但是其在多核情况下是比较适合设计成多线程程序的。 🧶 3. 线程同步线程同步有两种场景：① 线程互斥；② 线程通信 线程互斥： 互斥锁 mutex 原子类型 atomic 线程通信： 条件变量 condition_variable 信号量 semaphore 3.1 数据竞态（竞态条件）一个进程中的所有线程共享整个进程的堆内存，每个线程私有自己的栈内存。 如下图所示，当我们有多个线程想要执行 Code 时，那我们就要考虑这段代码能否在多线程环境下执行。是否能在多线程环境下执行主要要看这段代码是否存在数据竞态（或称竞态条件）。 竞态条件：代码片段 Code 在多线程环境下执行，随着线程的调度顺序不同，而得到不同的运行结果，这就说明这段代码存在竞态条件。简单来说就是，$Thread1→Thread2→Thread3$ 会得到一个结果，而 $Thread2→Thread1→Thread3$ 可能是另一个结果。这是我们不期待的。 存在竞态条件的代码片段称为临界区代码段。 为了保证不出现临界区代码段，我们应该要保证代码的原子操作。 如果在多线程环境下不存在竞态条件，那么我们称这段代码片段是可重入的（就是一段代码在没执行完的情况下又被运行了），否则是不可重入的。 3.2 线程互斥线程互斥包括：① 互斥锁 mutex；② 原子操作atomic。 mutex 如 lock 操作（悲观锁）、unlock 操作或者 try_lock 操作（活锁、乐观锁）。mutex 是重量级的锁。 但是我们有时候不需要很重量级的锁，比如我们有时候可能只是执行一个自增操作 x++，或者是较为简单的一些操作，这时候我们就不太需要一个重量级的锁，C++ 11 已经提供了 $CAS$ （无锁机制）操作，即 Compare &amp; Set/Compare &amp; Swap。无锁机制并不是说没有锁，而是说这个锁是轻量级的，我们可以用 $CAS$ 实现无锁队列、无锁链表、无锁数组等。 以 C++ 11 的 count++ 为例，count++ 在操作系统中其实进行了三步指令操作，如下图所示： 假设首先执行 $Thread \\ 1$，此时执行 mov eax, count 指令，将 count 的值写到 eax 此时 eax 的值由未知数 x 变为 0； 然后 $Thread \\ 1$ 执行 add eax, 1 指令，将 eax 寄存器中的值加 1； 此时时间片完，进行线程切换操作，切换到 $Thread \\ 2$； 此时执行 $Thread \\ 2$，执行 mov eax, count 指令，将 count 的值写到 eax，此时 count 为从全局获取到的 0，eax 为 未知数x，此时 eax 寄存器由 x 被写为 0； 然后 $Thread \\ 2$​ 执行 add eax, 1 指令，将 eax 寄存器中的值加 1； 此时时间片完，进行线程切换操作，切换到 $Thread 1$； 此时执行 $Thread \\ 1$，执行 mov count, eax 指令，将 $Thread \\ 1$ 上次切换之前的 eax 的值（为 $1$）写到 count 中，count 由 0 变成 1； 此时 $Thread \\ 1$ 执行完毕，切换到 $Thread \\ 2$； 此时执行 $Thread \\ 2$，执行 mov count, eax 指令，将 $Thread \\ 2$ 上次切换之前的 eax 的值（为 $1$）写到 count 中，count 由 1 变成 1。 我们可以发现，即使是简单的 count++ 也不是一个原子操作。 对于这种操作，我们当然可以用重量级的锁（lock）来进行锁定，但是我们更倾向于使用 $CAS$ 来实现这个问题。 3.2 线程通信线程通信包括：① 条件变量 condition_variable；② 信号量 semaphore。 在线程中，并不往往都是几个线程互不相干，有时候会存在一定的依赖关系，如 $Thread \\ 1$ 的某段代码需要依赖于 $Thread \\ 2$ 的某段代码，因为我们并不能保证线程的调度顺序（即有可能被依赖代码可能需要好久才能执行完毕，而已经有线程需要这个代码的结果了），所以我们这个时候就需要进行线程间的通信。 条件变量 条件变量需要配合互斥锁（互斥量）一起使用，其构造函数就需要传入一个互斥锁（互斥量），即 mutex + condition_variable。一个常见的用法就是生产者——消费者模型。*线程池其实就是使用了生产者——消费者模型*。 mutex 互斥锁就是资源计数只能是 $0$ 或 $1$ 的互斥锁，即执行 mutex.lock() 后锁的资源计数 $1→0$，执行 mutex.unlock() 后锁的资源计数 $0→1$。而信号量可以看作 资源计数没有限制 的 mutex 互斥锁。 信号量 信号量都是单独使用的，不需要配合其他条件一同使用。C++ 11 并没有提供信号量的操作，直到 C++ 20 才从语言层面支持了信号量，但是我们完全可以使用 C++ 11 来自己实现一个信号量。信号量也可以用于实现生产者消费者模型，但是无法做到精细的控制。初始信号量为$0$，生产者生产后信号量增加，消费者消费后信号量减少。通过判断信号量来进行简单的线程间通信。 此外还有二元信号量的概念，semaphore sem(1) 资源计数 $0 / 1$，可以完成和 mutex 互斥锁同样的线程互斥操作，但是其和 mutex 是存在一定区别的。mutex 只能是哪个线程获取锁，哪个线程释放锁；semaphore 则不同，sem.wait() 和 sem.post() 可以处在不同的线程中调用。例如：有三个线程调用了 sem.wait() 等待执行一块代码 Code，其中一个线程 $Thread \\ A$ 开始执行代码，其他两个线程 $Thread \\ B$ 和 $Thread \\ C$ 依旧在等待。如果此时有其他代码（存在误操作或者其他原因）调用了 sem.post()，此时会使信号量增加，$Thread \\ B$ 或 $Thread \\ C$ 其中一个会执行代码，若此时 $Thread \\ A$ 还没有执行完代码，这就有可能导致数据竞态问题。 🧶 4. C++ 11 Windows 线程库的基本使用C++ 11 线程库头文件 #include &lt;thread&gt; 创建线程 可以使用 std::thread 来创建线程，这其实是使用线程类 std::thread 声明一个线程实例 printThread 的过程。这里我们直接这样创建线程，生成应用程序之后运行输出结果：Hello World - I'm NilEra @-@，但是运行之后会出现错误。 这里出现错误的原因是：当我们启动了 thread_print 线程，此时程序不会在这里等待线程执行完成，而是会继续向下执行，导致出现程序已经执行到 return 0 时，线程还没有执行完成，因此这里会产生报错。 12345678910111213#include &lt;iostream&gt;#include &lt;thread&gt;// 定义线程需要执行的函数void printThread(void) { std::cout &lt;&lt; &quot;Hello World - I'm NilEra @-@&quot; &lt;&lt; std::endl;}int main(void) { // 创建线程 std::thread thread_print(printThread); return 0;} 主程序等待线程执行完毕 为了解决上述出现的问题，我们需要让主程序等待的线程执行完毕再进行退出，这时我们就需要用到 join() 函数。 1234567891011121314151617#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;// 定义线程需要执行的函数void printHelloWorld(std::string msg) { std::cout &lt;&lt; msg &lt;&lt; std::endl;}int main() { // 创建线程 std::thread thread_print(printHelloWorld, &quot;Hello Thread @-@ I'm NilEra...&quot;); // 等待线程执行完成 thread_print.join(); return 0;} 分离线程 上述的问题还可以使用 detach() 函数来进行解决。当执行下面的程序时，控制台不会又任何输出，直接退出程序。 这是因为线程执行完线程的创建之后，紧接着执行了 detach()，此时线程的具体操作还未来得及执行就进行了线程的分离。进程结束之后，thread_print 线程还在后台运行。但是因为此时进程已经结束，线程执行的过程中不会有输出。 1234567891011121314151617#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;// 定义线程需要执行的函数void printHelloWorld(std::string msg) { std::cout &lt;&lt; msg &lt;&lt; std::endl;}int main() { // 创建线程 std::thread thread_print(printHelloWorld, &quot;Hello Thread @-@ I'm NilEra...&quot;); // 线程分离 thread_print.detach(); return 0;} 1234567891011121314151617181920212223// 使用如下代码可以更清晰的看到执行 detach() 的效果, 以及 join() 和 detach() 的区别// join() 是阻塞的#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;Windows.h&gt;// 定义线程需要执行的函数void printHelloWorld(std::string msg) { std::cout &lt;&lt; msg &lt;&lt; std::endl; for (int i = 0; i &lt; 10000; i++) { std::cout &lt;&lt; i &lt;&lt; std::endl; }}int main() { // 创建线程 std::thread thread_print(printHelloWorld, &quot;Hello Thread @-@ I'm NilEra...&quot;); Sleep(100); thread_print.detach(); return 0;} 判断线程是否可以合并 有时我们需要对线程判断其是否可以进行 join() 操作，此时我们可以调用 joinable() 函数，joinable() 会返回一个 bool 值，用于判断线程是否可以进行 join() 操作。 如果我们对一个不可使用 join() 或者 detach() 的线程进行了 join() 和 detach() 操作，会出现一个 SystemError，在一些比较严谨的项目中，会先使用 joinable() 进行判断。 123456789101112131415161718#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;// 定义线程需要执行的函数void printHelloWorld(std::string msg) { std::cout &lt;&lt; msg &lt;&lt; std::endl;}int main() { // 创建线程 std::thread thread_print(printHelloWorld, &quot;Hello Thread @-@ I'm NilEra...&quot;); bool isJoin = thread_print.joinable(); if (isJoin) { thread_print.join(); } return 0;} 🧶 5. 线程函数中的数据未定义错误5.1 传递临时变量问题 错误示例：如下使用 std::thread 类时，传入线程函数 foo 和 参数 a，这里的参数 a 会被作为值传递，即传递的不是 a 的引用，而是 a 所存储的值 1。但是如果是值传递的话，这里调用的实际上是 foo((int&amp;) 1) 会产生编译错误，因为 1 实际上是一个右值，而非常量引用的初始值必须为左值，所以这里会产生编译错误。 我们平时调用 foo 时，可以使用 foo(a)，实际上我们进行了隐式转换，执行的实际上是 foo((int&amp;) a)。 12345678910111213/* 错误示例 */#include &lt;iostream&gt;#include &lt;thread&gt;void foo(int&amp; x) { x += 1;}int main(void) { int a = 1; std::thread thread_plus(foo, a); return 0;} 12345678910111213/* 修改示例 */#include &lt;iostream&gt;#include &lt;thread&gt;void foo(int&amp; x) { x += 1;}int main(void) { int a = 1; std::thread thread_plus(foo, std::ref(a)); return 0;} 1234567891011121314151617181920212223/* 拓展 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;Windows.h&gt;void foo(int&amp; x) { x += 1;}void const_foo(const int&amp; x) { std::cout &lt;&lt; &quot;const_foo param x is : &quot; &lt;&lt; x &lt;&lt; std::endl;}int main(void) { int a = 1; std::thread thread_plus(foo, std::ref(a)); Sleep(10); std::thread thread_plus_const(const_foo, a); thread_plus.join(); thread_plus_const.join(); return 0;} 5.2 传递指针或引用指向局部变量的问题 错误示例：在下面的程序中，我们定义了一个全局线程变量，并且在 test 函数被调用时开启了这个线程 t，在 main 函数中，执行了 test 函数，在执行到 t = std::thread(foo, std::ref(a)); 时，线程启动。这时会出现两种情况： ① （大概率出现）在线程启动的时候，test 函数已经结束了运行，而局部变量 a 的内存被释放，此时出现空指针错误； ② （小概率出现）程序正常执行，这是因为线程执行的比 test 函数更快，当线程执行结束时，a 还没有被释放。我们可以在 t = std::thread(foo, std::ref(a)); 下添加 Sleep(10)，使程序暂停 10 ms，保证线程结束的时候 test 还未执行完成来观察这一现象。 123456789101112131415161718192021/* 错误示例 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;std::thread t;void foo(int&amp; x) { x += 1;}void test(void) { int a = 1; t = std::thread(foo, std::ref(a));}int main(void) { test(); t.join(); return 0;} 12345678910111213141516171819202122/* 修改示例 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;std::thread t;int a = 1; // 延长 a 的声明周期void foo(int&amp; x) { x += 1;}void test(void) { // int a = 1; t = std::thread(foo, std::ref(a));}int main(void) { test(); t.join(); return 0;} 12345678910111213141516171819202122/* 拓展 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;std::thread t;void foo(int&amp; x) { x += 1;}void test(void) { int a = 1; t = std::thread(foo, std::ref(a)); Sleep(10);}int main(void) { test(); t.join(); return 0;} 5.3 传递指针或引用指向已经释放的内存问题 错误示例：这个问题和上面的问题是差不多的问题，这里有可能会① 直接报编译错误；② 通过编译，但是给出不期待的结果；③ 极小概率出现正常执行的情况。 当我们启动线程后，手动释放 ptr_a，此时若线程 t 的执行在释放内存之前（小概率），不会出现不期待的访问结果；但是如果线程 t 在释放内存之后执行（大概率），则会出现不期待的结果。 12345678910111213141516171819202122/* 错误示例 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;Windows.h&gt;std::thread t;void foo(int* x) { *x += 1; std::cout &lt;&lt; *x &lt;&lt; std::endl;}int main(void) { int* ptr_a = new int(1); // 定义一个指针类型的变量 ptr_a // 初始化为其内存中存储的值为 1 std::thread t(foo, ptr_a); delete ptr_a; // 这里手动释放 ptr_a t.join(); return 0;} 1234567891011121314151617181920212223/* 修改示例 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;Windows.h&gt;std::thread t;void foo(int* x) { *x += 1; std::cout &lt;&lt; *x &lt;&lt; std::endl;}int main(void) { int* ptr_a = new int(1); // 定义一个指针类型的变量 ptr_a // 初始化为其内存中存储的值为 1 std::thread t(foo, ptr_a); Sleep(10); // 启动线程后等待 10 ms, 此时线程大概率已经执行完毕 delete ptr_a; // 这里手动释放 ptr_a t.join(); return 0;} 5.4 类成员作为入口函数，类对象被提前释放 错误示例：这个错误和上面的错误基本上没有区别，只不过是将要 int 型换成了类 类型。 在 main 函数中，创建了一个 MyClass 类型的对象 obj。接着，启动了一个线程 t，这个线程执行 MyClass::func，并传递 obj 的地址 &amp;obj 给它，这里的 &amp;obj 为 this 指针。当 t 线程启动时，它会在后台执行 MyClass::func，但是，main 函数在启动线程后立即返回，而没有等待线程完成。此时，局部变量 obj 会被销毁。如果 t 线程还没有执行完 MyClass::func，则它将尝试访问一个已经销毁的对象，导致未定义行为和运行时错误。 我们可以使用智能指针的方式来防止出现指针提前释放的情况，使用 std::shared_ptr&lt;MyClass&gt; 创建并管理 MyClass 对象的生命周期。std::make_shared&lt;MyClass&gt;() 创建一个 shared_ptr，并返回一个指向堆上分配的 MyClass 对象的共享指针。传递 obj（shared_ptr）给 std::thread 的构造函数时，会增加引用计数，确保 MyClass 对象在 obj 和线程中都有效。 123456789101112131415161718192021/* 错误示例 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;memory&gt;std::thread t;class MyClass {public: void func(void) { std::cout &lt;&lt; &quot;[Thread &quot; &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot;] Started...&quot; &lt;&lt; std::endl; std::cout &lt;&lt; &quot;[Thread &quot; &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot;] Finished...&quot; &lt;&lt; std::endl; }};int main(void) { MyClass obj; std::thread t(&amp;MyClass::func, &amp;obj); return 0;} 12345678910111213141516171819202122232425/* 正确修改 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;memory&gt;std::thread t;class MyClass {public: void func(void) { std::cout &lt;&lt; &quot;[Thread &quot; &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot;] Started...&quot; &lt;&lt; std::endl; std::cout &lt;&lt; &quot;[Thread &quot; &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot;] Finished...&quot; &lt;&lt; std::endl; }};int main(void) { // 使用智能指针 std::shared_ptr&lt;MyClass&gt; obj = std::make_shared&lt;MyClass&gt;(); std::thread t(&amp;MyClass::func, obj); // 保证线程运行结束再退出主程序 t.join(); return 0;} 🧶 6. 互斥量解决多线程数据共享问题数据共享问题：在多个线程中共享数据时，需要注意 线程安全 问题。如果多个线程同时访问同一个变量，并且其中至少有一个线程对该变量进行了写操作，那么就会出现数据竞争问题。数据竞争可能导致程序崩溃、产生未定义错误或者得到错误的结果。 为了避免数据竞争问题，需要使用同步机制来确保多个线程之间对共享数据的访问量是安全的。常见的同步机制包括互斥量、条件操作、原子操作等。 可以看到下面这张图，来体会一下没有锁的情况下导致的多线程数据共享的问题。我们有两个线程 Thread 1 和 Thread 2，这两个线程都在执行的过程中，其操作都是 a += 1。具体过程如下：① 此时我们有一个变量 a = 1，首先 Thread 1 获取到了 a，执行操作后 a=2； ② Thread 2 获取到了 a，执行操作后 a=3； ③ Thread 1 获取到了 a，执行操作后 a=4； ④ Thread 2 获取到了 a，执行操作后 a=5； ⑤ Thread 1 和 Thread 2 同时获取到了 a，同时执行操作后 a=6； 一共执行了 6 次操作，a 应该由 1 变成 7。但是实际上他最终的结果是 6 。 下面演示这种错误： 在这个案例中，我们运行了两个线程 t1 和 t2，每个线程都让 a 加 50,000，因此我们期待的结果是 100,000。但是实际上运行的结果一般是小于 100,000 的。比如我运行了几次，分别是：78,301、61603、54843。说明两个线程多个瞬间同时获取到了变量 a。 当然，循环次数较小的时候也许会出现结果正确的问题，这是因为编译器帮我们汇编成了原子操作。 12345678910111213141516171819202122/* 错误示例 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;memory&gt;int a = 0;void func(void) { for (int i = 0; i &lt; 50000; i++) { a += 1; }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 为了解决这个问题，我们可以使用 互斥量mutex 对变量进行上锁操作。**互斥量的头文件是：#include &lt;mutex&gt;**。 我们修改程序如下： 123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { mtx.lock(); // 加锁操作 a += 1; mtx.unlock(); // 解锁操作 }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 如何判断线程是否是安全的：当多线程程序每一次运行的结果，和单线程程序运行的结果始终都一样，则认为你的线程是安全的。 🧶 7. 互斥量死锁假设有两个线程 Thread 1 和 Thread 2，他们需要对互斥量 mtx1 和互斥量mtx2 进行访问，而且需要按照以下顺序获取互斥量的所有权（获取所有权可以理解为加锁操作 ）： Thread 1 先获取 mtx1 的所有权，再获取 mtx2 的所有权。 Thread 2 先获取 mtx2 的所有权，再获取 mtx1 的所有权。 此时如果两个线程同时运行，就会产生死锁： Thread 1 拿到了 mtx1，同时 Thread 2 拿到了 mtx2。 但是此时因为 Thread 2 占有 mtx2 所以 Thread 1 等待 mtx2 空闲；而 Thread 1 占有 mtx1，所以 Thread 2 等待 mtx1 空闲。 所以此时 Thread 1 和 Thread 2 都无法进一步操作，所以造成了死锁。 1234567891011121314151617181920212223242526272829303132/* 死锁模拟 */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;memory&gt;#include &lt;mutex&gt;#include &lt;Windows.h&gt;std::mutex mtx1, mtx2;void func_1(void) { mtx1.lock(); // 先获取 mtx1 Sleep(1000); // 这里停等 1s, 确保 thread_2 抢占到 mtx2 mtx2.lock(); // 再获取 mtx2 mtx2.unlock(); // 释放 mtx2 mtx1.unlock(); // 释放 mtx1}void func_2(void) { mtx2.lock(); // 先获取 mtx2 mtx1.lock(); // 再获取 mtx1 mtx1.unlock(); // 释放 mtx1 mtx2.unlock(); // 释放 mtx2}int main(void) { std::thread thread_1(func_1); std::thread thread_2(func_2); thread_1.join(); thread_2.join(); std::cout &lt;&lt; &quot;THREAD OVER...&quot; &lt;&lt; std::endl; return 0;} 可以用顺序锁解决上述问题，即两个函数都先获取 mtx1 的所有权。 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;memory&gt;#include &lt;mutex&gt;#include &lt;Windows.h&gt;std::mutex mtx1, mtx2;void func_1(void) { mtx1.lock(); // 先获取 mtx1 mtx2.lock(); // 再获取 mtx2 mtx2.unlock(); // 释放 mtx2 mtx1.unlock(); // 释放 mtx1}void func_2(void) { mtx1.lock(); // 先获取 mtx1 mtx2.lock(); // 再获取 mtx2 mtx2.unlock(); // 释放 mtx2 mtx1.unlock(); // 释放 mtx1}int main(void) { std::thread thread_1(func_1); std::thread thread_2(func_2); thread_1.join(); thread_2.join(); std::cout &lt;&lt; &quot;THREAD OVER...&quot; &lt;&lt; std::endl; return 0;} 🧶 8. lock_guard 和 unique_lock()8.1 lock_guardlock_guard 是 C++ 标准库中一种互斥量的封装类，用于保护共享数据，防止多个线程同时访问统一资源而导致的数据竞争问题。lock_guard 具有以下特点。 当构造函数被调用时，该互斥量会被自动锁定 当析构函数被调用时，该互斥量会被自动解锁 std::lock_guard 对象不能复制或移动，因此他只能在局部作用域中使用 下面简单看几段代码，来体会一下不使用 lock_guard 和使用 lock_guard 的区别： 12345678910111213141516171819202122232425/* 不使用 lock_guard 和 unique_lock() */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { mtx.lock(); // 加锁操作 a += 1; mtx.unlock(); // 解锁操作 }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 当我们添加 lock_guard 后，代码会变成这样： 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { std::lock_guard&lt;std::mutex&gt; lg(mtx); // 这里调用了构造函数, mtx 自动加锁 a += 1; } // 这里调用了析构函数, mtx 自动解锁}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 我们可以详细剖析 lock_guard 的源码，来深刻理解该类。 补充阅读：C++ using 用法 12345678910111213141516171819202122232425262728293031323334353637template &lt;class _Mutex&gt;class _NODISCARD lock_guard { // class with destructor that unlocks a mutex// 这里定义了一个模板类 lock_guard, 其中 _Mutex 是模板参数, 表示互斥锁的类型// _NODISCARD 是一个属性, 表示这个类不应该被忽略(通常是为了警告开发者不要忽略这个类的对象创建)public: using mutex_type = _Mutex; // 定义一个别名 mutex_type, 指向模板参数 _Mutex // 这样可以方便地在类的其他地方使用 mutex_type 来表示互斥锁的类型 // 这是一个构造函数, 接收一个互斥锁的引用 _Mtx // explicit 关键字防止隐式转换 // 在构造函数体内, 调用 _MyMutex.lock() 方法锁定互斥锁 // 这样, 当 lock_guard 对象被创建时, 互斥锁会自动被锁定 explicit lock_guard(_Mutex&amp; _Mtx) : _MyMutex(_Mtx) { // construct and lock _MyMutex.lock(); } // 这是另一个构造函数, 接收一个互斥锁的引用 _Mtx 和一个标记类型 adopt_lock_t // 这个构造函数不会锁定互斥锁, 假设互斥锁已经被锁定 // adopt_lock_t 通常是一个空的结构体类型, 用于区分不同的构造函数 lock_guard(_Mutex&amp; _Mtx, adopt_lock_t) : _MyMutex(_Mtx) {} // construct but don't lock // 析构函数标记为 noexcept, 表示不会抛出异常 // 当 lock_guard 对象被销毁时, 调用 _MyMutex.unlock() 方法解锁互斥锁 // 这确保了即使在异常情况下，互斥锁也会被解锁。 ~lock_guard() noexcept { _MyMutex.unlock(); } // 显式删除拷贝构造函数和赋值操作符 // 防止 lock_guard 对象被复制或赋值 // 这是因为互斥锁的所有权不应该在多个对象之间共享, 以避免潜在的并发问题。 lock_guard(const lock_guard&amp;) = delete; lock_guard&amp; operator=(const lock_guard&amp;) = delete;private: _Mutex&amp; _MyMutex;}; 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { std::lock_guard&lt;std::mutex&gt; lg(mtx); // 1. 创建 lock_guard 对象 lg // 2. 传递 mtx 给构造函数的参数 _Mtx // 3. lock_guard 构造函数被调用 // 4. _MyMutex(_Mtx) 初始化私有成员 _MyMutex 为 mtx 的引用 // 5. _MyMutex.lock() 锁定 mtx a += 1; } // 6. 循环结束时, lg 对象销毁, 调用析构函数 _MyMutex.unlock() 解锁 mtx}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 8.2 unique_lockstd::unique_lock 是 C++ 标准库中提供的一个互斥量封装类，用于在多线程程序中对互斥量进行加锁和解锁操作。与 lock_guard 仅提供自动的加锁、解锁操作不同，unique_lock 还提供了对互斥量进行更加灵活的管理，包括：延迟加锁、条件变量、超时等。 std::unique_lock 提供了以下几个成员函数： lock：尝试对互斥量进行加锁操作，如果当前互斥量已经被其他线程持有，则当前线程会被阻塞，直到互斥量被成功加锁。 try_lock：尝试对互斥量进行加锁操作，如果当前互斥量已经被其他线程持有，则立刻返回 false，否则返回 true。 try_lock_for(const std::chrono::duration&lt;Rep, Period&gt;&amp; rel_time)：尝试对互斥量进行加锁操作，，如果当前互斥量已经被其他线程持有，则当前线程会被阻塞，直到互斥量被成功加锁或超过了指定时间。 try_lock_until(const std::chrono::time_point&lt;Clock, Duration&gt;&amp; abs_time)：尝试对互斥量进行加锁操作，如果当前互斥量已经被其他线程持有，则当前线程会被阻塞，直到互斥量被成功加锁或超过了指定时间点。 unlock()：对互斥量进行解锁操作 下面简单看几段代码，来体会一下不使用 unique_lock 和使用 unique_lock 的区别： 12345678910111213141516171819202122232425/* 不使用 unique_lock */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { mtx.lock(); // 加锁操作 a += 1; mtx.unlock(); // 解锁操作 }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 123456789101112131415161718192021222324/* 使用 unique_lock */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { std::unique_lock&lt;std::mutex&gt; uniqueLock(mtx); a += 1; }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 下面我们演示一下 unique_lock 的更多操作： 8.2.1 lock()/unlock() 手动加锁/解锁既然 unique_lock 支持自动加锁和自动解锁，那么我们为什么不让它自动的加锁和解锁呢？这是因为 unique_lock 提供了更多的加锁方式，在使用其他加锁方式之前，我们需要保证 unique_lock 不能自动加锁。 12std::unique_lock&lt;std::mutex&gt; uniqueLock(mtx, std::defer_lock); // 传入 defer_lock 表示构造函数什么都不做 // 加锁/解锁操作需要由程序员自己完成 那么下面将代码修改如下： 1234567891011121314151617181920212223242526/* 不使用 unique_lock */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50000; i++) { std::unique_lock&lt;std::mutex&gt; uniqueLock(mtx, std::defer_lock); // 使用 unique_lock 进行自动加锁 uniqueLock.lock(); a += 1; uniqueLock.unlock(); }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 8.2.2 try_lock 尝试加锁尝试对互斥量进行加锁操作，如果当前互斥量已经被其他线程持有，则当前线程会被阻塞，直到互斥量被成功加锁。 8.2.3 try_lock_for(const std::chrono::duration&lt;Rep, Period&gt;&amp; rel_time)尝试对互斥量进行加锁操作，，如果当前互斥量已经被其他线程持有，则当前线程会被阻塞，直到互斥量被成功加锁或超过了指定时间。 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;int a = 0;std::timed_mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 50; i++) { std::unique_lock&lt;std::timed_mutex&gt; uniqueLock(mtx, std::defer_lock); // 这里进行判断, 通过检查 try_lock_for 的返回值, 只有在成功锁定时才修改 a, 保证了互斥锁的有效性。 if (uniqueLock.try_lock_for(std::chrono::microseconds(5))) { std::this_thread::sleep_for(std::chrono::microseconds(10)); a += 1; } }}int main(void) { std::thread t1(func); std::thread t2(func); t1.join(); t2.join(); std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;} 常见错误：&quot;try_lock_for&quot;: 不是 &quot;std::mutex&quot; 的成员，std::mutex 不支持延迟加锁，当我们想要进行延迟加锁操作时，需要保证互斥量是时间锁，即 timed_mutex。 此外，不要想当然，以下是一种常见的错误写法： 12345678910111213141516171819202122/* 不使用 unique_lock */#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;memory&gt;#include &lt;mutex&gt;#include &lt;Windows.h&gt;int a = 0;std::timed_mutex mtx; // 定义互斥量void func(void) { for (int i = 0; i &lt; 200; i++) { std::unique_lock&lt;std::timed_mutex&gt; uniqueLock(mtx, std::defer_lock); // try_lock_for 尝试锁定互斥锁, 但返回值没有被检查 // 如果锁定失败, 代码仍然会执行 a += 1, 这会导致数据竞争 uniqueLock.try_lock_for(std::chrono::microseconds(5)); // 即使 try_lock_for 失败, 代码仍然会增加 a, 这使得互斥锁形同虚设 std::this_thread::sleep_for(std::chrono::microseconds(10)); a += 1; }} 8.2.4 try_lock_until(const std::chrono::time_point&lt;Clock, Duration&gt;&amp; abs_time)尝试对互斥量进行加锁操作，如果当前互斥量已经被其他线程持有，则当前线程会被阻塞，直到互斥量被成功加锁或超过了指定时间点。 🧶 9. call_once 实现单例模式9.1 单例模式单例模式：单例模式是一种常见的设计模式，用于确保在系统的整个声明周期内，某个类只能创建一个实例，确保该类的唯一性。由于单例模式是全局唯一的，因此在多线程环境中使用单例模式时，需要考虑线程安全问题。 为什么要使用单例模式：① 节约资源，一个类只有一个实例，不存在多份实例，节省资源；② 方便控制，在一些操作公共资源的场景时，避免了多个对象引起的复杂操作。 单例模式分类：单例模式可以分为 懒汉式 和 饿汉式 ，两者之间的区别在于创建实例的时间不同。 懒汉式：系统运行中，实例并不存在，只有当需要使用该实例时，才会去创建并使用实例，延迟实例化。这种方式要考虑线程安全。 饿汉式：系统一运行，就初始化创建实例，当需要时，直接调用即可，提前实例化。这种方式本身就线程安全，没有多线程的线程安全问题。 单例类的特点： 构造函数和析构函数为私有类型，目的是禁止外部构造和析构。 拷贝构造函数和赋值构造函数是私有类型，目的是禁止外部拷贝和赋值，确保实例的唯一性。 类中有一个获取实例的静态方法，可以全局访问。 9.2 emplace_back 函数快速了解这里使用到了 emplace_back，emplace_back 是 C++ 11 引入的一个 STL 容器方法，用于在容器的末尾直接构造元素。为了更好的理解代码，这里补充一下 emplace_back 的基本用法： emplace_back 方法提供了一种高效、简便的方式在容器末尾添加新元素。与 push_back 方法不同，emplace_back 直接在容器内部构造元素，而不是先构造临时对象然后再移动或复制到容器中。 9.2.1 具体示例假设我们有一个包含复杂对象的 std::vector，我们可以使用 emplace_back 来避免不必要的临时对象创建和销毁。 使用 push_back 12345678910111213141516#include &lt;vector&gt;#include &lt;string&gt;struct MyStruct { int x; std::string y; MyStruct(int a, const std::string&amp; b) : x(a), y(b) {}};int main() { std::vector&lt;MyStruct&gt; vec; MyStruct obj(1, &quot;example&quot;); // 先从外部构造 obj vec.push_back(obj); // 复制构造 vec.push_back(MyStruct(2, &quot;example2&quot;)); // 临时对象构造然后移动构造} 使用 emplace_back 123456789101112131415#include &lt;vector&gt;#include &lt;string&gt;struct MyStruct { int x; std::string y; MyStruct(int a, const std::string&amp; b) : x(a), y(b) {}};int main() { std::vector&lt;MyStruct&gt; vec; vec.emplace_back(1, &quot;example&quot;); // 直接在容器内构造 vec.emplace_back(2, &quot;example2&quot;); // 直接在容器内构造} 在使用 emplace_back 时，构造函数的参数直接传递给容器内的新对象构造函数，避免了临时对象的创建。 9.2.2 在多线程中使用 emplace_back123for (int i = 0; i &lt; 10; ++i) { threads.emplace_back(threadFunction);} 这里的 emplace_back 用于将新的 std::thread 对象添加到 std::vector&lt;std::thread&gt; 容器中。 等效的 push_back 用法如下： 123for (int i = 0; i &lt; 10; ++i) { threads.push_back(std::thread(threadFunction));} 在这种情况下，push_back 和 emplace_back 都可以使用，但 emplace_back 更加高效，因为它避免了临时对象的创建和销毁。使用 emplace_back 时，std::thread 对象直接在 threads 容器中构造： 构造新对象：emplace_back 方法直接在容器的内存空间中构造新对象，而不是先在别处构造然后移动到容器中。 传递参数：emplace_back 将传递的参数直接用于新对象的构造函数，此处 threadFunction 被作为构造函数参数传递给 std::thread。 9.2.3 总结**push_back**：需要一个已经构造好的对象（可能会导致额外的复制或移动）。 **emplace_back**：直接在容器的内存空间内构造对象，避免了额外的临时对象创建和移动操作。 在多线程代码中，使用 emplace_back 可以使代码更加高效和简洁，尤其是在添加新对象到容器时，可以避免不必要的对象拷贝和临时对象创建。 9.3 线程不安全的懒汉模式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;#include &lt;fstream&gt;#include &lt;vector&gt;#include &lt;stdexcept&gt;// 日志类class Logger {public: static Logger&amp; getInstance() { if (instance == nullptr) { instance = new Logger(); } return *instance; } Logger(const Logger&amp;) = delete; Logger&amp; operator=(const Logger&amp;) = delete; void log(const std::string&amp; message) { std::lock_guard&lt;std::mutex&gt; lock(mtx_); logFile_ &lt;&lt; message &lt;&lt; std::endl; } void printLog(const std::string&amp; message) { std::cout &lt;&lt; &quot;Instance Address IS: &quot; &lt;&lt; this &lt;&lt; std::endl; std::cout &lt;&lt; message &lt;&lt; std::endl; }private: Logger() { logFile_.open(&quot;log.txt&quot;, std::ios::out | std::ios::app); if (!logFile_.is_open()) { throw std::runtime_error(&quot;Unable to open log file&quot;); } } ~Logger() { if (logFile_.is_open()) { logFile_.close(); } } static Logger* instance; std::ofstream logFile_; std::mutex mtx_;};Logger* Logger::instance = nullptr;void threadFunction() { Logger&amp; logger = Logger::getInstance(); logger.printLog(&quot;Logging from thread&quot;);}int main() { std::vector&lt;std::thread&gt; threads; for (int i = 0; i &lt; 10; ++i) { threads.emplace_back(threadFunction); } for (auto&amp; thread : threads) { thread.join(); } return 0;} 输出结果如下： 1234567891011121314151617181920Instance Address IS: Instance Address IS: 000001FE29979930Instance Address IS: 000001FE2997A2A0Logging from thread000001FE29979930Logging from threadInstance Address IS: 000001FE2997C330Logging from threadInstance Address IS: 000001FE2997C330Logging from threadInstance Address IS: 000001FE2997B840Logging from threadInstance Address IS: 000001FE2997ACD0Logging from threadInstance Address IS: 000001FE2997C330Logging from threadLogging from threadInstance Address IS: 000001FE2997C330Logging from threadInstance Address IS: 000001FE2997C330Logging from thread 我们可以看到多个不同的实例地址，这是因为：在多线程环境中，当多个线程同时调用 getInstance 方法时，有可能多个线程同时通过 if (instance == nullptr) 检查，并进入实例化代码块。这会导致多个线程同时创建多个实例，违背了单例模式的初衷。 9.4 线程安全的懒汉模式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;#include &lt;fstream&gt;#include &lt;vector&gt;#include &lt;stdexcept&gt;class Logger {public: static Logger&amp; getInstance() { if (instance == nullptr) { std::lock_guard&lt;std::mutex&gt; lock(mtx_); if (instance == nullptr) { instance = new Logger(); } } return *instance; } Logger(const Logger&amp;) = delete; Logger&amp; operator=(const Logger&amp;) = delete; void log(const std::string&amp; message) { std::lock_guard&lt;std::mutex&gt; lock(log_mtx_); logFile_ &lt;&lt; message &lt;&lt; std::endl; } void printLog(const std::string&amp; message) { std::cout &lt;&lt; &quot;Instance Address IS: &quot; &lt;&lt; this &lt;&lt; std::endl; std::cout &lt;&lt; message &lt;&lt; std::endl; }private: Logger() { logFile_.open(&quot;log.txt&quot;, std::ios::out | std::ios::app); if (!logFile_.is_open()) { throw std::runtime_error(&quot;Unable to open log file&quot;); } } ~Logger() { if (logFile_.is_open()) { logFile_.close(); } } static Logger* instance; static std::mutex mtx_; std::mutex log_mtx_; std::ofstream logFile_;};Logger* Logger::instance = nullptr;std::mutex Logger::mtx_;void threadFunction() { Logger&amp; logger = Logger::getInstance(); logger.printLog(&quot;Logging from thread&quot;);}int main() { std::vector&lt;std::thread&gt; threads; for (int i = 0; i &lt; 10; ++i) { threads.emplace_back(threadFunction); } for (auto&amp; thread : threads) { thread.join(); } return 0;} 输出结果如下： 1234567891011121314151617181920Instance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from threadInstance Address IS: 000001E13A41AFE0Logging from thread 9.5 饿汉模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;#include &lt;fstream&gt;#include &lt;vector&gt;#include &lt;stdexcept&gt;class Logger {public: static Logger&amp; getInstance() { return instance; } Logger(const Logger&amp;) = delete; Logger&amp; operator=(const Logger&amp;) = delete; void log(const std::string&amp; message) { std::lock_guard&lt;std::mutex&gt; lock(mtx_); logFile_ &lt;&lt; message &lt;&lt; std::endl; } void printLog(const std::string&amp; message) { std::cout &lt;&lt; &quot;Instance Address IS: &quot; &lt;&lt; this &lt;&lt; std::endl; std::cout &lt;&lt; message &lt;&lt; std::endl; }private: Logger() { logFile_.open(&quot;log.txt&quot;, std::ios::out | std::ios::app); if (!logFile_.is_open()) { throw std::runtime_error(&quot;Unable to open log file&quot;); } } ~Logger() { if (logFile_.is_open()) { logFile_.close(); } } static Logger instance; std::ofstream logFile_; std::mutex mtx_;};Logger Logger::instance;void threadFunction() { Logger&amp; logger = Logger::getInstance(); logger.printLog(&quot;Logging from thread&quot;);}int main() { std::vector&lt;std::thread&gt; threads; for (int i = 0; i &lt; 10; ++i) { threads.emplace_back(threadFunction); } for (auto&amp; thread : threads) { thread.join(); } return 0;} 输出结果如下： 1234567891011121314151617181920Instance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from threadInstance Address IS: 00007FF652E1F6B0Logging from thread 9.6 使用 call_once 实现单例模式为了使用 std::call_once 保证 Logger 类的线程安全，我们可以利用 std::call_once 和 std::once_flag 来确保单例实例只被创建一次。std::call_once 是一个 C++ 11 引入的机制，用于确保给定的函数只被调用一次，即使在多线程环境下。 以下是一个使用 std::call_once 实现线程安全单例模式的示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;#include &lt;fstream&gt;#include &lt;vector&gt;#include &lt;stdexcept&gt;// Logger 类class Logger {public: static Logger&amp; getInstance() { std::call_once(initInstanceFlag, &amp;Logger::initSingleton); // 传入 once_flag 和 initSingleton 函数 return *instance; } Logger(const Logger&amp;) = delete; Logger&amp; operator=(const Logger&amp;) = delete; void log(const std::string&amp; message) { std::lock_guard&lt;std::mutex&gt; lock(mtx_); logFile_ &lt;&lt; message &lt;&lt; std::endl; } void printLog(const std::string&amp; message) { std::cout &lt;&lt; &quot;Instance Address IS: &quot; &lt;&lt; this &lt;&lt; std::endl; std::cout &lt;&lt; message &lt;&lt; std::endl; }private: Logger() { logFile_.open(&quot;log.txt&quot;, std::ios::out | std::ios::app); if (!logFile_.is_open()) { throw std::runtime_error(&quot;Unable to open log file&quot;); } } ~Logger() { if (logFile_.is_open()) { logFile_.close(); } } static void initSingleton() { instance = new Logger(); } static Logger* instance; static std::once_flag initInstanceFlag; std::ofstream logFile_; std::mutex mtx_;};// 初始化静态成员Logger* Logger::instance = nullptr;std::once_flag Logger::initInstanceFlag;void threadFunction() { Logger&amp; logger = Logger::getInstance(); logger.printLog(&quot;Logging from thread&quot;);}int main() { std::vector&lt;std::thread&gt; threads; for (int i = 0; i &lt; 10; ++i) { threads.emplace_back(threadFunction); } for (auto&amp; thread : threads) { thread.join(); } return 0;} 详细解释： 静态成员变量： static Logger* instance：指向单例实例的指针。 static std::once_flag initInstanceFlag：用于保证 initSingleton 只被调用一次的标志。 getInstance 方法： std::call_once(initInstanceFlag, &amp;Logger::initSingleton)：std::call_once 保证 initSingleton 在多线程环境下只被调用一次。initInstanceFlag 确保 initSingleton 只会被执行一次，即使多个线程同时调用 getInstance。 initSingleton 方法： initSingleton 是一个静态方法，用于初始化单例实例。std::call_once 会调用此方法来创建单例实例。 构造函数和析构函数： Logger 的构造函数和析构函数负责打开和关闭日志文件。 log 和 printLog 方法： log 方法使用互斥锁 mtx_ 保护对日志文件的写操作，以确保线程安全。 printLog 方法输出实例的地址和消息。 运行上述代码时，所有线程都会调用 Logger::getInstance() 获取单例实例。由于使用了 std::call_once，initSingleton 方法只会被执行一次，从而确保整个程序中只有一个 Logger 实例。通过输出的实例地址，可以验证所有线程获取的都是相同的实例。 使用 std::call_once 和 std::once_flag 可以确保单例实例在多线程环境下只被创建一次，从而实现线程安全的懒汉单例模式。这样不仅保证了线程安全性，还避免了不必要的锁开销。 🧶 10. condition_variable 条件变量conditon_variable 可以用来实现一个生产者消费者程序。 例如对于一个队列： 只要队列不满，生产者就可以进行生产 只要队列满了，生产者就停止生产 只要队列不空，消费者就可以进行消费 只要队列空了，消费者就停止消费 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;#include &lt;condition_variable&gt;#include &lt;queue&gt;#include &lt;chrono&gt;#include &lt;atomic&gt;std::queue&lt;int&gt; buffer; // 共享缓冲区const unsigned int maxBufferSize = 10; // 缓冲区的最大大小std::mutex mtx; // 互斥锁, 用于保护共享缓冲区std::condition_variable cv; // 条件变量, 用于通知生产者和消费者std::atomic&lt;bool&gt; done(false); // 原子变量, 用于通知消费者生产者已经完成生产// 生产者函数void producer(int id, int numItems) { for (int i = 0; i &lt; numItems; ++i) { std::unique_lock&lt;std::mutex&gt; lock(mtx); // 锁定互斥锁 // 等待直到缓冲区有空闲位置 cv.wait(lock, [] { return buffer.size() &lt; maxBufferSize; }); // 将生成的数据放入缓冲区 buffer.push(i); std::cout &lt;&lt; &quot;Producer &quot; &lt;&lt; id &lt;&lt; &quot; produced: &quot; &lt;&lt; i &lt;&lt; std::endl; lock.unlock(); // 解锁互斥锁 cv.notify_all(); // 通知消费者缓冲区中有新数据 // 模拟生产延迟 std::this_thread::sleep_for(std::chrono::milliseconds(100)); } // 设置 done 标志，通知消费者生产已经完成 done = true; cv.notify_all(); // 通知消费者停止等待}// 消费者函数void consumer(int id) { while (true) { std::unique_lock&lt;std::mutex&gt; lock(mtx); // 锁定互斥锁 // 等待直到缓冲区有数据或生产已经完成 cv.wait(lock, [] { return !buffer.empty() || done; }); // 如果生产已经完成且缓冲区为空, 退出循环 if (done &amp;&amp; buffer.empty()) { break; } // 从缓冲区取出数据 int item = buffer.front(); buffer.pop(); std::cout &lt;&lt; &quot;Consumer &quot; &lt;&lt; id &lt;&lt; &quot; consumed: &quot; &lt;&lt; item &lt;&lt; std::endl; lock.unlock(); // 解锁互斥锁 cv.notify_all(); // 通知生产者有空闲位置 // 模拟消费延迟 std::this_thread::sleep_for(std::chrono::milliseconds(150)); }}int main() { const int numProducers = 2; // 生产者线程数量 const int numConsumers = 2; // 消费者线程数量 const int numItems = 40; // 每个生产者生成的项目数量 std::vector&lt;std::thread&gt; producers; // 生产者线程列表 std::vector&lt;std::thread&gt; consumers; // 消费者线程列表 // 创建生产者线程 for (int i = 0; i &lt; numProducers; ++i) { producers.emplace_back(producer, i, numItems); } // 创建消费者线程 for (int i = 0; i &lt; numConsumers; ++i) { consumers.emplace_back(consumer, i); } // 等待所有生产者线程完成 for (auto&amp; producer : producers) { producer.join(); } // 等待所有消费者线程完成 for (auto&amp; consumer : consumers) { consumer.join(); } return 0;} 🧶 11. 线程池 Thread Pool11.1 线程的消耗为了完成任务，创建很多线程可以吗？线程真的是越多越好吗？ 线程的创建和销毁都是非常”重”的操作 线程的创建和销毁都需要执行不少操作，下面的操作只是一个简化的操作，具体的操作可以深入了解操作系统的原理。 那么如果要在业务执行的过程中去实时的创建和销毁线程，那么是一种很消耗系统资源和性能的操作。 线程栈本身占用大量内存 32 位操作系统的地址空间大小为 $2^{32}$ 个地址，即 4 GB。其中一部分地址空间用于操作系统的内核空间，而另一部分则用于用户空间。通常情况下，用户空间可以是 2 GB 或者 3 GB，剩下的全部是内核空间。具体取决于操作系统的设置。由当前进程创建的所有线程，共享进程的地址空间。 那么一个进程最多可以开多少线程呢？ 假设用户空间为 3 GB，即 $3 × 1024 = 3072$，在 Linux 环境下使用 ulimit -a 可以查询到系统的一些信息，如下： 12345678910111213141516core file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 3795max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 3795virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 我们可以看到 Stack Size 为 8192 kbytes，即 8 MB，则可计算 $3072 ÷ 8 = 384$，所以在 Linux 环境下，一个进程最多创建 $384$ 个线程。 我们将线程函数所用的栈空间就是线程栈。 通过上面的描述，我们可以发现，如果线程数量太多，创建了一大批线程，还没有具体做事情，每一个线程都需要线程栈，栈几乎都被占用完了，就没内存运行其他程序了。 线程的上下文切换要占用大量时间 线程过多，线程的调度是需要上下文切换的，也需要花费大量的 CPU 时间，如果更多的时间花费到上下文切换中，那么实际业务中利用 CPU 的时间就降低了，CPU 的利用率就降低了。 大量线程同时唤醒会使系统经常出现锯齿状负载或者瞬间负载量很大导致宕机 这种情况下，如果同一时间，很多 I/O 操作都准备好了，或者说很多线程都再等待一个 I/O 操作，有可能导致大量线程同时被唤醒，导致系统经常出现锯齿状负载或者瞬间负载量很大导致宕机。 所以我们可以看出，线程不是越多越好。那么创建多少线程才是合适的呢？ 一般来说，创建线程的数量一般由 CPU 的核心数来确定的，即：有几个核创建几个线程。C++ 的很多开源库如 moduo、libevent，Java 的 Netty、mina 等，都采取了这一策略。 当然如果一个功能是重 I/O 的，可以做出适当的调整，增加线程数量。 11.2 什么是线程池？为什么使使用线程池线程过多会带来调度开销，进而影响缓存局部性和整体性能。操作系统上创建线程和销毁线程都是很“重”的操作，耗时耗性能都比较多，那么在服务执行的过程中，如果业务量比较大，实时的去创建线程、执行业务、业务完成后销毁线程，那么会导致系统的实时性能降低，业务的处理能力也会降低。 线程池是一种预先创建一定数量线程的机制，或者说其是一种线程使用模式。 线程池维护着多个线程，这些线程可以在需要时被重复使用，而无需每次都重新创建和销毁线程。 线程池的主要目的是提高性能和资源利用效率，特别是在需要频繁创建和销毁大量线程的场景下，线程池不仅能够保证内核的充分利用，还能防止过分调度。可用线程数据取决于可用的并发处理器、处理器内核、内存、网络 sockets 等数量。 线程池的优势：线程池的优势就是（每个池都有自己的优势），在服务进程启动之初，就事先创建好线程池里面的线程，当业务流量到来时需要分配线程，直接从线程池中获取一个空闲线程执行 Task 任务即可，Task 执行完成后，也不用释放线程，而是把线程归还到线程池中继续给后续的 Task 提供服务。 线程池的优点： 线程和任务分离,提升线程重用性； 控制线程并发数量,降低服务器压力，统一管理所有线程； 提高性能：提升系统响应速度，假如创建线程用的时间为 T1，执行任务用的时间为T2，销毁线程用的时间为 T3，那么使用线程池就免去了 T1 和 T3 的时间。创建和销毁线程都是相对昂贵的操作，特别是在高并发场景下，频繁地创建和销毁线程会极大地降低程序的性能。通过线程池预先创建一定数量的线程并保存在内存中，可以避免频繁地创建和销毁线程，从而提高程序的性能。 资源管理：线程是操作系统级别的资源，如果线程数量过多，可能会导致系统资源的过度消耗，甚至可能导致系统崩溃。通过线程池，可以控制同时运行的线程数量，避免资源过度消耗。 任务调度：线程池可以更方便地进行任务的调度。通过线程池，可以将任务分配给不同的线程执行，实现并行处理，提高程序的执行效率。 简化编程：使用线程池可以简化多线程编程的复杂性。程序员只需要将任务提交给线程池，而不需要关心线程的创建、管理和销毁等细节，降低了多线程编程的难度。 因此，C++线程池的出现是为了解决在高并发场景下创建和销毁线程的开销问题，提高程序的性能和并发处理能力，简化多线程编程的复杂性。 在项目中如何使用线程池？ 以一个添加订单功能为例，我们需要查询用户的收获地址和商品信息。在单线程的代码中，我们需要按照顺序进行查询；而在多线程的代码中，我们在保证两个功能没有依赖关系的情况下可以同时查询（即不需要先查询用户的收获地址，再根据收获地址查询商品信息）。 顺序执行的速度是 503 ms，而多线程执行的速度是 329 ms，能明显提高运行速度（效率提升幅度在 50%~60%）。 11.3 线程池的两种模式11.3.1 fixed 模式线程池fixed 模式线程池里面的线程个数是固定不变的，一般是 ThreadPool 创建时，根据当前机器的 CPU 核心数量进行指定。 11.3.2 cache 模式线程池假设一个线程池内有四个线程，然后此时出现了四个非常耗时的 I/O 操作任务，此时每个线程分配了一个任务，导致四个线程都被占用阻塞在这里了。此时又出现了一些任务，但是由于线程池中的线程长时间阻塞，所以新的任务可能迟迟得不到处理，此时新的任务堵在任务队列中，相当于整个程序卡死在了这里。在这种情况下，我们希望我们线程池的大小是可以动态改变的，这就是 cache 模式的线程池。 cache模式的线程池里面的线程个数是可动态增长的，根据任务的数量动态的增加线程的数量，但是会设置一个线程数量的阈值（线程过多的坏处上面已经讲过了），任务处理完成，如果动态增长的线程空闲了60s还没有处理其它任务，那么关闭线程，保持池中最初数量的线程即可。 11.4 线程池的架构、流程和使用方式 由图所示，我们的线程池主要需要完成以下几个操作： 创建线程池、设置线程池模式、启动线程池 提交异步任务：Result result = pool.submitTask(concreteTask); 保证可以接受各种各样的任务 获取异步任务的处理结果（这里需要用到 Any 上帝类） 实现任务队列，这里任务队列要保证线程安全，且任务队列的任务数不宜过多（过多会导致大量的内存占用） 参考文献基于C++11实现线程池的工作原理 - 靑い空゛ - 博客园 (cnblogs.com) C++线程池的原理（画图）及简单实现+例子（加深理解）_C++ 线程池原理 - CSDN博客 C++: 每一个C++程序员都应该知道的RAII - 个人文章 - SegmentFault 思否 C++线程池 - BrianX - 博客园 (cnblogs.com) C++笔记-Atomic原子操作/CAS(Compare and Swap) - 流了个火 - 博客园 (cnblogs.com)","link":"/2024/07/29/Cpp-Muti-Threaded/"},{"title":"Cpp_Network_Programming","text":"","link":"/2024/07/29/Cpp-Network-Programming/"},{"title":"SpringCloud","text":"SpringCloud[TOC] 1. 微服务什么是微服务：微服务是一种软件架构风格，它是以专注于单一职责的很多小型项目为基础，组合出复杂的大型应用。 传统的单体项目开发将一个项目的所有模块都集中在这个项目中。而微服务可以理解为对一个单体项目的拆分，将单体项目的边界打破，并且将一个庞大的项目拆分成一个一个小项目。 当然微服务会涉及到一些问题，主要包括下面这些方面： 服务拆分 远程调用 服务治理 请求路由 身份认证 配置管理 服务保护 分布式事务 异步通信 消息可靠性 延迟消息 分布式搜索 倒排索引 数据聚合","link":"/2024/07/30/SpringCloud/"},{"title":"MyBatisPlus","text":"🐦MyBatis-Plus[TOC] 🚪 1. 快速入门1.1 入门案例入门案例：基于课前资料提供的项目，实现下列功能: 新增用户功能 根据 id 查询用户 根据 id 批量查询用户 根据 id 更新用户 根据 id 删除用户 1.1.1 引入 MyBatis-Plus 的依赖123456&lt;!-- MyBatisPlus --&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.3.1&lt;/version&gt;&lt;/dependency&gt; 1.1.2 定义 Mapper自定义的 Mapper 继承 MybatisPlus提供的 BaseMapper 接口。 BaseMapper&lt;T&gt; 给我们提供了很多基础方法，如常用的增、删、改、查等操作。 1public interface UserMapper extends BaseMapper&lt;User&gt; { } 1.1.3 直接使用做完上面两步操作，此时我们不需要再编写复杂的 SQL 语句即可对数据库进行增、删、改、查操作。 当然对于原有 MyBatis 操作，我们也可以直接使用，因为 MyBatis-Plus 对 MyBatis 是非侵入的，体现了其润物无声的特点。 1.2 常见注解在初次使用 MyBatis-Plus 时，我们并没有指定要执行操作的表信息和字段信息，那么 Mybatis-Plus 是如何知道我们要查询的是哪张表？表中有哪些字段呢？ MyBatisPlus 通过扫描实体类，并基于反射获取实体类信息作为数据库表信息。 在入门案例中 1.1.2 定义 Mapper 时，我们需要特别指定泛型类即 &lt;User&gt;。泛型中的 User 就是与数据库对应的 PO。 MyBatis-Plus 就是根据 PO 实体的信息来推断出表的信息，从而生成 SQL 的。默认情况下： MyBatis-Plus 会把 PO 实体的类名驼峰转下划线作为表名，如类名为 UserInfo，反射的表名为 user_info； MyBatis-Plus 会把 PO 实体的所有变量名驼峰转下划线作为表的字段名，并根据变量类型推断字段类型； MyBatis-Plus 会把名为 id 的字段作为主键。 但很多情况下，默认的实现与实际场景不符，因此 MyBatis-Plus 提供了一些注解便于我们声明表信息。 1.2.1 @TableName 注解 描述：表名注解，标识实体类对应的表 使用位置：实体类 123456/* 标注实体类对应的表名 */@TableName(&quot;t_user&quot;)public class User { private Long id; private String name;} @TableName 注解除了指定表名以外，还可以指定很多其它属性： 属性 类型 必须指定 默认值 描述 value String 否 “” 表名 schema String 否 “” schema keepGlobalPrefix boolean 否 false 是否保持使用全局的 tablePrefix 的值（当全局 tablePrefix 生效时） resultMap String 否 “” xml 中 resultMap 的 id（用于满足特定类型的实体类对象绑定） autoResultMap boolean 否 false 是否自动构建 resultMap 并使用（如果设置 resultMap 则不会进行 resultMap 的自动构建与注入） excludeProperty String[] 否 {} 需要排除的属性名 @since 3.3.1 1.2.2 @TableId 注解 描述：主键注解，标识实体类中的主键字段 使用位置：实体类的主键字段 123456@TableName(&quot;t_user&quot;)public class User { @TableId(value=&quot;id&quot;, type=IdType.AUTO) private Long id; private String name;} TableId 注解支持两个属性： 属性 类型 必须指定 默认值 描述 value String 否 “” 表名 type Enum 否 IdType.NONE 指定主键类型 IdType支持的类型有： 值 描述 AUTO 数据库 ID 自增 NONE 无状态，该类型为未设置主键类型（注解里等于跟随全局，全局里约等于 INPUT） INPUT insert 前自行 set 主键值 ASSIGN_ID 分配 ID（主键类型为 Number（Long 和 Integer）或（String）（since 3.3.0），使用接口IdentifierGenerator 的方法 nextId （默认实现类为 DefaultIdentifierGenerator 雪花算法） ASSIGN_UUID 分配 UUID，主键类型为 String*(since 3.3.0)*，使用接口 IdentifierGenerator 的方法 nextUUID（默认 default 方法） ID_WORKER 分布式全局唯一 ID 长整型类型(please use ASSIGN_ID) UUID 32 位 UUID 字符串（please use ASSIGN_UUID） ID_WORKER_STR 分布式全局唯一 ID 字符串类型(please use ASSIGN_ID) 这里比较常见的有三种： AUTO：利用数据库的 id 自增长 INPUT：手动生成 id ASSIGN_ID：雪花算法生成 Long 类型的全局唯一 id，这是默认的ID策略 1.2.3 @TableField 注解 描述：普通字段注解 示例： 12345678910111213141516171819@TableName(&quot;user&quot;)public class User { @TableId(value=&quot;u_id&quot;, type=IdType.AUTO) private Long id; @TableField(&quot;username&quot;) // 成员变量名与数据库字段名不一致 private String name; private Integer age; @TableField(&quot;isMarried&quot;) // 成员变量是以 `isXXX` 命名 private Boolean isMarried; @TableField(&quot;`concat`&quot;) // 成员变量名与数据库一致, 但是与数据库的关键字冲突 private String concat; @TableField(exist = false) // 成员变量不是数据库字段 private String address;} 一般情况下我们并不需要给字段添加 @TableField 注解，一些特殊情况除外： 成员变量名与数据库字段名不一致； 成员变量是以 isXXX 命名，按照JavaBean的规范，MyBatis-Plus识别字段时会把 is 去除，这就导致与数据库不符； 成员变量名与数据库一致，但是与数据库的关键字冲突。使用 @TableField 注解给字段名添加转义字符：``； 成员变量不是数据库字段，添加 exist = false 支持的其它属性如下： 属性 类型 必填 默认值 描述 value String 否 “” 数据库字段名 exist boolean 否 true 是否为数据库表字段 condition String 否 “” 字段 where 实体查询比较条件，有值设置则按设置的值为准，没有则为默认全局的 %s=#{%s}，参考(opens new window) update String 否 “” 字段 update set 部分注入，例如：当在version字段上注解update=&quot;%s+1&quot; 表示更新时会 set version=version+1 （该属性优先级高于 el 属性） insertStrategy Enum 否 FieldStrategy.DEFAULT 举例：NOT_NULL insert into table_a(&lt;if test=&quot;columnProperty != null&quot;&gt;column&lt;/if&gt;) values (&lt;if test=&quot;columnProperty != null&quot;&gt;#{columnProperty}&lt;/if&gt;) updateStrategy Enum 否 FieldStrategy.DEFAULT 举例：IGNORED update table_a set column=#{columnProperty} whereStrategy Enum 否 FieldStrategy.DEFAULT 举例：NOT_EMPTY where &lt;if test=&quot;columnProperty != null and columnProperty!=''&quot;&gt;column=#{columnProperty}&lt;/if&gt; fill Enum 否 FieldFill.DEFAULT 字段自动填充策略 select boolean 否 true 是否进行 SELECT 查询 keepGlobalFormat boolean 否 false 是否保持使用全局的 format 进行处理 jdbcType JdbcType 否 JdbcType.UNDEFINED JDBC 类型 (该默认值不代表会按照该值生效) typeHandler TypeHander 否 类型处理器 (该默认值不代表会按照该值生效) numericScale String 否 “” 指定小数点后保留的位数 1.3 常见配置MyBatis-Plus 也支持基于 yaml 文件的自定义配置，详见官方文档： MyBatis-Plus 官方文档 大多数的配置都有默认值，因此我们都无需配置。但还有一些是没有默认值的，例如: 实体类的别名扫描包 全局id类型 12345678910mybatis-plus: type-aliases-package: com.itheima.mp.domain.po # 别名扫描包 mapper-locations: &quot;classpath*:/mapper/**/*.xml&quot; # Mapper.xml文件地址，当前这个是默认值。 configuration: mapper-underscore-to-camel-case: true # 是否开启下划线驼峰映射 cache-enabled: false # 是否开启二级缓存 global-config: db-config: id-type: auto # 全局 id 类型为自增长, 可以更换为其他方法, 如 assign_id update-strategy: not_null # 更新策略: 只更新非空字段 需要注意的是，MyBatis-Plus也支持手写 SQL 的，而 mapper 文件的读取地址可以通过 mapper-locations 配置：默认值是classpath*:/mapper/**/*.xml，也就是说我们只要把 mapper.xml 文件放置这个目录下就一定会被加载。 🥑 2. 核心功能入门案例中都是以 id 为条件的简单CRUD，一些复杂条件的 SQL 语句就要用到一些更高级的功能了。 2.1 条件构造器除了新增以外，修改、删除、查询的 SQL 语句都需要指定 WHERE 条件。因此 BaseMapper 中提供的相关方法除了以 id 作为 WHERE 条件以外，还支持更加复杂的 WHERE条件。 参数中的 Wrapper 就是条件构造的抽象类，其下有很多默认实现，继承关系如图： Wrapper 的子类 AbstractWrapper 提供了 WHERE 中包含的所有条件构造方法： 而 QueryWrapper 在 AbstractWrapper 的基础上拓展了一个 SELETE 方法，允许指定查询字段： 而 UpdateWrapper 在 AbstractWrapper 的基础上拓展了一个 SET 方法，允许指定 SQL 中的 SET 部分： 接下来，我们就来看看如何利用 Wrapper 实现复杂查询。 2.1.1 QueryWrapper无论是修改、删除、查询，都可以使用QueryWrapper来构建查询条件。 当前表的结构如下： # 名称 数据类型 注释 长度/集合 默认 1 id BIGINT 用户ID 19 AUTO_INCREMENT 2 username VARCHAR 用户名 50 无默认值 3 password VARCHAR 密码 128 无默认值 4 phone VARCHAR 注册手机号 20 NULL 5 info JSON 详细信息 无默认值 6 status INT 使用状态(1: 正常 2: 冻结) 10 “1” 7 balance INT 账户余额 10 8 create_time DATETIME 创建时间 CURRENT_TIMESTAMP 9 update_time DATETIME 更新时间 CURRENT_TIMESTAMP 接下来看一些例子： 查询：查询出名字中带o的，存款大于等于1000元的人。代码如下： 123SELECT id,username,info,balanceFROM userWHERE username LIKE &quot;%o%&quot; AND Balance &gt;= 100 1234567891011@Testvoid testQueryWrapper() { // 1. 构建查询条件 WHERE name LIKE &quot;%o%&quot; AND balance &gt;= 1000 QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;() .select(&quot;id&quot;, &quot;username&quot;, &quot;info&quot;, &quot;balance&quot;) .like(&quot;username&quot;, &quot;o&quot;) .ge(&quot;balance&quot;, 1000); // 2. 查询数据 List&lt;User&gt; users = userMapper.selectList(wrapper); users.forEach(System.out::println);} 更新：更新用户名为 Jack 的用户的余额为 2000，代码如下： 123UPDATE user SET balance=2000 WHERE (username=&quot;Jack&quot;) 12345678910@Testvoid testUpdateByQueryWrapper() { // 1.构建查询条件 WHERE name=&quot;Jack&quot; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;().eq(&quot;username&quot;, &quot;Jack&quot;); // 2.更新数据, user 中非 NULL 字段都会作为 SET 语句 User user = new User(); user.setBalance(2000); userMapper.update(user, wrapper);} 2.1.2 UpdateWrapper基于 BaseMapper 中的 UPDATE 方法更新时只能直接赋值，对于一些复杂的需求就难以实现。 例如：更新 id 为 1、2、4 的用户的余额，扣 200，对应的SQL应该是： 1UPDATE user SET balance = balance - 200 WHERE id in (1, 2, 4) SET 的赋值结果是基于字段现有值的，这个时候就要利用 UpdateWrapper 中的 setSql 功能了： 123456789101112@Testvoid testUpdateWrapper() { List&lt;Long&gt; ids = List.of(1L, 2L, 4L); // 1.生成SQL UpdateWrapper&lt;User&gt; wrapper = new UpdateWrapper&lt;User&gt;() .setSql(&quot;balance = balance - 200&quot;) // SET balance = balance - 200 .in(&quot;id&quot;, ids); // WHERE id in (1, 2, 4) // 2.更新, 注意第一个参数可以给null, 也就是不填更新字段和数据， // 而是基于 UpdateWrapper 中的 setSQL 来更新 userMapper.update(null, wrapper);} 2.1.3 LambdaQueryWrapper无论是 QueryWrapper 还是 UpdateWrapper 在构造条件的时候都需要写死字段名称，会出现字符串魔法值。这在编程规范中显然是不推荐的。 那怎么样才能不写字段名，又能知道字段名呢？ 其中一种办法是基于变量的 getter 方法结合反射技术。因此我们只要将条件对应的字段的 getter 方法传递给 MyBatis-Plus，它就能计算出对应的变量名了。而传递方法可以使用 JDK8 中的 方法引用 和 Lambda 表达式。 因此 MyBatis-Plus 又提供了一套基于Lambda 的 Wrapper，包含两个： LambdaQueryWrapper LambdaUpdateWrapper 分别对应 QueryWrapper 和 UpdateWrapper 其使用方式如下： 123456789101112@Testvoid testLambdaQueryWrapper() { // 1.构建条件 WHERE username LIKE &quot;%o%&quot; AND balance &gt;= 1000 QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); wrapper.lambda() .select(User::getId, User::getUsername, User::getInfo, User::getBalance) .like(User::getUsername, &quot;o&quot;) .ge(User::getBalance, 1000); // 2.查询 List&lt;User&gt; users = userMapper.selectList(wrapper); users.forEach(System.out::println);} 2.2 自定义 SQL一般企业的开发规范中，是不允许开发人员将 SQL 脱离出 Mapper 层或者 mapper.xml 文件的，但是我们想要使用 MyBatis-Plus 又想要遵守企业的开发规范，应该如何做呢？ 我们可以利用 MyBatis-Plus 的 Wrapper 来构建复杂的 WHERE 条件，然后自己定义 SQL 语句中剩下的部分。 ① 基于 Wrapper 构造 WHERE 条件 12345678List&lt;Long&gt; ids = List.of(1L, 2L, 4L);int amount = 200;// 1. 构造条件LambdaQueryWrapper&lt;User&gt; wrapper = new LambdaQueryWrapper&lt;User&gt;().in(User::getId, ids);// 2. 自定义SQL方法调用userMapper.updateBalanceByIds(wrapper, amount); ② 在 mapper 方法参数中用 @Param 注解声明 Wrapper 变量名称，这个名称必须是 ew 1void updateBalanceByIds(@Param(&quot;ew&quot;) LambdaQueryWrapper&lt;User&gt; wrapper, @Param(&quot;amount&quot;) int amount); ③ 自定义 SQL，并使用 Wrapper 条件，这里 ew.customSqlSegment 是 Wrapper 的一个方法，用于获取用户自定义SQL片段。如果使用出现问题，参考博客：MyBatis-Plus ${ew.customSqlSegment} 使用的史诗级大坑-CSDN博客 123&lt;update id=&quot;updateBalanceByIds&quot;&gt; UPDATE t_user SET balance = balance - #{amount} ${ew.customSqlSegment};&lt;/update&gt; 2.3 Service 接口MyBatis-Plus 不仅提供了 BaseMapper，还提供了通用的 Service 接口及默认实现，封装了一些常用的 service 模板方法。 通用接口为 IService，默认实现为 ServiceImpl，其中封装的方法可以分为以下几类： save：新增 remove：删除 update：更新 get：查询单个结果 list：查询集合结果 count：计数 page：分页查询 2.3.1 CRUD首先了解基本的CRUD接口。 新增： save 是新增单个元素 saveBatch 是批量新增 saveOrUpdate 是根据id判断，如果数据存在就更新，不存在则新增 saveOrUpdateBatch 是批量的新增或修改 删除： removeById：根据id删除 removeByIds：根据id批量删除 removeByMap：根据Map中的键值对为条件删除 remove(Wrapper&lt;T&gt;)：根据Wrapper条件删除 ~~removeBatchByIds~~：暂不支持 修改： updateById：根据id修改 update(Wrapper&lt;T&gt;)：根据UpdateWrapper修改，Wrapper中包含set和where部分 update(T，Wrapper&lt;T&gt;)：按照T内的数据修改与Wrapper匹配到的数据 updateBatchById：根据id批量修改 Get： getById：根据 id 查询 1 条数据 getOne(Wrapper&lt;T&gt;)：根据Wrapper 查询 1 条数据 getBaseMapper：获取Service 内的 BaseMapper 实现，某些时候需要直接调用 Mapper 内的自定义 SQL 时可以用这个方法获取到 Mapper List： listByIds：根据id批量查询 list(Wrapper&lt;T&gt;)：根据Wrapper条件查询多条数据 list()：查询所有 Count： count()：统计所有数量 count(Wrapper&lt;T&gt;)：统计符合Wrapper条件的数据数量 getBaseMapper： 当我们在service中要调用Mapper中自定义SQL时，就必须获取service对应的Mapper，就可以通过这个方法： 2.3.2 基本用法由于 Service 中经常需要定义与业务有关的自定义方法，因此我们不能直接使用 IService，而是自定义 Service 接口，然后继承 IService 以拓展方法。同时，让自定义的 Service实现类 继承 ServiceImpl，这样就不用自己实现 IService 中的接口了。 具体方法如下： ① 首先，定义 IUserService 类，继承 IService： 12345678package com.itheima.mp.service;import com.baomidou.mybatisplus.extension.service.IService;import com.itheima.mp.domain.po.User;public interface IUserService extends IService&lt;User&gt; { // 拓展自定义方法} ② 然后，编写 UserServiceImpl 类，继承 ServiceImpl，实现 UserService： 123456789101112package com.itheima.mp.service.impl;import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl;import com.itheima.mp.domain.po.User;import com.itheima.mp.domain.po.service.IUserService;import com.itheima.mp.mapper.UserMapper;import org.springframework.stereotype.Service;@Servicepublic class UserServiceImpl extends ServiceImpl&lt;UserMapper, User&gt; implements IUserService {} 项目结构如下： 接下来，我们基于 RESTful 风格快速实现下面几个接口（什么是RESTful风格）： 简单理解 REST 就是：URL 中只使用名词来定位资源，用 HTTP 协议里的动词（GET、POST、PUT、DELETE）来实现资源的增删改查操作。 什么是 RESTful： 使用客户/服务器（B/S、 C/S）模型：客户和服务器之间通过一个统一的接口来互相通讯。 层次化的系统：在一个 REST 系统中，客户端并不会固定地与一个服务器打交道。 无状态：在一个 REST 系统中，服务端并不会保存有关客户的任何状态。也就是说，客户端自身负责用户状态的维持，并在每次发送请求时都需要提供足够的信息。 可缓存：REST 系统需要能够恰当地缓存请求，以尽量减少服务端和客户端之间的信息传输，以提高性能。 统一的接口。一个 REST 系统需要使用一个统一的接口来完成子系统之间以及服务与用户之间的交互。这使得 REST 系统中的各个子系统可以独自完成演化。 如果一个系统满足了上面所列出的五条约束，那么该系统就被称为是 RESTful 的。 编号 接口 请求方式 请求路径 请求参数 返回值 1 新增用户 POST /users 用户表单实体 无 2 删除用户 DELETE /users/{id} 用户 id 无 3 根据 id 查询用户 GET /users/{id} 用户 id 用户VO 4 根据 id 批量查询 GET /users 用户 id 集合 用户VO集合 5 根据 id 扣除余额 PUT /users/{id}/deduction/{money} 1. 用户 id2. 扣除金额 无 🕸 3. 拓展功能🧩 4. 插件功能","link":"/2024/07/30/MyBatisPlus/"},{"title":"LLM_General_Education","text":"🤗 大语言模型通识大语言模型的配置需求首先要搞清楚，本地可以部署什么大模型，取决于个人电脑的硬件配置，尤其需要关注 GPU 的显存。一般来说，只要本地机器 GPU 的显存能够满足大模型的要求，那基本上都可以本地部署。 那么大模型类别这么多，有 $7B$、$13B$、$70B$ 等等，GPU 显存如何准备呢？ 在没有考虑任何模型量化技术的前提下，有公式如下：$$GB = B × 2$$ 其中为 $GB$ 模型显存占用，$B$ 为大模型参数量。 参考资料AI大模型本地化部署Q/A硬件篇 如何找到最新的大模型、如何判断本地硬件是否满足大模型需求、如何快速部署大模型 大模型综合评测对比 | 当前主流大模型在各评测数据集上的表现总榜单 | 数据学习 (DataLearner)","link":"/2024/08/01/LLM-General-Education/"},{"title":"Deploy_LLMs_ON_PC","text":"如何搭建运行在本地的 LLMs 🤔[TOC] 🤗 1. 基于 LM-Studio 访问 LM-Studio，网址：LM Studio - Discover, download, and run local LLMs 下载对应系统的安装包，然后双击运行即可。 访问 ModelScope魔搭社区 或者 🤗Hugging FaceHugging Face，这里以 ModelScope 为例，进入模型库，下载相应模型。 魔搭社区官网 找到需要的模型并下载 下载好响应的模型后，将模型组织好，放到相应的文件夹中，这里按照 models/Publisher/Repository/*.gguf 的路径组织模型路径，然后选择 Change 更改模型的位置。如果不按照该路径组织，则会出现 You have 1 uncategorized model files. 错误，如下图所示： 但是那种方式是不太推荐的，我们组织 USER/MODEL_NAME/*.gguf 的结构，这种结构会比较明了： 完成模型文件的下载和组织后，我们可以进入聊天页面，选择模型进行加载。这里为了节约空间，我删除了 nilera/Qwen1.5-7B-Chat-Q4-GGUF 目录下的文件。 选择模型加载，等待加载完成即可像平时使用其他大模型的时候一样使用这些模型。 但是如果我们想在代码中使用我们的大模型应该怎么做呢？我们可以选择 LM-Studio 的 Local Server 菜单项，选择 Start Server 即可部署到一个本地指定的端口（默认是 1234）。 右侧有许多样例，我们可以选择一段样例，如：chat(python)，这里对这段代码进行简单的解释。 12345678910111213141516# Example: reuse your existing OpenAI setupfrom openai import OpenAI# Point to the local serverclient = OpenAI(base_url=&quot;http://localhost:1234/v1&quot;, api_key=&quot;lm-studio&quot;)completion = client.chat.completions.create( model=&quot;Publisher/Repository&quot;, # 可以理解为模型路径, 这里以启动在这个端口的模型为准 messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Always answer in Chinese.&quot;}, # 系统设置: 每次都用中文回答 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Introduce yourself.&quot;} # 对话设置: 这里希望 AI 介绍一下他自己 ], temperature=0.7,)print(completion.choices[0].message) # 获取模型的回复 然后我们就可以愉快的使用 Python 调用我们的本地大模型了。 ⛵ 2. 使用 PowerInfer 框架PowerInfer 框架 GitHub 链接：SJTU-IPADS/PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs (github.com) $2024$ 年发布论文 PowerInfer-2：[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone (arxiv.org) Anaconda 命令使用：【anaconda】conda创建、查看、删除虚拟环境（anaconda命令集）_conda 创建环境-CSDN博客 参考博客：大模型笔记之-3090显卡推理70B参数模型|基于PowerInfer 一个 CPU/GPU LLM 推理引擎-CSDN博客 使用 Conda 创建环境，这里 Python 版本需要大于 3.8： 1conda create -n powerinfer1 python=3.8 激活 Conda 环境： 1conda activate powerinfer1 克隆 PowerInfer 框架代码： 1git clone git@github.com:SJTU-IPADS/PowerInfer.git 安装所需依赖： 1pip install -r requirements.txt 使用 CMake 进行编译（CMake 版本需要大于：3.17+） 这里很大概率可能会出现编译器版本与 CUDA 版本不一致的情况，解决方案：fatal error C1189: #error: – unsupported Microsoft Visual Studio version! - CSDN博客 这里我有三个 CUDA 版本，貌似修改其中任意一个就可以，这里我修改的是 CUDA v11.6 版本。 ① 如果是 NVIDIA GPUs，需要使用如下方式进行编译： 12cmake -S . -B build -DLLAMA_CUBLAS=ONcmake --build build --config Release ② 如果是 AMD GPUs，需要使用下面的方式进行编译： 1234# Replace '1100' to your card architecture name, you can get it by rocminfoCC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++ cmake -S . -B build -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1100cmake --build build --config Release ③ 如果是 CPU ONLY，需要使用下面的方式进行编译： 12cmake -S . -B buildcmake --build build --config Release 这里我有一块 Nvidia 1050ti 所以我使用方式 ①进行编译。 对于我们下载的模型，可以使用提供的方式进行转化，转化为 PowerInfer 可以使用的类型： 123# make sure that you have done `pip install -r requirements.txt`python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR# python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor 1python convert.py --outfile D:/LMStudio/models/Publisher/Repository/qwen1_5-7b-chat-q4_0.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor 或者将要 原始模型转化为 GGUF 模型 1python convert-dense.py --outfile /PATH/TO/DENSE/GGUF/REPO/MODELNAME.gguf /PATH/TO/ORIGINAL/MODEL 运行模型 123./build/bin/Release/main.exe -m C:/Users/NilEra/Downloads/llama-7b-relu.powerinfer.gguf -n 128 -t 2 -p &quot;Once upon a time&quot;# 其中/home/user/data/ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.q4.powerinfer.gguf为GPTQ量化过的模型文件 一些问题：","link":"/2024/08/05/Deploy-LLMs-ON-PC/"},{"title":"CPP_11 functional bind","text":"C++ 11 函数适配器 bind参考博客： C++11新特性：参数绑定——std::bind - 莫水千流 - 博客园 (cnblogs.com) 【C++】C++11的std::function和std::bind用法详解_c++11 新增了 std::function、std::bind-CSDN博客","link":"/2024/08/06/CPP-11-functional-bind/"},{"title":"CPP_11 Enum Class","text":"C++ 11 枚举类 enum class我们在 C++ 中常使用 enum 来给同一类别中的多个值命名，如：给颜色中的 0, 1, 2, 3, ... 值命名，可以用下面的写法： 1234567enum Color { Red, Yellow, Blue, Gray, ...}; C++ 的 C98 标准称 enum 为不限范围的枚举型别。因为 C++ 中的枚举量会泄露到包含这个枚举类型的作用域内，在这个作用域内就不能有其他实体取相同的名字。我们可以通过一段代码来演示这一现象： 123456789101112131415#include &lt;iostream&gt;enum Color { RED, YELLOW, BLUE, GRAY};auto GRAY = 10; int main(void) { std::cout &lt;&lt; GRAY &lt;&lt; std::endl; return 0;} 当我们编译时，会出现重定义错误：error: 'auto GRAY' redeclared as different kind of entity。 为了解决这一问题，C++ 11 新标准提供了 enum Class 枚举类。对于上面的代码，我们再一次做出演示： 12345678910111213141516#include &lt;iostream&gt;enum class Color { RED, YELLOW, BLUE, GRAY};auto GRAY = 10;int main(void) { std::cout &lt;&lt; static_cast&lt;int&gt;(Color::GRAY) &lt;&lt; std::endl; std::cout &lt;&lt; GRAY &lt;&lt; std::endl; return 0;} 此时，输出 3 和 10。可以看到在全局作用域的 GRAY 被赋值成了 10，而枚举类中的 GRAY 还是 3，且必须使用作用域限定符进行访问。 这里可以看到我使用了一个 static_cast&lt;int&gt; (Color::GRAY) 进行了一个强制类型转换，这是因为 enum 不支持隐式类型转换。如果想要进行转换，则必须使用 static_cast 进行强制类型转换。","link":"/2024/08/06/CPP-11-Enum-Class/"},{"title":"Windows Build llama.cpp","text":"Windows 平台下构建 llama.cpp在使用 LM-Studio 时，对于一些参数量不是很大的模型来说，大多数不需要进行模型的合并，如 qwen2-7b 等。这些模型往往只需要下载后加载到 LM-Studio 中即可。 但是对于参数量很大的模型，如 qwen2-72b-instruct 等，因为模型文件较大不利于传输，因此模型开发者可能会使用 llama.cpp 对 GGUF 模型进行拆分，所以这个时候我们在下载模型时就需要进行模型的合并。 qwen2-72b-instruct 在 q8 量化给出了两个模型文件，分别是： 12qwen2-72b-instruct-q8_k_m-00001-of-00002.ggufqwen2-72b-instruct-q8_k_m-00002-of-00002.gguf 为了使用这些分割后的 GGUF 文件，我们可以使用 llama-gguf-split 合并他们 1llama-gguf-spilt --merge input.gguf output.gguf","link":"/2024/08/12/Windows-Build-llama-cpp/"},{"title":"Deploy_LLMs_ON_Linux","text":"如何在 Linux 服务器上搭建本地LLMs 🤔如何在 Linux 服务器上部署大语言模型，以 qwen1_5-32b-chat-q8_k_0 为例。服务器使用显卡 A4000，预算：$5950$ 元。 搭建 qwen1_5-32b-chat-q8_k_0 下载 🤗Hugging Face 库，这个库主要是用于下载模型使用。当然为了保证速度，我们可以使用 wget 命令替代他。如果你决定使用 wget 命令，你可以选择跳过这一步，具体的使用方式在第五步呈现。 1(base) ➜ ~ pip install huggingface_hub 或者是直接下载 modelscope 库，使用 modelscope 下载模型。 1(base) ➜ ~ pip install modelscope 创建一个 LocalGit 文件夹，并进入该文件夹 123(base) ➜ ~ mkdir LocalGit(base) ➜ ~ cd LocalGit(base) ➜ LocalGit 克隆 llama.cpp 的仓库 123(base) ➜ LocalGit git clone https://github.com/ggerganov/llama.cpp(base) ➜ LocalGit cd llama.cpp(base) ➜ llama.cpp git:(master) 在有 GPU 的环境下编译 llama.cpp 前置条件：安装 nvcc + cmake 执行代码进行编译： 1(base) ➜ llama.cpp git:(master) make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc 如果出现错误：(base) ➜ llama.cpp git:(master) make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc Makefile:76: *** LLAMA_CUBLAS is removed. Use GGML_CUDA instead.. Stop. 修改代码如下： 1(base) ➜ llama.cpp git:(master) make GGML_CUDA=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc 下载相应的模型 ① 使用 Hugging Face 下载相应模型，实测服务器网速在 3M~6M 左右，具体方式如下： 1(base) ➜ ~ huggingface-cli download Qwen/Qwen1.5-32B-Chat-GGUF qwen1_5-32b-chat-q8_0.gguf --local-dir . --local-dir-use-symlinks False ② 使用 wget 下载 modelscope 的模型文件，实测网速在 10M~22M 左右，这需要你先获取到模型的下载链接，具体方式如下： 1(base) ➜ ~ wget https://www.modelscope.cn/models/qwen/Qwen1.5-32B-Chat-GGUF/resolve/master/qwen1_5-32b-chat-q8_0.gguf ③ 直接使用 modelscope 库下载模型，实测网速在 18M~65M 左右，具体方式如下： 1modelscope download --model=qwen/Qwen2-7B-Instruct-GGUF --local_dir . qwen2-7b-instruct-q8_0.gguf 使用 llama.cpp 的相关命令进行操作 1(base) ➜ llama.cpp git:(master) ./main -m ../models/qwen1_5-32b-chat-q8_0.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt 1(base) ➜ llama.cpp git:(master) ./llama-server -m ../models/qwen1_5-32b-chat-q8_0.gguf -ngl 80 -fa 兼容 OpenAI API，使用 Python 代码测试 123456789101112131415import openaiclient = openai.OpenAI( base_url=&quot;http://localhost:8080/v1&quot;, # &quot;http://&lt;Your api-server IP&gt;:port&quot; api_key = &quot;sk-no-key-required&quot;)completion = client.chat.completions.create( model=&quot;qwen&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;tell me something about michael jordan&quot;} ])print(completion.choices[0].message.content) 命令启动 12345./llama-cli -m qwen2-7b-instruct-q5_k_m.gguf \\ -n 512 -co -i -if -f prompts/chat-with-qwen.txt \\ --in-prefix &quot;&lt;|im_start|&gt;user\\n&quot; \\ --in-suffix &quot;&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&quot; \\ -ngl 24 -fa 拓展补充Llama.cpp大模型量化简明手册_llamacpp量化-CSDN博客","link":"/2024/08/13/Deploy-LLMs-ON-Linux/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"软件源","slug":"软件源","link":"/tags/%E8%BD%AF%E4%BB%B6%E6%BA%90/"},{"name":"开发工具","slug":"开发工具","link":"/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"Axure RP 9","slug":"Axure-RP-9","link":"/tags/Axure-RP-9/"},{"name":"闲聊","slug":"闲聊","link":"/tags/%E9%97%B2%E8%81%8A/"},{"name":"计划","slug":"计划","link":"/tags/%E8%AE%A1%E5%88%92/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"VSCode","slug":"VSCode","link":"/tags/VSCode/"},{"name":"大数据技术","slug":"大数据技术","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"ECharts","slug":"ECharts","link":"/tags/ECharts/"},{"name":"Tomcat","slug":"Tomcat","link":"/tags/Tomcat/"},{"name":"CppDev","slug":"CppDev","link":"/tags/CppDev/"},{"name":"QT6","slug":"QT6","link":"/tags/QT6/"},{"name":"项目","slug":"项目","link":"/tags/%E9%A1%B9%E7%9B%AE/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"EasyX","slug":"EasyX","link":"/tags/EasyX/"},{"name":"GameDev","slug":"GameDev","link":"/tags/GameDev/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"多线程编程","slug":"多线程编程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"},{"name":"JavaDev","slug":"JavaDev","link":"/tags/JavaDev/"},{"name":"MyBatis-Plus","slug":"MyBatis-Plus","link":"/tags/MyBatis-Plus/"},{"name":"LLMs","slug":"LLMs","link":"/tags/LLMs/"},{"name":"LM-Studio","slug":"LM-Studio","link":"/tags/LM-Studio/"},{"name":"PowerInfer","slug":"PowerInfer","link":"/tags/PowerInfer/"},{"name":"语法点","slug":"语法点","link":"/tags/%E8%AF%AD%E6%B3%95%E7%82%B9/"},{"name":"C++ 11 新标准","slug":"C-11-新标准","link":"/tags/C-11-%E6%96%B0%E6%A0%87%E5%87%86/"},{"name":"网络编程","slug":"网络编程","link":"/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"闲聊","slug":"闲聊","link":"/categories/%E9%97%B2%E8%81%8A/"},{"name":"软件源","slug":"Linux/软件源","link":"/categories/Linux/%E8%BD%AF%E4%BB%B6%E6%BA%90/"},{"name":"Axure RP 9","slug":"开发工具/Axure-RP-9","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Axure-RP-9/"},{"name":"计划","slug":"闲聊/计划","link":"/categories/%E9%97%B2%E8%81%8A/%E8%AE%A1%E5%88%92/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Git","slug":"开发工具/Git","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Git/"},{"name":"VSCode","slug":"开发工具/VSCode","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/VSCode/"},{"name":"PyTorch","slug":"深度学习/PyTorch","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/"},{"name":"大数据技术","slug":"大数据技术","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"HBase","slug":"大数据技术/HBase","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/HBase/"},{"name":"数据结构&#x2F;算法","slug":"数据结构-算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/"},{"name":"哈希(Hash)","slug":"数据结构-算法/哈希-Hash","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E5%93%88%E5%B8%8C-Hash/"},{"name":"栈(Stack)","slug":"数据结构-算法/栈-Stack","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%88-Stack/"},{"name":"树(Tree)","slug":"数据结构-算法/树-Tree","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%91-Tree/"},{"name":"Kaggle","slug":"深度学习/Kaggle","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Kaggle/"},{"name":"ECharts","slug":"大数据技术/ECharts","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/ECharts/"},{"name":"CppDev","slug":"CppDev","link":"/categories/CppDev/"},{"name":"QT6","slug":"CppDev/QT6","link":"/categories/CppDev/QT6/"},{"name":"项目","slug":"大数据技术/项目","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/"},{"name":"Scala","slug":"大数据技术/Scala","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Scala/"},{"name":"Spark","slug":"大数据技术/Spark","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Spark/"},{"name":"Gaming","slug":"CppDev/Gaming","link":"/categories/CppDev/Gaming/"},{"name":"数据可视化","slug":"大数据技术/数据可视化","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"Docker","slug":"开发工具/Docker","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Docker/"},{"name":"多线程编程","slug":"CppDev/多线程编程","link":"/categories/CppDev/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"},{"name":"LLMs","slug":"深度学习/LLMs","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLMs/"},{"name":"语法点","slug":"CppDev/语法点","link":"/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/"},{"name":"bind","slug":"CppDev/语法点/bind","link":"/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/bind/"},{"name":"enum class","slug":"CppDev/语法点/enum-class","link":"/categories/CppDev/%E8%AF%AD%E6%B3%95%E7%82%B9/enum-class/"},{"name":"网络编程","slug":"CppDev/网络编程","link":"/categories/CppDev/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"pages":[]}