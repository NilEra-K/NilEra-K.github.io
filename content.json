{"posts":[{"title":"CentOS7更换阿里&#x2F;清华源","text":"Blog网址：https://www.cnblogs.com/wswind/p/10173591.html","link":"/2024/04/23/CentOS7%E6%9B%B4%E6%8D%A2%E9%98%BF%E9%87%8C-%E6%B8%85%E5%8D%8E%E6%BA%90/"},{"title":"Axure RP 9 产品设计入门","text":"🚪Axure RP 9 产品设计入门Axure RP 9 软件的安装可以直接官网下载，然后去某宝买一个账号的激活码，详情不再赘述，可以自行 Bing。下载网址：https://www.axure.com/release-history/rp9学习参考视频：https://www.bilibili.com/video/BV1hU4y1L77u/ Axure 软件的一些基础设置 偏好设置 文件 → Preferences（首选项、偏好设置） 网格设置 视图设置 → 标尺·网格·辅助线 → 网格设置 我个人认为舒服的设置：网格对齐，间距 10 像素，样式更改为线段，颜色 #F2F2F2 如何预览、发给别人可以浏览的文件发布 → 生成 HTML 文件（选择一个文件夹，会生成很多文件，因此最好选择一个独立的文件夹） 常用的设计尺寸手机端：365*667 开始设计绘制一个简单的UI界面 用矩形绘制一个背景：使用线性方式填充 用水平线绘制隔断 用矩形绘制按钮 如何绘制按钮使用矩形绘制一个按钮 绘制矩形 右键矩形 → 交互样式 美化输入框 绘制矩形 Ctrl + D 复制一份，调整大小 添加输入框和一个搜索图标 母版选择绘制好的组件，右键选择转换为母版 单选框的使用和自定义单选框（美化操作）使用单选框时，需要对单选框进行编组。编组方式：框选对应组的单选框，右键选择指定单选按钮的组，然后命名组名即可。自定义单选框操作步骤： 使用圆形绘制一个大圆以及一个小圆 内部的小圆的线条和填充设置为透明 右键添加交互样式，设置当选中时，内部小圆的填充和线条更改颜色，外部大圆线条更改颜色 添加文字后进行编组，对于编组内容设置交互（和交互样式不同），在菜单栏的右侧，交互面板 效果如下： 常用工具页面工具 页面工具中可以创建多个页面 页面工具中可以按住 Ctrl + 左右方向键 来调整页面的级别 页面工具中可以按住 Ctrl + 上下方向键 来调整页面的顺序 概要工具 概要工具可以看到整个页面全部的元件 当存在遮罩关系时，可以选择概要工具中需要修改的元件，不需要破坏遮罩关系 可以通过筛选按钮来对元件进行筛选 建议给元件起好名字 钢笔工具 世上无难事，只怕有心人 使用钢笔工具来绘制自定义图案 绘制复杂的UI界面 常用的交互事件及操作页面跳转 页面跳转原理：事件 → 动作 → 目标 对于跳转来说：事件（点击） → 动作（跳转） → 目标（页面2） 事件的选择在“交互”面板处（注：不是交互样式） 实现时点击右侧交互面板新建交互，选择单击时，动作设置为打开链接，目标设置为要切换到的界面 当点击预览时候，会发现元件的显示往往出现在屏幕中心位置，如果想要让元件按照画布的绝对位置显示，可以点击页面，选择样式中的“页面排列”，在其中调整相关显示样式。 热区 为什么使用热区？ 对于图片中的情况，如果我们想点击按钮完成跳转操作，可以直接给使用方式 ① 按钮添加一个交互事件。 但是当我们想要点击这个区域时进行跳转，一个元件一个元件添加事件过于繁琐（技巧：可以通过 Ctrl + C 在右侧交互面板直接复制整个事件，然后点击想要添加该事件的元件，通过 Ctrl + V 粘贴到元件上），而且不方便后续的修改。 于是可以使用方式 ② 将要整个部分编组，然后为整个组添加事件。但是当我们因为某种原因破坏了整个组时，可能会遗忘添加事件，导致整个事件消失。 所以热区就是为了解决上面的问题，我们可以通过方式 ③ 给组件添加热区，只要点击这个区域即可完成指定事件。 且热区占用内存更小。 显示和隐藏设置文字设置某元件上的文字 设置选中设置一个元件为选中状态 启用或禁用 启用/禁用是针对于一个元件 常用场景是：当满足某条件时启用/禁用某元件 例如：当用户输入账号和密码后启用登录按钮（瞎编的），当用户同意协议才可进行下一步这种 移动 移动一个元件，可以在“移动”处设置经过（相对位置）、到达（绝对位置）来移动元件 等待 有时候等待看起来并不生效，需要观察是否是等待动作前的动作设置了动画。 例如：移动动作设置了 500ms 的动画，其实移动动作是瞬间完成的，动画只是一种呈现的方式。所以动画呈现的效果和等待是几乎同时进行的（“移动”动作设置 500ms 的动画和 500ms 的“等待”，动画开始播放时“等待”同时进行，动画结束播放时，“等待”结束）。 旋转设置尺寸设置透明度制作更复杂的动效 Axure RP 9 的动画效果是可以叠加的。 等待可以分割两个本来融合的叠加效果。 使用该上图所示的操作，会导致“移动”和“旋转”的融合无法叠加，先执行“移动”，再执行“旋转”。 文本框的长度是可以固定或者跟随的，当文本框样式如下时，则说明文本框此时的长度是固定的。此时可以双击两侧中间的锚点来使文本框变为跟随模式。下图为固定长度：下图为跟随长度：双击角上的锚点可以设置是否跟随高度、宽度（若不跟随，则此时文本框高度和宽度完全固定）。 复杂动效总体流程： 滚动到元件想要动画展示，一般设置动画的时间为 500ms，但是对于用户来说，500ms 看起来有些卡顿，所以 350ms 往往是个不错的选择 更多事件 事件 条件 备注 单击时 点击形状时 一般单击事件不能和“鼠标按下”/“鼠标松开” 事件同时使用 双击时 双击形状时 鼠标右击时 形状的上下文菜单被触发时（右键点击形状时候） 鼠标按下时 鼠标在形状上按下时 鼠标松开时 鼠标在形状上松开时 鼠标移动时 鼠标移动到形状上时 鼠标移入时 鼠标指针进入形状元件区域中时 鼠标移出时 鼠标指针离开形状元件区域中时 鼠标停放时 鼠标指针在停放在形状上超过 1s 时 鼠标长按时 鼠标指针在按压在形状上超过 1s 时 按键按下时 当焦点在形状上并按下键盘上的任意按键时 按键松开时 当焦点在形状上并按下键盘上的任意按键松开时 移动时 当形状发生移动时 旋转时 当形状发生旋转时 尺寸改变时 当形状尺寸发生改变时 显示时 当形状发生显示时 隐藏 当形状发生被隐藏时 获取焦点时 当形状获取焦点时 失去焦点时 当形状失去焦点时 选中 当形状被选中时 取消选中时 当形状取消选中时 选中改变时 当形状的选中状态发生改变时 载入时 形状已加载时（首次显示页面时） 更多动作内部框架 若概念不明确，往往会将内部框架和快照混淆。 快照往往用于在一个页面预览所有的页面，点击快照后跳转到快照包含的页面，并且在跳转后的页面进行修改。 而内部框架和快照不同，内部框架相当于将需要包含的页面复制了一份，粘贴到了内部框架中，因此可以直接在内部框架中修改内容，修改内部框架中的内容不会影响原页面内容。 动态面板 动态面板也是一个元件，也可以显示或隐藏。 我们可以将动态面板形象的理解为一个盒子。在盒子里没有东西时，动态面板“无色无味”，看不见摸不到，双击后可以键入盒子内部。 盒子内部也可以添加很多层： 我们可以通过一个按钮，添加“设置动态面板”动作来切换层，因此我们可以想到动态面板常用于实现轮播图（当然还有很多其他的玩法） 动态面板案例 1：遮罩使用动态面板可以实现遮罩的一些操作，如： 超出指定范围的元件可以使用动态面板，隐藏超出部分，右键直接将元件转换为动态面板即可： 将元件一直固定再某一位置选择固定到浏览器，并且进一步选择固定位置，当浏览器页面过长需要滚动时，动态面板的位置不会发生改变。 通栏选择 “100%宽度（仅浏览器中有效）”，即可将动态面板的宽度按照浏览器的宽度进行设置，也就是所谓的通栏（Banner，Banner也可以用来表示轮播图） 动态面板是可以嵌套的 动态面板案例 2：浮窗 动态面板案例 3：弹窗 动态面板案例 4：轮播 动态面板案例 5：Tab切页 动态面板案例 6：列表切页 动态面板案例 7：拖动 使用动态面板实现拖动的难点在于使用“动态面板的嵌套”和“设置边界” 使用动态面板的嵌套是为了防止直接拖动最外层的动态面板，而是拖动动态面板内部的一个面板。 使用边界的难点在于 Axure RP 9 在设计这个功能时的让人难以理解。这里简单的理解是： 对于“左侧”，$-35 &lt;= x &lt;= 0$ 对于“左侧”可以移动 0 像素或移动 -35 像素 对于“左侧”，可以不移动，或者向左移动 35 像素 用不熟练的时候多尝试 中继器 添加局部变量的方式： 首先点击添加局部变量 变量全局变量可以跨页面传参 判断 添加判断当添加交互事件时，不进一步选择动作，而是选择“启用情形”，在启用情形中添加判断。 局部变量获取元件位置，结合判断可以用 [[元件局部变量.x]] 获取元件的 x 位置 多条件判断多条件判断可以通过在“添加情形”部分添加行来增加判断条件，“匹配所有”和“匹配任何”两个情况，分别代表的是 and 和 or。 嵌套判断 案例：绘制注册 中间页","link":"/2024/04/12/Axure-RP-9-%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1%E5%85%A5%E9%97%A8/"},{"title":"[20240412] 下一步计划","text":"[Date: 20240412] 下一步计划[TOC] 📕 计划 1: 学习 Axure RP 9[*注] 2024.04.20 该计划已经完成主要是为了计划2做准备 📕 计划 2: 开发计划开发一个好玩的小工具这个其实是一个奇思妙想啦，目前还不知道能不能实现，总的来说就是希望用户上传一张文件，然后我们转化成命令行图片。就例如我用的网站博客的框架，他能用符号绘制一个很好看终端欢迎界面，我觉得是一个很有意思的东西，所以我准备在接下来的一年时间里实现这个小工具。目前连文件夹都没有创建，哈哈哈 :P 12345678INFO ======================================= ██╗ ██████╗ █████╗ ██████╗ ██╗ ██╗███████╗ ██║██╔════╝██╔══██╗██╔══██╗██║ ██║██╔════╝ ██║██║ ███████║██████╔╝██║ ██║███████╗ ██║██║ ██╔══██║██╔══██╗██║ ██║╚════██║ ██║╚██████╗██║ ██║██║ ██║╚██████╔╝███████║ ╚═╝ ╚═════╝╚═╝ ╚═╝╚═╝ ╚═╝ ╚═════╝ ╚══════╝============================================= 使用 QT 6 开发一个好玩的桌面端应用刷抖音的时候看到了，一些很好玩的评论，因为大家好像觉得小红书是一个给女孩子交流的平台，所以男孩子也希望有一个自己交流的平台，大概就是小蓝书的样子。这也是个很大的项目了，需要很长时间设计，不过最起码要等我学完Axure RP 9 吧，当然肯定不是全部实现，我就是想边学边开发，能做多少是多少的样子。 📕 计划 3: 读书📕 计划 4: 复习数据结构与算法复习的列表会开一篇新博客来归档","link":"/2024/04/12/20240412-%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AE%A1%E5%88%92/"},{"title":"PyTorch快速上手指南","text":"PyTorch 深度学习框架快速上手指南PyTorch 可以说是目前最常用的深度学习框架 , 常应用于搭建深度学习网络 , 完成一些深度学习任务 (CV、NLP领域) 要想快速上手 PyTorch , 你需要知道什么 : 一个项目的完整流程 , 即到什么点该干什么事 几个常用 (或者说必备的) 组件 剩下的时间你就需要了解 , 完成什么任务 , 需要什么网络 , 而且需要用大量的时间去做这件事情 $^{(e.g.)}$例如 : 你现在有一个图像分类任务 , 完成该任务需要什么网络, 你需要通过查找资料来了解需要查找什么网络。 需要注意的是 , 有一些常识性的问题你必须知道 , 例如: 图像层面无法或很难使用机器学习方法 , 卷积神经网络最多的是应用于图像领域等 下面我将通过一个具体的分类项目流程来讲述到什么点该干什么事一个完整的 PyTorch 分类项目需要以下几个方面: 准备数据集 加载数据集 使用变换(Transforms模块) 构建模型 训练模型 + 验证模型 推理模型 准备数据集 一般来说 , 比赛会给出你数据集, 不同数据集的组织方式不同 , 我们要想办法把他构造成我们期待的样子 分类数据集一般比较简单, 一般是将某个分类的文件全都放在一个文件夹中, 例如: 二分类问题 : Fake(文件夹) / Real(文件夹) 多分类问题 : 分类 1(文件夹) / 分类 2(文件夹) / … / 分类 N(文件夹) 当然有些时候他们会给出其他方式 , 如 UBC-OCEAN , 他们将所有的图片放在一个文件夹中 , 并用 csv 文件存储这些文件的路径(或者是文件名) , 然后在 csv 文件中进行标注(如下): 以后你可能还会遇到更复杂的目标检测的数据集, 这种数据集会有一些固定格式 , 如 VOC格式 , COCO格式等 在数据集方面 , 需要明确三个概念——训练集、验证集和测试集 , 请务必明确这三个概念 , 这是基本中的基本 训练集(Train) : 字如其名 , 简单来说就是知道数据 , 也知道标签 的数据 , 我们用其进行训练 验证集(Valid) : 验证集 和 测试集 是非常容易混淆的概念 , 简单来说 , 验证集就是我们也知道数据和标签 , 但是我们的一般不将这些数据用于训练 , 而是将他们当作我们的测试集 , 即我们已经站在了出题人的角度 , 给出参赛者输入数据 , 而我们知道这个数据对应的输出 , 但是我们不让模型知道 测试集(Test) : 测试集就是 , 我们不知道输入数据的输出标签 , 只有真正的出题人知道 , 一般来说 , 我们无法拿到测试集 , 测试集是由出题人掌控的 需要注意的是 , 如果你通过某种途径知道了所有的测试集的标签时 , 不可使用测试集进行训练 , 这是非常严重的学术不端行为 , 会被学术界和工业界唾弃 1234567891011# 现在我们已经有了一个数据集 , 我将以 FAKE_OR_REAL 数据集为例 , 展示我们数据集的结构# D:\\REAL_OR_FAKE\\DATASET# ├─test --------- 测试集路径, 这里可以放你自己的数据, 你甚至可以将他们分类, 但是请注意, 实际情况下你只能通过这种方式来“得到”测试集# │ ├─fake ------ 你自己分的类, 开心就好# │ └─real ------ 同上# ├─train -------- 训练集路径, 这里面放的是题目给出的数据, 下面有 fake 和 real 两个文件夹, 这两个文件夹中就是两个类别, 我们要用这里面的图片进行分类 # │ ├─fake# │ └─real# └─valid -------- 验证集路径, 这里面放的是题目给出的数据, 下面有 fake 和 real 两个文件夹, 这两个文件夹中就是两个类别, 这里面的图片不需要进行训练# ├─fake# └─real 加载数据集 请务必记住 , 不管是什么数据集 , 数据集是如何构成的 , 在使用 PyTorch 框架时 , 我们都要像尽办法将他们加载入 Dataset 类中 简单来说 , Dataset 类就是描述了我们数据的组成的类 需要注意 , PyTorch 实现了许多自己的 Dataset 类 , 这些类可以轻松的加载特定格式的数据集 , 但是我强烈建议所有的数据集都要自己继承Dataset类 , 自行加载 , 这样我们可以跟清晰的指导数据集的组成方式 , 也可以使得我们加载任意格式的数据集 实现 DataSet 类需要我们先继承 Dataset 类 , 在继承 Dataset 类后, 我们只需要实现其中的__init__、__len__和__getitem__三个方法 , 即可完成对数据集的加载 , 这三个方法就和他的名字一样 : __init__ 方法是构造函数 , 用于初始化 __len__ 方法用于获取数据集的大小 __getitem__ 方法用于获取数据集的元素 , 我将从下面的代码中进行更详细的解释 有些数据集并不分别提供 Train训练集 和 Valid验证集, 我们可以使用 random_split() 方法对数据集进行划分 需要注意的是, 每次重新划分数据集时, 必须重新训练模型, 因为 random_split() 方法随机性, 划分后的数据不可能和之前的数据完全重合, 因此会导致数据交叉的情况, 下面一段使用 random_split() 进行划分的 Python 代码示例 : 12345678# 下面演示使用 random_split 来划分数据集的操作# 我们假设已经定义了 CustomImageDataSetsplit_ratio = 0.8 # 表示划分比例为 8 : 2dataset = CustomImageDataSet(fake_dir, real_dir) # 定义 CustomImageDataSet 类, 假设此时没有划分训练集和验证集train_dataset_num = int(dataset.lens * split_ratio) # 定义训练集的大小valid_dataset_num = dataset.lens - train_dataset_num # 定义验证集的大小# random_split(dataset, [train_dataset_num, valid_dataset_num]) 表示将 dataset 按照 [train_dataset_num: valid_dataset_num] 的比例进行划分train_dataset, valid_dataset = random_split(dataset, [train_dataset_num, valid_dataset_num]) 当数据集不是很大的时, 推荐人为的将数据集进行划分, 可以写一个 Python 脚本(.py) 或者 批处理脚本(.bat) 来完成这个操作 完整的数据集加载代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torchfrom torch.utils.data import Datasetimport osfrom PIL import Image# 这里我们定义了一个 CustomImageDataset(...) 类, 括号中的内容表示我们继承了 ... 类# 因此我们这里 CustomImageDataset(Dataset): 表示我们定义了一个“自定义图片”类, 这个“自定义图片”类继承自 Dataset 类class CustomImageDataset(Dataset): # 这里我们实现 __init__ 方法, __init__ 方法其实就是一个类的构造函数, 他也分有参构造和无参构造, 只是在这里我们说无参构造基本没啥意义 # 因此我们常常实现这个类, 使得可以指定这个类的输入输出 # 比如下面我们写的 def __init__(self, fake_dir, real_dir, transform=None): # self : 自己, 我一般直接理解为 this 指针, 如果有兴趣了解更深层的东西可以查阅一些资料, 这个是必填的 # fake_dir : 用于指定 fake 类型图片的位置的 # real_dir : 用于指定 real 类型图片的位置的 # transform : 用于指定变换, 简单来说就是对输入进行某些操作, 我会在下面的板块中进行详细叙述 def __init__(self, fake_dir, real_dir, transform=None): self.fake_dir = fake_dir # 这里表示这个类内定义了一个 fake_dir, 其值为传入的 fake_dir self.real_dir = real_dir # 这里表示这个类内定义了一个 real_dir, 其值为传入的 real_dir self.transform = transform # 这里表示这个类内定义了一个 transform, 其值为传入的 transform, 当没有传入时, 这个变量为 None self.fake_images = os.listdir(fake_dir) # 传入的 fake_dir 是一个路径, 我们使用 os.listdir(fake_dir) 可以加载 fake_dir 文件夹下的内容, 也就是所有 fake 图片 self.real_images = os.listdir(real_dir) # 传入的 real_dir 是一个路径, 我们使用 os.listdir(real_dir) 可以加载 real_dir 文件夹下的内容, 也就是所有 real 图片 self.total_images = self.fake_images + self.real_images # 总图片列表, 就是将 fake 图片列表和 real 图片列表进行组合 self.labels = [0]*len(self.fake_images) + [1]*len(self.real_images) # 对图片打标签, fake 为 0, real 为 1 # [0] * 10 得到的结果为 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # [1] * 10 得到的结果为 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # 这里我们实现 __len__ 方法, 这个方法用于获取数据集的大小 def __len__(self): return len(self.total_images) # 这里我们直接返回总图片列表的长度即可, 这里的实现方式不唯一, 只要能做到表示数据集大小即可 # 这里我们实现 __getitem__ 方法, 这个方法用于获取数据集中的某个元素 # 其中 idx 表示索引, 这个参数是必须的, 当然可以起其他名字, 不过最好还是使用 idx # __getitem__(self, idx) 表示获取 idx 位置的元素 def __getitem__(self, idx): # 这里表示获取一个元素的逻辑 # 当 idx 位置的标签为 0 时, 图片的路径为 fake_dir + self.total_images[idx], idx 即为图片的索引位置 # 当 idx 位置的标签为 1 时, 图片的路径为 real_dir + self.total_images[idx] image_path = os.path.join(self.fake_dir if self.labels[idx] == 0 else self.real_dir, self.total_images[idx]) # 使用 PIL 库加载图片, 通过 image_path 打开图片, 并且将图片转化为 RGB 格式 image = Image.open(image_path).convert('RGB') # 这里是 transform, 表示变换, 当其值为 None 时不进行操作, 当传入自己的 transform 时即为非空, 即对输入数据进行变换 if self.transform: # 我们将变换后的图片直接保存在原位置 image = self.transform(image) # 最后函数的返回值为 image 和 self.labels[idx], 即表示索引位置 idx 处的图片和标签 return image, self.labels[idx] 使用 Transforms 不要简单的使用原始图片进行训练 , 当然如果一定要使用原始图片进行训练, 也可以使用 transforms 模块 一般来说, 训练集和验证集的 transforms 是不同的, 因为我们希望验证集和测试集的图片贴合真实的情况 下面的代码演示了如何定义 transforms 在定义完 transforms 我们就可以完全定义我们的 Dataset 和 Dataloader 了 123456789101112131415161718192021222324252627282930import torchfrom torchvision import transforms# 定义transform# transforms.Compose(transforms) 实际上就是将多个 transform 方法变为逐步执行, 一般我们直接使用这种方式来对图片进行连续的变换train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(), # 随机水平翻转 transforms.RandomVerticalFlip(), # 随机垂直翻转 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # 改变图像的属性, 将图像的brightness亮度/contrast对比度/saturation饱和度/hue色相 随机变化为原图亮度的 10% transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # 对图片先进行随机采集, 然后对裁剪得到的图像缩放为同一大小, 意义是即使只是该物体的一部分, 我们也认为这是该类物体 transforms.RandomRotation(40), # 在[-40, 40]范围内随机旋转 transforms.RandomAffine(degrees=0, shear=10, scale=(0.8,1.2)), # 随机仿射变换 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 色彩抖动 transforms.ToTensor(), # [重点] 将图片转化为 Tensor 张量, 在 PyTorch 中, 一切的运算都基于张量, 请一定将你的输入数据转化为张量 # 请理解什么是张量 : 我们在线性代数中有向量的概念, 简单来说就是张量就是向量, 只不过张量往往具有更高的维度 # 而大家一般习惯将高于三维的向量称为张量, 某些人(比如我)也习惯所有的向量统称为张量 # 可以简单的将数组的维数来界定张量的维度 # 例如 [ ] 为一维张量, [[ ]] 为二维张量, [[[ ]]]为三维张量, [[[[ ]]]]为四维张量 # 对于图像来说, jpg 图像实际为三维矩阵, png 图像实际为四维矩阵, 这个维数是根据图像的通道数进行划分的 # 例如 jpg 有 R、G、B三个通道, png 具有 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 归一化, 可以对矩阵进行归一化 # 详细查看这个Blog : https://blog.csdn.net/qq_38765642/article/details/109779370 transforms.RandomErasing() # 随机擦除])valid_transform = transforms.Compose([ transforms.Resize((256, 256)), # Resize 操作, 将图片转换到指定的大小 transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) 12345678910111213141516171819from torch.utils.data import DataLoader# 定义 Dataset 实例train_dataset = CustomImageDataset(fake_dir=&quot;./dataset/train/fake&quot;, real_dir=&quot;./dataset/train/real&quot;, transform=train_transform)valid_dataset = CustomImageDataset(fake_dir=&quot;./dataset/valid/fake&quot;, real_dir=&quot;./dataset/valid/real&quot;, transform=valid_transform)# 创建 DataLoader 实例# 这里将要涉及到超参数的概念, 什么是超参数: 简单来将, 超参数就是我们自己能指定的一些数据, 超参数的选择将很大程度上影响模型的性能# 因此 深度学习领域的工程师 常称自己为 炼丹师、调参师等batch_size = 32 # batch_size 就是一个超参数, batch 即为 “批次”, 表示一次使用 DataLoader 加载多少张图片进行运算 # 这个数值并不是越大越好, 也不是越小越好, 但是往往大一些比较好, 这个数字最大能选择多大和你的图片大小和显卡显存有很大的关系 # 当出现 [Out Of Memery] 错误时往往表明你选取了过大的 batch_size, 导致显卡出现了爆显存的问题# batch_size : 每次训练时，模型所看到的数据数量。它是决定训练速度和内存使用的重要参数。# shuffle : 是否在每个训练周期之前打乱数据集的顺序。这对于许多模型（如卷积神经网络）是很有帮助的，因为它可以帮助模型避免模式识别。# sampler : 定义如何从数据集中抽样。默认情况下，它使用随机采样。但你可以使用其他更复杂的采样策略，如学习率调度采样。# batch_sampler : 与sampler类似，但它在批处理级别上进行采样，而不是在整个数据集上。这对于内存使用效率更高的场景很有用。# num_workers : 定义了多少个工作进程用于数据的加载。这可以加快数据加载的速度，但需要注意内存的使用情况。train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) 1234567891011# 查看Dataloader数据# 为了了解Dataloader中的数据, 我们可以使用以下方法来查看:# 使用 Python 的 len() 函数 : 我们可以直接通过 len() 函数获取 Dataloader 的长度, 即数据集中数据块的数量# 使用 torch.utils.data.DataLoader.len() 方法 : 这个方法也会返回Dataloader的长度。# 使用 iter() 函数：Dataloader是一个可迭代对象，我们可以直接通过iter()函数对其进行迭代，以获取每个批次的数据。# 使用torchvision.utils.save_image()函数 : 如果我们正在处理的是图像数据集，那么可以使用这个函数来保存Dataloader中的图像数据。 len(train_loader) # 401len(valid_loader) # 100images, labels = next(iter(train_loader))print(images)print(labels) 构建模型 构建模型是比较重要的一部分, 一般来说做好数据集之后, 最重要的事情就是修改模型, 通过训练结果改进模型, 判断自己的模型的正确性, 这里就是整个你要用到的神经网络的部分 , 需要注意的是 , 这里指定什么输入 , 推理的时候就要指定什么输入 简单用几个符号说明一下就是: $^{Train} model (inputX, inputY, …)$ → $^{Valid} model (inputX, inputY, …)$ 如何确定输入是什么: 看 forward() 的输入是啥模型的输入就是啥 我下面展现了我复现的 ResNet50 , 用这种方式可以顺便教你如何复现网络结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140import torch.nn as nnfrom torch.nn import functional as F# 这里是对 ResNet50 的实现, 请对照论文来进行对照阅读# 定义 ResNet50Basic类, 这里并不是完整的模型, 而是模型的一个部分class ResNet50BasicBlock(nn.Module): def __init__(self, in_channel, outs, kernerl_size, stride, padding): # super(ResNet50BasicBlock, self).__init__() 这里是干什么的? # 1. 首先找到 ResNet50BasicBlock 的父类, 这里是 nn.Module # 2. 把类 ResNet50BasicBlock 的对象self转换为 nn.Module 的对象 # 3. &quot;被转换&quot;的 nn.Module 对象调用自己的 init 函数 # 简单理解一下就是 : 子类把父类的 __init__ 放到自己的 __init__ 当中, 这样子类就有了父类的 __init__ 的那些东西 super(ResNet50BasicBlock, self).__init__() # 这里只是定义部分, 在这里的定义并不一定会在推理过程中使用 self.conv1 = nn.Conv2d(in_channel, outs[0], kernel_size=kernerl_size[0], stride=stride[0], padding=padding[0]) self.bn1 = nn.BatchNorm2d(outs[0]) self.conv2 = nn.Conv2d(outs[0], outs[1], kernel_size=kernerl_size[1], stride=stride[0], padding=padding[1]) self.bn2 = nn.BatchNorm2d(outs[1]) self.conv3 = nn.Conv2d(outs[1], outs[2], kernel_size=kernerl_size[2], stride=stride[0], padding=padding[2]) self.bn3 = nn.BatchNorm2d(outs[2]) # 输入是啥看 forward(), 例如这里是 forward(self, x), 则表示输入是 x, 也就是一个 def forward(self, x): # nn.Conv2d 是卷积层, 请了解[1]什么是卷积层, 以及[2]卷积层是干啥用的, [3]卷积后会变成什么 # 卷积运算的目的是提取输入的不同特征, 第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级, 更多层的网路能从低级特征中迭代提取更复杂的特征 out = self.conv1(x) # [*] 什么是 ReLU, ReLU是激活函数, 请了解 [1]什么是激活函数, [2]为什么要使用激活函数 # [*] 什么是 Batch Normalization层, BN 层是批次归一化层 out = F.relu(self.bn1(out)) out = self.conv2(out) out = F.relu(self.bn2(out)) out = self.conv3(out) out = self.bn3(out) return F.relu(out + x)# 定义 ResNet50DownBlock类, 这里并不是完整的模型, 而是模型的一个部分class ResNet50DownBlock(nn.Module): def __init__(self, in_channel, outs, kernel_size, stride, padding): super(ResNet50DownBlock, self).__init__() self.conv1 = nn.Conv2d(in_channel, outs[0], kernel_size=kernel_size[0], stride=stride[0], padding=padding[0]) self.bn1 = nn.BatchNorm2d(outs[0]) self.conv2 = nn.Conv2d(outs[0], outs[1], kernel_size=kernel_size[1], stride=stride[1], padding=padding[1]) self.bn2 = nn.BatchNorm2d(outs[1]) self.conv3 = nn.Conv2d(outs[1], outs[2], kernel_size=kernel_size[2], stride=stride[2], padding=padding[2]) self.bn3 = nn.BatchNorm2d(outs[2]) self.extra = nn.Sequential( nn.Conv2d(in_channel, outs[2], kernel_size=1, stride=stride[3], padding=0), nn.BatchNorm2d(outs[2]) ) def forward(self, x): x_shortcut = self.extra(x) out = self.conv1(x) out = self.bn1(out) out = F.relu(out) out = self.conv2(out) out = self.bn2(out) out = F.relu(out) out = self.conv3(out) out = self.bn3(out) return F.relu(x_shortcut + out)class ResNet50(nn.Module): def __init__(self): super(ResNet50, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # Sequential 类是 torch.nn 模块中的一个容器, 可以将多个层封装在一个对象中, 方便顺序连接 self.layer1 = nn.Sequential( ResNet50DownBlock(64, outs=[64, 64, 256], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(256, outs=[64, 64, 256], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(256, outs=[64, 64, 256], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ) self.layer2 = nn.Sequential( ResNet50DownBlock(256, outs=[128, 128, 512], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50BasicBlock(512, outs=[128, 128, 512], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(512, outs=[128, 128, 512], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(512, outs=[128, 128, 512], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.layer3 = nn.Sequential( ResNet50DownBlock(512, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50BasicBlock(1024, outs=[256, 256, 1024], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(1024, outs=[256, 256, 1024], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.layer4 = nn.Sequential( ResNet50DownBlock(1024, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50DownBlock(2048, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(2048, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1, ceil_mode=False) # self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) self.fc = nn.Linear(2048, 10) # 使用卷积代替全连接 self.conv11 = nn.Conv2d(2048, 10, kernel_size=1, stride=1, padding=0) def forward(self, x): out = self.conv1(x) out = self.maxpool(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) # avgpool 平均池化层, 了解什么是平均池化层 out = self.avgpool(out) out = self.conv11(out) out = out.reshape(x.shape[0], -1) # out = self.fc(out) return out# 这里展现了对 ResNet 的一个具体的应用# x = torch.randn(1, 3, 224, 224) # 这个是我们 ResNet50 期待的输入样子, 可以看到他是 [1] 个 [3] 通道, 宽度为[224], 高度为 [224]的张量 image_path = './dataset/test/fake/test_fake_1.png'image = Image.open(image_path).convert('RGB') # 图片加载transform = transforms.ToTensor() # 将图片转化为张量, 此时的 张量的形状为[3, 1024, 1024]# 当输入数据的维度不足时, 我们可以通过 unsqueeze() 添加维度, 这个东西简单理解一下就是, 在某个维度外面加括号[], 即可拓展出更高的维度img_tensor = transform(image).unsqueeze(dim=0)# print(x.shape) 我们可以使用 shape 来查看一个张量的形状# print(img_tensor.shape)# 这里加载我们的网络架构net = ResNet50()# 这里进行输入, 输入 img_tensor, 进入 forword() 部分, 然后得到最终输出的结果out = net(img_tensor)print(out) 1234567891011121314151617181920212223242526import torchfrom torchvision import models# 这里为了方便, 我们直接加载 PyTorch 预训练好的 ResNet50 的模型# PyTorch 已经为我们提供了不少已经预训练好的模型, 我们只需要加载他们与训练好的模型即可# 但是我还是希望你可以掌握上面这种自定义模型的方法, 这样遇到 PyTorch 未提供的模型, 我们也可以尝试自己实现该模型model = models.resnet50(pretrained=True)# 冻结参数 : 即不更新模型的参数# 可以看到下面的代码, 这里表示冻结了所有层for param in model.parameters(): param.requires_grad = False# 但是我们可以通过替换层来接触某些层的冻结num_ftrs = model.fc.in_features # 这里是获取 ResNet50 的 fc 层的输入特征数model.fc = torch.nn.Linear(num_ftrs, 2) # 这里是对 fc 层进行修改, Linear(input_feather_num, output_feather_num) # 这里输入特征数是 num_ftrs, 输出特征数为 2 # 这一行很重要, 指定了模型的位置, cuda 可以理解为 GPU 设备, cuda: 0 表示使用编号为 0 的GPU进行训练# 当有多块 GPU 时, 可以用其他的方式指定 GPU# model = torch.nn.DataParallel(model, device_ids=[0, 1, 2]), 当然向我们这种小白(穷B), 当然还是单卡为主# 为了避免出现多卡的情况, 我在下面放入两篇博客, 有兴趣可以参考这两篇文章进行多卡训练# https://zhuanlan.zhihu.com/p/102697821# https://blog.csdn.net/qq_34243930/article/details/106695877device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model = model.to(device) 训练模型 + 验证模型 这里需要直接对模型进行训练 , 一般来说 , 在训练的过程中我们会加入 tqdm 库使得训练过程可视化 , 有时我们还会在训练过程中保存更好的训练结果 , 并且设置断点训练等操作 , 我只使用最简单的方式进行预测 train 部分的代码因人而异, 基本上每个人的写法都可能不同, 没有固定的写法 对于训练完的模型我们需要对其进行评价, 一般来说, 训练和验证都是放在一起的, 不可分开的 记得保存一下训练后的模型, 使用如下代码保存/加载整个模型123456# 保存模型model_path = &quot;xxxx.pth&quot; # xxxx 表示一个你喜欢的名字torch.save(model, model_path) # 使用 torch.save(model, model_path) 保存模型# 加载模型model = torch.load(model_path) # 使用 torch.load(model_path) 即可加载模型 完整的”训练模型 + 验证模型”代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from tqdm import tqdmfrom sklearn.metrics import f1_score, accuracy_score# 定义损失函数和优化器# 这里包含了 PyTorch 的 19 种损失函数 https://blog.csdn.net/qq_35988224/article/details/112911110criterion = torch.nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters())# 计算 F1 值和 准确率def evaluate(loader, model): preds = [] targets = [] loop = tqdm(loader, total=len(loader), leave=True) for images, labels in loop: images, labels = images.to(device), labels.to(device) with torch.no_grad(): outputs = model(images) _, predicted = torch.max(outputs, 1) preds.extend(predicted.cpu().numpy()) targets.extend(labels.cpu().numpy()) # Update the progress bar loop.set_description(&quot;Evaluating&quot;) return f1_score(targets, preds), accuracy_score(targets, preds)# 训练循环best_f1 = 0.0loss_values = []num_epochs = 10 # 定义训练的轮次for epoch in range(num_epochs): model.train() # 将模型设置为训练模式 loop = tqdm(train_loader, total=len(train_loader), leave=True) print(loop) for images, labels in loop: images, labels = images.to(device), labels.to(device) # 前向推理 outputs = model(images) loss = criterion(outputs, labels) # 反向传播及优化 # 在用 PyTorch训练模型时, 通常会在遍历 Epochs 的过程中依次用到 # optimizer.zero_grad() : 先将梯度归零 # loss.backward() : 反向传播计算得到每个参数的梯度值 # optimizer.step() : 通过梯度下降执行一步参数更新 # 对于这三个函数, 这篇博客写的很好 : https://blog.csdn.net/PanYHHH/article/details/107361827 # 可以简单阅读一遍 optimizer.zero_grad() loss.backward() optimizer.step() # 保存该批次的损失 loss_values.append(loss.item()) # 更新进度条 loop.set_description(f&quot;Epoch [{epoch + 1}/{num_epochs}]&quot;) loop.set_postfix(loss=loss.item()) # 在每轮之后验证模型 model.eval() # 将模型设置为推理模式, 此时模型中的参数不会进行更新, 即完全用于推理/验证 f1_value, accuracy = evaluate(valid_loader, model) print(f'F1 score: {f1_value:.4f}, Accuracy: {accuracy:.4f}') # 保存 F1 值最高的模型 if f1_value &gt; best_f1: best_f1 = f1_value # 这里和上面 Markdown 的保存方式不同, model.state_dict(), 表示模型的参数, 简单来说呢我们仅仅保存了模型的参数, 但是我们并没有保存模型的结构 # 上面 Markdown 的保存方式是即保存了整个模型的结构, 也保存了模型的参数 torch.save(model.state_dict(), 'best_model.pth')print('训练结束') 当然我们也可以使用绘图函数，来展示过程中的相关数据。 12345678910import matplotlib.pyplot as pltplt.figure(figsize=(12, 8))plt.plot(loss_values, label='Train Loss')plt.title('Loss values over epochs')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.grid(True)plt.show() 推理模型 很高兴, 如果你到这一步, 你的水平肯定已经有了质的飞跃, 这里已经是最后一步了, 结束这个部分, 你就要开始自己的探索之路了 推理模型很简单, 我在上面说过, 构造模型时指定什么输入 , 推理的时候就要指定什么输入, 这里就是对应的部分了 1234567891011121314151617181920212223242526272829303132from torchvision.transforms import ToTensor, Resize, Normalize# predict_by_file 表示推理一个文件, 我们需要传入文件路径以及模型def predict_by_file(file_path, model): # image = Image.open(file_path).convert('RGB') # 这里的 transform 有与没有都无所谓, 纯看心情 transform = transforms.Compose([ Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) image = transform(image) # 这里和上面一样, 表示在最外面加一层括号, 使 [3, 256, 256] 变为 [1, 3, 256, 256] image = image.unsqueeze(0).to(device) model.eval() with torch.no_grad(): outputs = model(image) # 模型推理 # torch.max(...) # input (Tensor) – 输入张量 # dim (int) – 指定的维度 _, predicted = torch.max(outputs, 1) # 返回指定维度的最大值, 其实这里只有一维 print(outputs) # tensor([[0.7360, 0.2668]], device='cuda:0') print(outputs.shape) # torch.Size([1, 2]) return &quot;Fake&quot; if predicted.item() == 0 else &quot;Real&quot;path = './dataset/test/real/test_real_7.jpg'print(predict_by_file(path, model))","link":"/2024/03/20/PyTorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"title":"Git常用指令","text":"Git 常用命令 初始化仓库:git init 查看当前仓库的状态git status 切换分支 切换到已有 branchName 分支git checkout branchName 创建新分支，创建的同时切换到该分支git checkout -b newBranch 查看所在目录的分支git branch -a 拉取请求git pull 上传到远程仓库(GitHub) 在 GitHub 配置 SSH_KEY 查看本地是否具有已存在 ssh_key : ll ~/.ssh 如果不存在则生成 ssh_key : ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot; 如果存在则复制 ssh_key 的内容 : cat ~/.ssh/id_rsa.pub 在 GitHub 上添加公钥 : Settings → SSH and GPG Keys → New SSH Key 验证是否成功 : ssh -T git@github.com 克隆远程仓库 : git clone &lt;SSH_Addr&gt; 添加远程仓库地址:git remote add origin &lt;git_addr&gt; 添加文件 git add &lt;files&gt; 添加注释 git commit -m &quot;f&quot; 推送请求 git push -u origin main","link":"/2024/03/26/Git%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/03/17/hello-world/"},{"title":"VSCode C++ 的安装和配置","text":"VSCode C++ 的安装和配置 下载 VSCodeVSCode的安装地址 : https://code.visualstudio.com/ 安装和配置 Remote SSH 连接远程的 Linux 服务器 在 VSCode 安装插件 Remote-SSH 点击 左下角→连接到主机→配置SSH主机→选择要更新的SSH的配置文件(C:\\Users\\UserName\\.ssh\\config) 配置文件123Host 服务器的 IP 地址HostName 服务器的 IP 地址User 用户名 连接远程服务器, 按照要求选择操作系统, 输入用户名和密码等即可(第一次连接会较慢) 配置免密登录 Windows 系统下查看是否具有 ssh-key : cd C:/Users/UserName/.ssh 如果没有 ssh-key 则生成 ssh-key : ssh-keygen -t rsa -b 4096 如果有 ssh-key 则上传至 Windows : scp id_rsa.pub UserName@IPAddr:FilePath 在 Linux 系统下进行签名: cat pubPath &gt;&gt; authorized_keys","link":"/2024/03/27/VSCode-C-%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"},{"title":"在 Hadoop 高可用的基础上搭建 HBase 高可用","text":"在 Hadoop 高可用的基础上搭建 HBase 高可用当Hadoop高可用搭建完成后，需要进一步再Hadoop高可用集群上搭建HBase高可用时，过程如下： 前提说明： 我的集群为三台机器，每台机器上都有ZooKeeper，使用用户名和主机名（Username@Hostname）分别如下： 123master@mastermaster@slaver01master@slaver02 首先保证 ZooKeeper 正常部署 1zkServer.sh start 需要保证Hadoop正常部署 12[master@master ~]$ start-dfs.sh[master@slaver01 ~]$ start-yarn.sh 解压HBase 配置环境变量 生效环境变量 修改配置文件 修改 hbase-site.xml 文件 123456789101112131415161718192021222324252627282930313233343536&lt;configuration&gt; &lt;!-- HBase数据在HDFS中的存放的路径 --&gt; &lt;!-- 这里 ns 是 Hadoop 的 nameservice的值, 指向的是一个高可用的通道 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase 的运行模式 --&gt; &lt;!-- false是单机模式, 若为 false, HBase 和 ZooKeeper 会运行在同一个 JVM 里面 --&gt; &lt;!-- true是分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slaver01,slaver02&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper快照的存储位置 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.6/data&lt;/value&gt; &lt;/property&gt; &lt;!-- V2.1版本，在伪分布式情况下, 设置为 false --&gt; &lt;!-- 当使用 hdfs 时, 设置为 true --&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 需要注意的是： 这里使用了ns这个高可用通道，因此需要将Hadoop的 core-site.xml 与 hdfs-site.xml移动到/opt/module/hbase-2.1.0/conf下 修改 regionserver 文件 123masterslaver01slaver02","link":"/2024/05/04/Hadoop2HBase/"},{"title":"Hash","text":"哈希(Hash)[TOC] 哈希的基本概念 哈希（Hash）在我的理解中是一种映射关系，例如，将学生映射到学号上、将口红的颜色映射到一个色号上。 哈希函数（Hash Function）就是将一种你想要查询的关键字，比如说姓名、手机号码、数字或者字符串等，映射成便于查找的东西(一般来说是一个数字)的函数。 一般而言，一个好的哈希函数可以帮我们将我们想要查找的东西，从一个较大集合映射到一个较小集合，然后我们可以将这个较小集合中的信息存储下来，例如存入一个数组，这个用于存储较小集合的数组就称之为哈希表。 一张格式如下的表： 学号 姓名 0001 张三 0002 李四 0003 王五 0004 赵六 … … 这张表就可以理解为一个姓名和学号的哈希表，我们通过学号就可以获得学号对应的人的姓名，即：学号[0001] -&gt; &quot;张三&quot;，反映到代码中，可以理解为一个一维数组通过下标直接访问。 而在一些加密工作中，可能需要将要简单的组合复杂化，比如密码组合，这时会有一种类似反向哈希（加密）的过程。 比较常见的哈希函数的例子：$$H(x) = x ; mod ; 11$$ 这个函数可以让我们将任意的整数映射到 0 ~ 10 的范围中 哈希的基本操作 定义哈希函数 1234567const int modnum = 11;int hashTable[modnum];// 定义哈希函数int hash(int x) { return x % modnum;} 在哈希表中插入元素 12345// 在哈希表中插入元素void insert(int x) { int addr = hash(x); hashTable[addr] = x;} 在哈希表中查找元素 12345// 在哈希表中查找元素bool isExist(int x) { int addr = hash(x); return hashTable[addr] == x;} 哈希表中解决冲突的方式不同关键字通过相同哈希函数计算出相同的哈希地址，该种现象称为哈希冲突或哈希碰撞，或者称为哈希冲突。哈希表在存放数据时有可能出现冲突的情况，以上文中的哈希函数为例我们分别向其中插入元素，如下：因为希望哈希表底层数组的容量小于实际要存储的关键字的数量，这就会导致一个问题：冲突的发生是必然的，我们能做的是尽量的降低冲突率。 冲突的解决方式一般有以下两种： 方式 1：顺延一种很好想到的解决方式是将哈希表中插入时产生冲突的元素向后顺延，直到找到一个空位置再进行插入。这种方式称为顺延，也有说法称之为线性探测。此时插入和查找的代码也要发生相应的改变，插入时需要我们需要找到一个空位置来执行插入操作；相对应的查找方式也要做出改变，当我们查询一个数时，也要查询哈希函数对应的位置，并依次比较连续的非空的哈希表中的值： 插入操作 1234567void insert(int x) { int addr = hash(x); while(hashTable[addr] NOT NULL) { // 当哈希表的插入位置不为空 addr = (addr + 1) % modnum; } hashTable[addr] = x;} 查找操作 12345678910void isExist(int x) { int addr = hash(x); while(hashTable[addr] NOT NULL) { // 当哈希表的查询位置不为空(查询一段连续的哈希表) if(hashTable[addr] == x) { // 如果查询到指定元素, 返回 true return true; } addr = (addr + 1) % modnum; } return false;} 可以定义多种解决冲突的顺延函数，即addr = (addr + 1) % modnum，实际使用中可以是每次$+k$，或者$+1^2$，$+2^2$，$+3^2$，…等。 但是这种顺延方式会存在一定的问题：插入时可能会出现多次冲突，当哈希表即将满的时候，插入操作的冲突可能会出现的更多，此时插入和查询操作都会变成一个非常耗时的操作。 方式 2：哈希链表我们可以通过链表的方式，来实现在一个位置放置多个元素的操作。在 C++ 的 STL 库中，我们可以使用 STL 库中提供的 vector 来简单的替代链表。通过这种方式，每次查找元素时，先找到对应的链表头，然后遍历这个位置的整张链表即可。 此时的哈希表的定义、插入操作和查询操作要发生相应的变化： 定义哈希函数 12345678910#include &lt;iostream&gt;#include &lt;vector&gt;const int modnum = 11;vector&lt;int&gt; hashTable[modnum];// 定义哈希函数int hash(int x) { return x % modnum;} 插入操作 1234void insert(int x) { int addr = hash(x); hashTable[addr].push_back(x);} 查询操作 12345678910111213void isExist(int x) { int addr = hash(x); int tableSize = hashTable[addr].size(); // 这里不使用 for(int i = 0; i &lt; hashTable[addr].size(); i++) 的写法, 而是首先计算出 hashTable[addr].size() // 因为 vector 的 size() 是一个比较耗时的操作, 他是通过将 vector 按照一个一个数出来的方式来进行计数的 // 在数据量小的时候可能并不明显, 当数据量大的时候可能就会出现较为严重的耗时问题 for(int i = 0; i &lt; tableSize; i++) { if(hashTable[addr][i] == x) { return true; } } return false;} 但是这种方式还是不能彻底解决我们的问题。对于插入操作来说，时间复杂度可以看作是$O(1)$，对于查询操作来说，时间复杂度和其冲突次数相关联。 哈希函数的设计面对上面的问题，设计好哈希函数才是解决问题的关键。哈希函数在设计的时候，一般要注意几个原则： 设计哈希函数时，我们需要尽可能让查询的值均匀地存储在哈希表中，或者说尽量分散再哈希表中。 在手搓哈希函数时，我们会要求 $H(x) = x ; mod; p$，其中的 $p$ 为素数 哈希函数中的对 $p$ 取摸的操作，会使得哈希值落在 $0 &lt;= value &lt;= p-1$ 的范围内，这个哈希表的长度 $p$，一般被称为哈希表的容量（Capacity）。 插入哈希表的元素总数除以哈希表的容量得到的数，称为负载因子，这里可以用 $\\alpha$ 表示，即：$$\\alpha = ElumNum \\div p$$ 当负载因子$\\alpha$达到一定程度时（一般认为是$0.7\\sim0.8$），则说明再对哈希表继续进行操作，就会面临大量冲突的情况，这时就要考虑增大哈希表的容量以避免出现更多的冲突。 哈希函数的冲突率和负载因子的关系一般如下： 字符串哈希字符串哈希是学习或者工作中经常遇到的一种操作，常用于比较两组字符串的内容等操作。通过比较两个字符串的哈希值就可以完成字符串的比较。当然这个操作也可以用于数组的哈希，因为字符串本质就是一个字符数组。$$s = s_1s_2s_3 \\dots s_n\\qquad s_i \\in a, b \\dots z$$ 一个字符串 $s$ 由 $n$ 个字符组成，每个字符 $s_i$ 属于 $a \\sim z$。其哈希函数为：$$\\begin{aligned}H(S)&amp;=(\\sum_{i=1}^{n} c_i × base^{n-i});mod ;p\\&amp;=(c_1 × base ^ {n-1} + c_2 × base ^ {n-2} + \\dots + c_{n-1} × base ^ {1}) ; mod ; p\\&amp;=base^{n-(n-1)}(base\\dots(base(base × c_1 + c_2)));mod ;p\\&amp;=base^{1}(base\\dots(base(base × c_1 + c_2)+c_3)+\\dots + c_n)\\end{aligned};mod ;p$$ 其中 $c_i$ 是一个和 $s_i$ 有关的数字，我们可以将字符映射到数字，例如：$a → 1$、$b → 2$ 等。这里不将 a 映射为 0，因为如果将 a 映射为 0，字符串 a 和 ab 的哈希值是相等的。$base$ 是一个可以自己指定的数字，其值一般是大于字符集中的字符数量（$c_i$的最大值）的素数，这里可以取 31，常见的选择是 9999971。$p$ 是一个素数，常见的选择是 101 或 137。 用代码实现为： 12345678int hash(char s[], int n) { int res = 0; for(int i = 0; i &lt; n; i++) { // 为什么是 res * base, 见上文的描述的公式推导 res = (res * base + (s[i] - 'a' + 1)) % p; } return res;} 当 $base$ 为 31，$p$ 为 101 时。当 $s$ 为 $a$ 时，hash(char s[], int n) 的值为 1。过程可以描述如下： 12[1] res = (0 * base + ('a' - 'a' + 1)) % p&gt;&gt;&gt; res = 1 当 $s$ 为 $ab$ 时，hash(char s[], int n) 的值为 1。过程可以描述如下： 1234[1] res = (0 * base + ('a' - 'a' + 1)) % p&gt;&gt;&gt; res = 1[2] res = (1 * base + ('b' - 'a' + 1)) % p&gt;&gt;&gt; res = 33 当我们定义好一个良好的哈希函数之后，因为哈希函数的值相等的概率比较小，当两个字符串的哈希值相同的时候，我们可以认为两个字符串也相同，从而避免使用按位比较，节约时间。 $base$ 为什么一般是一个大于字符集中的字符数量（$c_i$的最大值）的素数呢？当 $base$ 值过小时，此处假定为 $11$，会出现有可能两个字符运算完后还没有超过字母表的范围。如：字母表的范围为 $1 \\sim 26$，字符串 ba 运算完成后的结果为 $23$，两个字符串的运算结果小于 $26$，字符串w运算完成后的结果为 $23$，可以发现 ba 和 w 出现了冲突。 我们字符串的定义方式有一种好处，可以使得我们借助类似前缀和的思想，快速得到任意一段字符串 $s$ 的字串的哈希值。 我们可以使用一个数组 $a$ 来记录我们计算 hash(s) 的中间过程，即：1234a[1] = hash(c1)a[2] = hash(c1c2)...a[i] = hash(c1c2...ci) 那么字符串 $s$ 的任意子串 $s_{l, r} = s_ls_{l+1}\\dots s_r$ 的哈希值为：$$hash(s_{l, r}) = (a[r] - a[l-1] × base^{r - l + 1});mod;p$$ 双哈希冲突往往是不可避免的，纵使我们的哈希函数再好，也有可能会出现冲突的情况，那么我们如何尽可能避免这个问题呢？这时候，我们可以使用双哈希的方法来避免冲突。在上文字符串哈希中，我们的字符串哈希函数为： $$H(S) = (\\sum_{i=1}^{n} c_i × base^{n-i});mod ;p$$ 那么如何实现双哈希呢？我们可以选取两组 $base$ 和 $p$，然后使用这分别使用这两组 $base$ 和 $p$，分别来求取哈希值。如果这两组参数得到的哈希值都相同，我们则认为两个字符串相等。在竞赛中，最好书写双哈希，甚至三哈希，因为这样出错误的概率会呈指数级的降低。 STL中的哈希 unordered_map在 C++ 的STL库中，有一个非常好用的数据结构 unordered_map，可以帮助我们实现大多数哈希表的操作。 1unordered_map&lt;K, V&gt; hash_table; 其中 K 为我们想要存储的关键字信息，V 表示与他关联的一些信息。例如： 1unordered_map&lt;string, int&gt; hash_table; 这个表会以string作为K，int作为V进行存储，基于此我们可以很方便的实现一些功能。例如我们现在要保存一个班级上学生的年龄信息: 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;int query(string name) { if(hash_table.find(name) == hash_table.end()) { cout &lt;&lt; &quot;Cannot Find...&quot; &lt;&lt; endl; } return hash_table[name];}int main(void) { unordered_map&lt;string, int&gt; hash_table; hash_table[&quot;zhangsan&quot;] = 2000; hash_table[&quot;lisi&quot;] = 2001; query(&quot;zhangsan&quot;); return 0;}","link":"/2024/05/05/Hash/"},{"title":"栈(Stack)","text":"栈(Stack)栈的基本概念 栈是一种 先进后出(First in Last Out, FILO) 的数据结构，其类似于我们生活中的衣娄，衣服只能从最顶部放入，也只能从最顶部拿出，要想拿出中间的某件衣服，就需要将顶部的衣服全部拿出，再进行后续的操作。 衣娄对应到栈上，就是以下的概念： 栈(Stack) 是一种类似于衣娄的数据结构，我们可以向其内部存入或者取出数据 栈按照 先进后出 的原则存储数据，每次新进入的数据都会被放在最上面，越先进入的越靠下，越后进入的数据越靠上。 我们只能对最上面的数据进行操作 栈的两大元素：栈的大小和栈顶指针Top（该指针指向栈最顶部的位置） 栈的基本操作 新建栈(题目简单时可以用数组模拟栈) 插入数据 删除栈顶数据 查询栈顶数据 清空栈","link":"/2024/05/04/Stack/"},{"title":"复习数据结构&#x2F;算法清单","text":"复习数据结构清单有日期则表示在该日期已经复习完毕，或者表示复习后的最新一次更新 数据结构 链接 备用链接(Backup Link) 日期 栈(Stack) - - - 队列(Queue) - - - 链表(Link List) - - - 二叉树(Binary Tree) https://hello-nilera.com/2024/05/07/Tree/ https://blog.csdn.net/NilEra/article/details/138624929?spm=1001.2014.3001.5501 2024.05.09 哈希(Hash) - - - 算法复习清单 算法 链接 备用链接(Backup Link) 日期","link":"/2024/05/04/ReviewAlgorithm/"},{"title":"Heap","text":"堆（Heap）[TOC] 堆的基本概念二叉堆的基本概念和基本性质堆是一种树形结构，有二叉树就有二叉堆。 二叉堆总是一棵完全二叉树 堆中某个结点的值总是不大于或不小于其父结点的值 根节点的值为整个堆中的最小/最大值。 父节点中的值大于等于两个子节点中的值，根节点的值最大的堆称为大根堆。 父节点中的值小于等于两个子节点中的值，根节点的值最小的堆称为小根堆。 我们可以在堆中插入和删除元素，然后通过调整元素的位置来维护堆的性质。 堆的操作 堆的初始化在建立堆之前，需要初始化一些东西： 一个空数组，用于存储堆中的元素 一个记录堆中元素个数的变量123const int MAXSIZE = 10000;int len = 0;int heap[MAXSIZE + 1]; 堆中元素的插入堆在插入时，需要首先将插入元素放在数组末尾，然后插入元素不断的和其父节点比较，直到位置合适。下面是对小根堆插入过程的模拟：小根堆插入的代码如下： 12345678910111213141516void insert(int x) { heap[++len] = x; up(len);}void up(int k) { while(k &gt; 1 &amp;&amp; heap[k] &lt; heap[k/2]) { swap(heap[k], heap[k/2]); k /= 2; }}int main(void) { insert(x); return 0;} 大根堆和小根堆插入元素的方式基本相同，只需要改变大于/小于符号，插入操作的时间复杂度为 $O(log;n)$ 堆顶元素的删除堆最常用的功能就是维护最小/最大值。在使用小根堆时，我们经常会求得最小的数字，然后让它出堆，这时就要从堆中删除堆顶数据。这时除了堆顶为空，它的左子树堆和右子树堆仍满堆结构。为了操作简单，一般选择将堆尾部元素放到堆顶，然后将其逐步下移的方式，下移时，如进行交换操作，交换的是该节点左右儿子中较小的一个与该节点。下图模拟小根堆删除堆顶元素的操作：小根堆删除元素的代码如下： 123456789101112131415void pop(void) { swap(heap[1], heap[len]); len--; down(1);}void down(void) { while(k * 2 &lt;= len) { int j = k * 2; if(k*2+1 &lt; len &amp;&amp; heap[j+1] &lt; heap[j]) j++; if(heap[k] &lt;= heap[j]) break; swap(heap[k], heap[j]); k = j; }} 堆中任意位置的删除删除堆中的任意一个元素时，我们可以发现这个时候这个元素下的左子树堆和右子树堆也满足堆结构，但是我们不可以像删除堆顶节点一样，和数组尾部元素互换，然后尝试下移，原因如下：此时无需向下调整，因为 $5&lt;8$ 且 $5&lt;9$，依旧满足小根堆的性质，但是其父节点 $6&gt;5$，破坏了小根堆的性质，因此此时需要上移。所以我们删除堆中的任意一个元素，跟数组尾元素互换时，不仅要考虑下移，还有可能会上移。小根堆中删除一个位于数组位置 pos 的元素的代码如下:1234567891011121314151617void deleteElem(int pos) { if (pos == len) { heap[len] = 0; len--; return; } int x = heap[pos], y = heap[len]; swap(heap[pos], heap[len]); len--; if (y &lt; x) { // 堆尾的数比原数大, 尝试上移 up(pos); } else { // 堆尾的数比原数小, 尝试下移 down(pos); }} STL 中的 priority_queue(优先队列)STL 库中的 priority_queue 是一个很类似于堆的结构，它包含如下操作： empty - 判断是否为空 size - 返回队列内元素个数 top - 访问队首元素 push - 往队列中插入一个元素 pop - 弹出队首元素 这里的 priority_queue 相当于堆，队首元素相当于堆顶元素。我们可以使用如下语句创建一个小根堆： 1priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 这段C++语句创建了一个优先队列 q，其中元素类型为 int，底层容器使用 vector&lt;int&gt;，并且使用 greater&lt;int&gt; 作为比较器。在优先队列中，当元素被插入队列时，会根据比较器的规则进行排序，从而实现堆的性质。 在这段语句中，greater&lt;int&gt; 是一个函数对象，代表使用“大于”运算符进行比较。因此，当要创建一个小根堆时，即希望队列中的元素按照从小到大的顺序排列，可以利用 greater 作为比较器，这样队列中的最小元素将位于队首。同样的，我们可以使用如下语句创建一个大根堆： 1234// 写法 1：priority_queue&lt;int&gt; q;// 写法 2：priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt; q; 当使用 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 创建一个小根堆后，可以按以下方式操作这个小根堆： 插入元素：使用 push() 方法将元素插入小根堆。 123q.push(5); // 插入元素5q.push(3); // 插入元素3q.push(7); // 插入元素7 获取堆顶元素：使用 top() 方法获取小根堆的头部元素。 12int topElement = q.top(); // 获取小根堆的头部元素cout &lt;&lt; &quot;Top element of the min heap: &quot; &lt;&lt; topElement &lt;&lt; endl; 删除堆顶元素：使用 pop() 方法删除小根堆顶部的元素。 1q.pop(); // 删除小根堆的头部元素 查看堆是否为空：使用 empty() 方法检查小根堆是否为空。 12345if (q.empty()) { cout &lt;&lt; &quot;Min heap is empty&quot; &lt;&lt; endl;} else { cout &lt;&lt; &quot;Min heap is not empty&quot; &lt;&lt; endl;}","link":"/2024/05/07/Heap/"},{"title":"Tree","text":"树(Tree)[TOC] 树的基本概念树是一种非常重要的非线性数据结构，树的一个节点可能会生出多个分支。一般而言，一棵树会包含一个根节点，向下延伸出若干子节点，每个末端的节点被称为叶子节点。 有根树有根树存在一个根节点Root，如下：对于图中概念的一些补充： 节点拥有的子节点个数叫做节点的度。 具有相同深度的节点处于同一层，方便表示。 节点和节点之间的线叫做边。 路径：指从树上一点到另外一点所经过的不重合的点和边的集合，题目中有时会单指点或边的集合。 一颗 $n$ 个节点的树，一定有 $n-1$ 条边 无根树 二叉树二叉树是一种特殊的树。 所有节点的度都不超过2的树称为二叉树。 因为每个二叉树的节点最多只会有两个子结点，它的两个子节点一般会被称为左、右儿子，两棵子树一般会被称为左、右子树。 左、右儿子甚至根节点本身都有可能缺失（一个节点都没有可以称为空二叉树）。 满二叉树和完全二叉树二叉树也有两个比较特殊的类型：满二叉树和完全二叉树。 满二叉树：所有层的节点全满。 满二叉树的一些规律 第 $n$ 层的节点个数为 $2^{n-1}$ 深度为 $n$ 的满二叉树节点数为 $2^0 + 2^1 + 2^2 + \\dots + 2^{n-1}= 2^n-1$ 完全二叉树：除了最后一层以外，其他层的节点个数全满，而且最后一层的节点从左到右排满直到最后一个节点。 完全二叉树的一些规律 完全二叉树的节点个数不会少于 $(2^{n-1}-1)+1 = 2^{n-1}$ 完全二叉树的节点个数不会多于 $2^{n} - 1$ 一棵完全二叉树，设当前节点为 $t$，其父节点为 $t/2$，其左儿子为 $2t$，其右儿子为 $2t+1$，借助该规律，我们可以将完全二叉树使用数组进行存储。 完全二叉树的存储 完全二叉树由于它的特性，可以简单用数组来模拟其结构 一般会以数组$[1]$位置为根节点建立二叉树 数组$[t]$位置的左儿子和右儿子对应的位置分别为$[2t]$和$[2t+1]$，父节点的位置为$[t/2]$。 堆、线段树等数据结构的建立也会参考这个方式 完全二叉树的建立（使用数组），使用这种方法建立非完全二叉树，会导致空间的浪费： 12345678void build(int t) { // 添加数据 UpdateData(t); // 如果子节点存在 Build(2 * t); Build(2 * t + 1);} 为了解决这个问题，我们可以使用其他方法来完成一般二叉树的存储，可以用数组下标模拟节点编号，用多个数组来记录节点信息。为了方便，我们也可以使用结构体来存储这些信息： 12345// 使用结构体来实现上述操作struct TreeNode { int value; int l, r, fa;}a[100010]; 当然，作为一种树形结构，使用指针显然是更合适的方法： 123456789// 使用指针来实现上述操作struct TreeNode { int value; TreeNode* l; TreeNode* r; TreeNode* fa;};TreeNode* root; 使用指针的一些操作： 新建节点： 1234567struct TreeNode { int value; TreeNode *l, *r, *fa; // 初始为 NULL TreeNode(int x){ value = x; }};TreeNode* treeNode = new TreeNode(x); 根节点初始化： 12TreeNode* root;root = new TreeNode(v); 插入节点： 1234567891011void Insert(TreeNode* fa, TreeNode* p, int flag){ // flag = 0 插入到左边 // flag = 1 插入到右边 if (!flag) fa-&gt;l = p; else fa-&gt;r = p; p-&gt;fa = fa;}TreeNode* treeNode = new TreeNode(v);Insert(fa, treeNode, flag); 删除节点 1// 删除节点 二叉树的遍历二叉树的遍历可分为先序遍历、中序遍历和后序遍历，这三种方式以访问根节点的时间来区分。先序遍历（Degree-Left-Right, DLR）：根→左→右中序遍历（Left-Degree-Right, LDR）：左→根→右先序遍历（Left-Right-Degree, LRD）：左→右→根 在该图中，先序遍历的结果为 1 2 4 5 3 6 7，先序遍历代码如下： 1234567void preOrder(TreeNode* treeNode) { cout &lt;&lt; p-&gt;value &lt;&lt; endl; if(treeNode-&gt;l) preOrder(treeNode-&gt;l); if(treeNode-&gt;r) preOrder(treeNode-&gt;r);}preOrder(root); 在该图中，中序遍历的结果为 4 2 5 1 6 3 7，中序遍历代码如下： 1234567void inOrder(TreeNode* treeNode) { if(treeNode-&gt;l) inOrder(treeNode-&gt;l); cout &lt;&lt; p-&gt;value &lt;&lt; endl; if(treeNode-&gt;r) inOrder(treeNode-&gt;r);}inOrder(root); 在该图中，后序遍历的结果为 4 5 2 6 7 3 1，后序遍历代码如下： 1234567void postOrder(TreeNode* treeNode) { if(treeNode-&gt;l) postOrder(treeNode-&gt;l); if(treeNode-&gt;r) postOrder(treeNode-&gt;r); cout &lt;&lt; p-&gt;value &lt;&lt; endl;}postOrder(root); 除了上述的几种遍历方式，还有层级遍历（BFS）方式对树进行遍历。层级遍历是借助队列（Queue）来实现的，其过程可以描述如下： 层级遍历的代码如下： 1234567891011121314TreeNode* q[N];void bfs(TreeNode* root) { int front = 1, rear = 1; q[1] = root; while (front &lt;= rear) { TreeNode* p = q[front]; // 选取队列中最前面的节点 front++; cout &lt;&lt; p-&gt;value &lt;&lt;endl; if(p-&gt;l) q[++rear] = p-&gt;l; if(p-&gt;r) q[++rear] = p-&gt;r; }}bfs(root); 计算节点的深度我们可以在遍历树的时候同时进行节点深度的记录，简单来讲就是：$$depth_{儿子} = depth_{父亲} + 1$$ 有根树(Tree)这里不再是二叉树这种特殊的树，而是一般意义的树。 树的存储方式 vector/链表 123456789101112131415161718// vector 方式vector&lt;int&gt; nodes[N + 1];int n, father[N + 1];// 在 x 和 y之间构建一条边void addEdge(int x, int y) { nodes[x].push_back(y);}// 遍历 x 的所有儿子int l = nodes[x].size();for (int i = 0; i &lt; l; i++) { nodes[x][i];}for (auto i: nodes[x]) { ...} 123456789101112131415161718// 链表方式struct Node { int where; Node *next;} *head[N + 1], a[M];int n, father[N + 1], l = 0;void addEdge(int x, int y) { a[++i].where = y; a[l].next = head[x]; head[x] = &amp;a[l];}// 遍历 x 的所有儿子for (Node* p = head[x]; p; p-&gt;next) { p-&gt;where;} 有根树遍历遍历一棵树一般有 DFS 和 BFS 两种方式。DFS：深度优先搜索，从一个节点开始，选择一条路径并走到底，并通过回溯来访问所有节点。BFS：广度优先搜索，也称层级顺序探索，从一个节点开始，遍历该节点的所有子节点，或称按照深度从小到大的顺序依次遍历所有点。 有根树的DFS序有根树的 DFS 序是指，从根节点开始的深度优先搜索过程中，依次记录的点所生成的序列。对于上图，所生成的 DFS 序即为 ABCDEFGHIJKLMN。当然这个只是其中一种 DFS 序，因为 A 可以走向 B，也可以走向 E，当然也可以走向 F。不同的走向会有不同的 DFS 序。 1234567891011vector&lt;int&gt; dfn; // 用于存储 DFS 序, 常用 DFN 表示 DFS 序 // dfn 中的元素即为 DFS 序void dfs(int x) { dfn.push_back(x); // for x的所有儿子y { dfs(y); } // for (Node* p = x; p; p-&gt;next){ dfs(p-&gt;next) }}dfs(root); 有根树的BFS序有根树的 BFS 序是指，从根节点开始的广度优先搜索过程中，依次记录的点所生成的序列。对于上图，所生成的 BFS 序即为 ABENCDFMGJHIKL。当然这个只是其中一种 BFS 序，因为同一深度可能会有不同的遍历顺序，如深度为 $2$ 时，BEN、BNE、EBN、…都是可能出现的顺序，不同的顺序会有不同的 BFS 序。 12345678910111213void bfs(int root) { // 将 root 加入队列 q; q.push(root); // 遍历队列 q while(队列 q 非空) { x = q.top(); // 取队首元素 q.pop(); // x 出队 for x的所有儿子y { y 入队; } }} 无根树(Unrooted Tree)无根树即没有固定根结点的树，树中的节点只有相邻关系而没有父子关系。无根树有几种等价的形式化定义（建议搭配图论一起学习）： 有 $n$ 个结点， $n−1$ 条边的连通无向图 无向无环的连通图 任意两个结点之间有且仅有一条简单路径的无向图 任何边均为桥的连通图 没有圈，且在任意不同两点间添加一条边之后所得图含唯一的一个圈的图 如下图所示，即一棵无根树：无根树中的任意一个节点可以被指定为根，变成一棵有根树。 无根树的遍历遍历一棵无根树一般也有 DFS 和 BFS 两种方式。遍历无根树时，可以从任意一个节点开始，以类似有根树的方式，遍历整棵树。唯一的区别是在进入一个新节点时，需要记录这个节点的来源节点，在遍历新节点的相邻节点时，避免重复访问来源节点即可。 无根树的 DFS 123456789void dfs(int from, int x) { for x的所有响铃节点y { if (y != from) { dfs(x, y); } }}dfs(-1, x); 无根树的 BFS 12345678910111213141516void bfs(int x) { // 将 x 加入队列 q，x 的来源为空 while (队列 q 非空) { x = q.top(); from = x的来源节点; q.pop; for x的所有相邻节点 y { if (y != from) { y 入队; 记录 y 的来源节点为 x; } } }}bfs(x); 树的直径 树的直径是指树上任意两个节点之间最长（路径的长度一般指的是路径经过的边的数量）的路径。 一棵树可以存在很多条直径，他们的长度相等。 树的直径的中间节点被称为树的中心（图中C节点），如果直径上有偶数个节点，那么中间的两个节点都可以是树的中心。 树的中心到其它点的最长路径最短。","link":"/2024/05/07/Tree/"},{"title":"BELKA_2024","text":"[TOC] Leash Bio - Predict New Medicines with BELKA用 BELKA 预测新药 Predict small molecule-protein interactions using the Big Encoded Library for Chemical Assessment (BELKA) 使用化学评估大编码库（BELKA）预测小分子蛋白质相互作用 OverviewIn this competition, you’ll develop machine learning (ML) models to predict the binding affinity of small molecules to specific protein targets – a critical step in drug development for the pharmaceutical industry that would pave the way for more accurate drug discovery. You’ll help predict which drug-like small molecules (chemicals) will bind to three possible protein targets. 在这场比赛中，你将开发机器学习（ML）模型来预测小分子与特定蛋白质靶标（目标蛋白）的结合亲和力——这是制药行业药物开发的关键一步，将为更准确的药物发现铺平道路。你将帮助预测哪种药物样的小分子（化学物质）将与三种可能的蛋白质靶点结合。 DescriptionSmall molecule drugs are chemicals that interact with cellular protein machinery and affect the functions of this machinery in some way. Often, drugs are meant to inhibit the activity of single protein targets, and those targets are thought to be involved in a disease process. A classic approach to identify such candidate molecules is to physically make them, one by one, and then expose them to the protein target of interest and test if the two interact. This can be a fairly laborious and time-intensive process. 小分子药物是与细胞蛋白质机制相互作用并以某种方式影响该机制功能的化学物质。通常，药物旨在抑制单个蛋白质靶标的活性，而这些靶标被认为与疾病过程有关。识别这类候选分子的一种经典方法是一个接一个地进行物理制造，然后将其暴露于感兴趣的蛋白质靶点，并测试两者是否相互作用。这可能是一个相当费力和耗时的过程。 The US Food and Drug Administration (FDA) has approved roughly 2,000 novel molecular entities in its entire history. However, the number of chemicals in druglike space has been estimated to be 10^60, a space far too big to physically search. There are likely effective treatments for human ailments hiding in that chemical space, and better methods to find such treatments are desirable to us all. 美国食品药品监督管理局（FDA）已经批准了大约2000种新型分子实体在其整个历史. 然而，类药物领域的化学物质数量估计为$10^60$，这个空间太大了，无法进行物理搜索。在这个化学空间里，可能有有效的治疗人类疾病的方法，而找到更好的治疗方法对我们所有人来说都是可取的。 To evaluate potential search methods in small molecule chemistry, competition host Leash Biosciences physically tested some 133M small molecules for their ability to interact with one of three protein targets using DNA-encoded chemical library (DEL) technology. This dataset, the Big Encoded Library for Chemical Assessment (BELKA), provides an excellent opportunity to develop predictive models that may advance drug discovery. 为了评估小分子化学中潜在的搜索方法，比赛主办方Leash Biosciences使用DNA编码化学文库（DEL）技术对约133M个小分子进行了物理测试，以确定它们与三个蛋白质靶标之一相互作用的能力。该数据集，即化学评估大编码库（BELKA），为开发可能促进药物发现的预测模型提供了极好的机会。 Datasets of this size are rare and restricted to large pharmaceutical companies. The current best-curated public dataset of this kind is perhaps bindingdb, which, at 2.8M binding measurements, is much smaller than BELKA. 这种规模的数据集非常罕见，仅限于大型制药公司。目前这类最好的公共数据集可能是bindingdb，在2.8M的结合测量值下，比BELKA小得多。 This competition aims to revolutionize small molecule binding prediction by harnessing ML techniques. Recent advances in ML approaches suggest it might be possible to search chemical space by inference using well-trained computational models rather than running laboratory experiments. Similar progress in other fields suggest using ML to search across vast spaces could be a generalizable approach applicable to many domains. We hope that by providing BELKA we will democratize aspects of computational drug discovery and assist the community in finding new lifesaving medicines. 这项竞赛旨在通过利用ML技术彻底改变小分子结合预测。ML方法的最新进展表明，使用训练有素的计算模型而不是进行实验室 实验，通过推理搜索化学空间是可能的。其他 领域的类似进展表明，使用ML在广阔的空间中搜索可能是一种适用于许多领域的通用方法。我们希望通过提供BELKA，我们将使计算药物发现的各个方面民主化，并帮助社区寻找新的救命药物。 Here, you’ll build predictive models to estimate the binding affinity of unknown chemical compounds to specified protein targets. You may use the training data provided; alternatively, there are a number of methods to make small molecule binding predictions without relying on empirical binding data (e.g. DiffDock, and this contest was designed to allow for such submissions). 在这里，你将建立预测模型来估计未知化合物与特定蛋白质靶标的结合亲和力。您可以使用提供的培训数据；或者，有许多方法可以在不依赖经验结合数据的情况下进行小分子结合预测（例如DiffDock，而本次竞赛旨在允许此类提交）。 Your work will contribute to advances in small molecule chemistry used to accelerate drug discovery. 你的工作将有助于促进用于加速药物发现的小分子化学的进步。 EvaluationThis metric for this competition is the average precision calculated for each (protein, split group) and then averaged for the final score. Please see this forum post for important details. 这项比赛的指标是为每个（蛋白质、分组）计算的平均精度，然后为最终得分取平均值。请参阅此论坛帖子了解重要细节。 Here’s the code for the implementation. 这是代码以供实施。 Submission FileFor each id in the test set, you must predict a probability for the binary target binds target. The file should contain a header and have the following format: 对于测试集中的每个id，您必须预测二进制目标“绑定”目标的概率。该文件应包含一个标头，并具有以下格式： 12345id,binds295246830,0.5295246831,0.5295246832,0.5etc. Timeline April 4, 2024 - Start Date. July 1, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete. July 1, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams. July 8, 2024 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes First Prize: $12,000 Second Prize: $10,000 Third Prize: $10,000 Fourth Prize: $8,000 Fifth Prize: $5,000 Top Student Group: $5,000 to the highest performing student team. A team would be considered a student team if majority members (e.g. at least 3 out of a 5 member team) are students enrolled in a high school or university degree. In the case of an even number of members, half of them must be students. Competition HostLeash Biosciences is a discovery-stage biotechnology company that seeks to improve medicinal chemistry with machine learning approaches and massive data collection. Leash is comprised of wet lab scientists and dry lab scientists in equal numbers, and is proudly headquartered in Salt Lake City, Utah, USA. Additional DetailsChemical RepresentationsOne of the goals of this competition is to explore and compare many different ways of representing molecules. Small molecules have been [represented](https://pubs.acs.org/doi/10.1021/acsinfocus.7e7006?ref=infocus%2FAI_&amp; Machine Learning) with SMILES, graphs, 3D structures, and more, including more esoteric methods such as spherical convolutional neural nets. We encourage competitors to explore not only different methods of making predictions but also to try different ways of representing the molecules. We provide the molecules in SMILES format. 这场比赛的目标之一是探索和比较许多不同的分子表现方式。小分子已经用SMILES、图形、3D结构等表示，包括更深奥的方法，如球形卷积神经网络。我们鼓励竞争对手不仅探索不同的预测方法，还尝试不同的分子表示方法。 我们提供SMILES格式的分子。 SMILESSMILES is a concise string notation used to represent the structure of chemical molecules. It encodes the molecular graph, including atoms, bonds, connectivity, and stereochemistry as a linear sequence of characters, by traversing the molecule graph. SMILES is widely used in machine learning applications for chemistry, such as molecular property prediction, drug discovery, and materials design, as it provides a standardized and machine-readable format for representing and manipulating chemical structures. The SMILES in this dataset should be sufficient to be translated into any other chemical representation format that you want to try. A simple way to perform some of these translations is with RDKit. SMILES是一种简明的字符串表示法，用于表示化学分子的结构。它通过遍历分子图，将分子图（包括原子、键、连接性和立体化学）编码为线性字符序列。SMILES广泛用于化学的机器学习应用，如分子性质预测、药物发现和材料设计，因为它为表示和操纵化学结构提供了标准化和机器可读的格式。该数据集中的SMILES应该足以转换为您想要尝试的任何其他化学表示格式。执行其中一些翻译的一种简单方法是使用RDKit. Details about the experimentsDELs are libraries of small molecules with unique DNA barcodes covalently attachedTraditional high-throughput screening requires keeping individual small molecules in separate, identifiable tubes and demands a lot of liquid handling to test each one of those against the protein target of interest in a separate reaction. The logistical overhead of these efforts tends to restrict screening collections, called libraries, to 50K-5M small molecules. A scalable solution to this problem, DNA-encoded chemical libraries, was described in 2009. As DNA sequencing got cheaper and cheaper, it became clear that DNA itself could be used as a label to identify, and deconvolute, collections of molecules in a complex mixture. DELs leverage this DNA sequencing technology. These barcoded small molecules are in a pool (many in a single tube, rather than one tube per small molecule) and are exposed to the protein target of interest in solution. The protein target of interest is then rinsed to remove small molecules in the DEL that don’t bind the target, and the remaining binders are collected and their DNA sequenced. DEL是共价连接有独特DNA条形码的小分子库传统高通量筛选需要将单个小分子保持在单独的、可识别的管中，并且需要大量的液体处理来在单独的反应中针对感兴趣的蛋白质靶标测试其中的每一个。这些工作的后勤开销往往将筛选收藏（称为文库）限制在5000万至500万个小分子以内。这个问题的一个可扩展的解决方案，DNA编码的化学文库，在2009年描述. 随着DNA测序变得越来越便宜，很明显，DNA本身可以用作标签来识别和消除复杂混合物中分子的聚集。DELs这种DNA测序技术。这些条形码小分子在一个池中（许多在单管中，而不是每个小分子一管），并暴露于溶液中感兴趣的蛋白质靶标。然后冲洗感兴趣的蛋白质靶标，以去除DEL中不与靶标结合的小分子，收集剩余的结合物并对其DNA进行测序。 DELs are manufactured by combining different building blocksAn intuitive way to think about DELs is to imagine a Mickey Mouse head as an example of a small molecule in the DEL. We attach the DNA barcode to Mickey’s chin. Mickey’s left ear is connected by a zipper; Mickey’s right ear is connected by velcro. These attachment points of zippers and velcro are analogies to different chemical reactions one might use to construct the DEL. We could purchase ten different Mickey Mouse faces, ten different zipper ears, and ten different velcro ears, and use them to construct our small molecule library. By creating every combination of these three, we’ll have 1,000 small molecules, but we only needed thirty building blocks (faces and ears) to make them. This combinatorial approach is what allows DELs to have so many members: the library in this competition is composed of 133M small molecules. The 133M small molecule library used here, AMA014, was provided by AlphaMa. It has a triazine core and superficially resembles the DELs described here. DEL是通过组合不同的构建块来制造的一个思考DEL的直观方法是想象一个米老鼠的头作为DEL中一个小分子的例子。我们把DNA条形码贴在米奇的下巴上。米奇的左耳由拉链连接；米奇的右耳是用尼龙搭扣连接的。拉链和尼龙搭扣的这些连接点类似于可能用于构建DEL的不同化学反应。我们可以购买十个不同的米老鼠脸、十个不同拉链耳朵和十个不同尼龙搭扣耳朵，并用它们来构建我们的小分子库。通过创建这三者的每一个组合，我们将拥有1000个小分子，但我们只需要30个构建块（脸和耳朵）就可以制造它们。这种组合方法使DEL能够拥有如此多的成员：这场竞争中的文库由133M个小分子组成。这里使用的133M小分子文库AMA014由AlphaMa提供。它有一个三嗪核心，表面上类似于此处描述的DEL。 AcknowledgementsLeash Biosciences is grateful for the generous cosponsorship of Top Harvest Capital and AlphaMa. CitationAndrew Blevins, Ian K Quigley, Brayden J Halverson, Nate Wilkinson, Rebecca S Levin, Agastya Pulapaka, Walter Reade, Addison Howard. (2024). Leash Bio - Predict New Medicines with BELKA. Kaggle. https://kaggle.com/competitions/leash-BELKA Dataset DescriptionOverviewThe examples in the competition dataset are represented by a binary classification of whether a given small molecule is a binder or not to one of three protein targets. The data were collected using DNA-encoded chemical library (DEL) technology. 比赛数据集中的例子由给定小分子是否与三个蛋白质靶标之一结合的二元分类表示。使用DNA编码化学文库（DEL）技术收集数据。 We represent chemistry with SMILES (Simplified Molecular-Input Line-Entry System) and the labels as binary binding classifications, one per protein target of three targets. 我们用SMILES（简化分子输入 行输入系统)和二元绑定分类来表示化学，三个靶标中的每个蛋白质靶标都有一个。 Files[train/test].[csv/parquet] - The train or test data, available in both the csv and parquet formats. id - A unique example_id that we use to identify the molecule-binding target pair. buildingblock1_smiles - The structure, in SMILES, of the first building block buildingblock2_smiles - The structure, in SMILES, of the second building block buildingblock3_smiles - The structure, in SMILES, of the third building block molecule_smiles - The structure of the fully assembled molecule, in SMILES. This includes the three building blocks and the triazine core. Note we use a [Dy] as the stand-in for the DNA linker. protein_name - The protein target name binds - The target column. A binary class label of whether the molecule binds to the protein. Not available for the test set. sample_submission.csv - A sample submission file in the correct format [train/test].[csv/parquet] - 训练或测试数据，csv和parquet格式均可。 id - 我们用来识别分子结合靶标对的唯一示例_id。 buildingblock1_smiles - 第一个构建块的结构，以SMILES表示 buildingblock2_smiles - 第二个构建块的结构，以SMILES表示 buildingblock3_smiles - 第三个构建块的结构，以SMILES表示 molecule_smiles - 完全组装的分子的结构，以SMILES表示。这包括三个构建块和三嗪核心。请注意，我们使用[Dy]作为DNA连接子的替代。 protein_name - 蛋白质靶标名称 binds - 目标列。分子是否与蛋白质结合的二进制类标签。不适用于测试集。 Competition dataAll data were generated in-house at Leash Biosciences. We are providing roughly 98M training examples per protein, 200K validation examples per protein, and 360K test molecules per protein. To test generalizability, the test set contains building blocks that are not in the training set. These datasets are very imbalanced: roughly 0.5% of examples are classified as binders; we used 3 rounds of selection in triplicate to identify binders experimentally. Following the competition, Leash will make all the data available for future use (3 targets × 3 rounds of selection × 3 replicates × 133M molecules, or 3.6B measurements). 所有数据均由Leash Biosciences公司内部生成。我们为每种蛋白质提供了大约 98M 个训练实例，为每种蛋白提供了 200K 个验证实例，为每个蛋白质提供了 360K 个测试分子。为了测试可推广性，测试集包含不在训练集中的构建块。这些数据集非常不平衡：大约0.5%的示例被归类为绑定；我们使用了三轮一式三份的选择来实验鉴定粘合剂。比赛结束后，Leash将提供所有数据供未来使用（3个靶标×3轮选择×3个重复×3.33M个分子，或3.6B测量值）。 TargetsProteins are encoded in the genome, and names of the genes encoding those proteins are typically bestowed by their discoverers and regulated by the Hugo Gene Nomenclature Committee. The protein products of these genes can sometimes have different names, often due to the history of their discovery. We screened three protein targets for this competition. 蛋白质在基因组中编码，编码这些蛋白质的基因的名称通常由其发现者命名，并由雨果基因命名委员会监管。这些基因的蛋白质产物有时可能有不同的名称，通常是由于它们的发现历史。我们为这次比赛筛选了三个蛋白质靶点。 EPHX2 (sEH)The first target, epoxide hydrolase 2, is encoded by the EPHX2 genetic locus, and its protein product is commonly named “soluble epoxide hydrolase”, or abbreviated to sEH. Hydrolases are enzymes that catalyze certain chemical reactions, and EPHX2/sEH also hydrolyzes certain phosphate groups. EPHX2/sEH is a potential drug target for high blood pressure and diabetes progression, and small molecules inhibiting EPHX2/sEH from earlier DEL efforts made it to clinical trials. EPHX2/sEH was also screened with DELs, and hits predicted with ML approaches, in a recent study but the screening data were not published. We included EPHX2/sEH to allow contestants an external gut check for model performance by comparing to these previously-published results. We screened EPHX2/sEH purchased from Cayman Chemical, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino sequence is positions 2-555 from UniProt entry P34913, the crystal structure can be found in PDB entry 3i28, and predicted structure can be found in AlphaFold2 entry 34913. Additional EPHX2/sEH crystal structures with ligands bound can be found in PDB. 第一个靶标环氧化物水解酶2由EPHX2基因座编码，其蛋白产物通常被命名为“可溶性环氧化物水解酶”，或缩写为sEH。水解酶是催化某些化学反应的酶，EPHX2/sEH也水解某些磷酸基团。EPHX2/sEH是高血压和糖尿病进展的潜在药物靶点，早期DEL研究中抑制EPHX2/s EH的小分子已进入临床试验.EPHX2/sEH也用DEL进行了筛选，并用ML方法预测了命中率(https://blog.research.google/2020/06/unlocking-chemome-with-dna-encoded.html学习https://pubs.acs.org/doi/10.1021/acs.jmedchem.0c00452)但筛选数据没有公布。我们纳入了EPHX2/sEH，通过与之前公布的结果进行比较，让参赛者能够对模型性能进行外部检查。我们筛选了EPHX2/sEH购自开曼化学. 在PDB中可以发现具有结合配体的额外的EPHX2/sEH晶体结构。 BRD4The second target, bromodomain 4, is encoded by the BRD4 locus and its protein product is also named BRD4. Bromodomains bind to protein spools in the nucleus that DNA wraps around (called histones) and affect the likelihood that the DNA nearby is going to be transcribed, producing new gene products. Bromodomains play roles in cancer progression and a number of drugs have been discovered to inhibit their activities. BRD4 has been screened with DEL approaches previously but the screening data were not published. We included BRD4 to allow contestants to evaluate candidate molecules for oncology indications. We screened BRD4 purchased from Active Motif, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 44-460 from UniProt entry O60885-1, the crystal structure (for a single domain) can be found in PDB entry 7USK and predicted structure can be found in AlphaFold2 entry O60885. Additional BRD4 crystal structures with ligands bound can be found in PDB. ALB (HSA)The third target, serum albumin, is encoded by the ALB locus and its protein product is also named ALB. The protein product is sometimes abbreviated as HSA, for “human serum albumin”. ALB, the most common protein in the blood, is used to drive osmotic pressure (to bring fluid back from tissues into blood vessels) and to transport many ligands, hormones, fatty acids, and more. Albumin, being the most abundant protein in the blood, often plays a role in absorbing candidate drugs in the body and sequestering them from their target tissues. Adjusting candidate drugs to bind less to albumin and other blood proteins is a strategy to help these candidate drugs be more effective. ALB has been screened with DEL approaches previously but the screening data were not published. We included ALB to allow contestants to build models that might have a larger impact on drug discovery across many disease types. The ability to predict ALB binding well would allow drug developers to improve their candidate small molecule therapies much more quickly than physically manufacturing many variants and testing them against ALB empirically in an iterative process. We screened ALB purchased from Active Motif. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 25 to 609 from UniProt entry P02768, the crystal structure can be found in PDB entry 1AO6, and predicted structure can be found in AlphaFold2 entry P02768. Additional ALB crystal structures with ligands bound can be found in PDB. Good luck!","link":"/2024/05/19/BELKA-2024/"},{"title":"IDEA 2022 搭建 Tomcat 环境","text":"[TOC] Tomcat 环境的搭建参考教程 下载 TomcatTomcat官网地址在 Tomcat 官网中下载指定版本的 Tomcat，左侧 Download 处有相应版本，这里推荐 Tomcat 9 版本（因为Tomcat 10 在配置时会出现一定的问题）。下载后解压到指定位置即可。 配置环境变量即可配置 Tomcat 环境变量前一定要配置好 Java 的环境变量，尤其是JAVA_HOME，这里我一开始并没有配置 JAVA_HOME，我的环境变量是JAVA_HOME_180=xxx，这种方式Tomcat是找不到JAVA_HOME的，因此我又重新配置了JAVA_HOME。我的 JAVA_HOME 环境变量为： 1JAVA_HOME=D:\\JDK\\jdk1.8.0_231 下面是 Tomcat 的环境变量配置：新建 CATALINA_HOME 环境变量： 1CATALINA_HOME=D:\\tomcat\\apache-tomcat-9.0.89 修改Path，在 Path 后添加（新建）如下环境变量： 123%CATALINA_HOME%\\lib%CATALINA_HOME%\\bin%CATALINA_HOME%\\lib\\servlet-api.jar 验证是否配置成功在命令行中，执行命令：startup.bat，若正常打印相关配置变量、且 Tomcat 进程被阻塞，即证明环境搭建成功。访问localhost:8080，出现以下界面即证明成功搭建。使用 shutdown.bat 命令即可使阻塞的 Tomcat 进程被关闭，推荐使用这种方式关闭 Tomcat。 可能会出现的问题 协议处理程序初始化失败：参考教程这个问题有可能是由于8080端口被占用了，在Windows中可以使用如下命令查看端口的占用情况： 1netstat -aon|findstr &quot;8080&quot; 如果确实被占用了，可以使用如下命令杀死端口号为 &lt;PIDNUM&gt; 的进程。 1taskkill -PID &lt;PIDNUM&gt; -F 闪退可能原因是：环境变量配置不正确，仔细检查环境变量的配置。 乱码问题描述：打开startup.bat后汉字乱码解决方法：在.\\apache-tomcat-9.0.43\\conf下打开logging.properties文件将java.util.logging.ConsoleHandler.encoding = UTF-8替换为java.util.logging.ConsoleHandler.encoding = GBK 社区版 IDEA 如何配置 TomcatCSDN 上大多数教程使用 Maven 创建 Tomcat 项目，但是这种方法实在是过于麻烦，社区版和专业版又有些不同，找不到很多东西。 如何配置 IDEA 2022 社区版中的 Tomcat 安装插件在 File → Settings → Plugin 中安装插件，搜索 Tomcat，安装插件。 配置Tomcat路径安装插件后，在 File → Settings → Plugin → Tomcat Server添加配置如下： 完成","link":"/2024/05/22/Build-Tomcat/"},{"title":"Better QT","text":"[TOC] Qt 的一些常用技巧快捷键 快捷键 Ctrl + Tab 可以切换文件； 快捷键 Alt + ENTER 可以快速提示错误修改方案，类似于 IDEA 的 Alt + ENTER； 快捷键 Ctrl + R 运行程序； 快捷键 Ctrl + M 创建书签（Bookmark），或者直接在某行代码前右键添加书签； 用键盘模拟鼠标操作 功能键 方向键 备注 Ctrl Shift Alt 左/右 上/下 Home/End 方向键具有移动光标的作用 × × × 字符 字符 行首/行尾 - √ × × 单词 滚动条 文件头/尾 - √ √ × 单词 移动 行首/行尾 Shift具有选中文本的作用 √ × √ - 向上/下复制选中部分 - - Qt 代码/文件解释Qt的源代码和文件解释 Qt 代码 hellocosbrowser.h 1234567891011121314151617181920212223242526272829#ifndef HELLOCOSBROWSER_H#define HELLOCOSBROWSER_H#include &lt;QWidget&gt;#include &lt;QMessageBox&gt;QT_BEGIN_NAMESPACEnamespace Ui { class HelloCOSBrowser; }QT_END_NAMESPACEclass HelloCOSBrowser : public QWidget // QWidget 是所有应用程序窗口的基类{ Q_OBJECT // Qt的宏, 支持 Qt 的特性, 如信号与槽、对象树、元对象等public: // 这里 HelloCOSBrowser 指定父窗口指针为 nullptr, 则它会作为一个独立的窗口进行展示, 否则则会作为父窗口的一个控件 // 关于这个父窗口指针, 一个很典型的应用就是 微信 // 当我们创建新窗口的时候, 如果不指定父窗口, 就会弹出一个独立的新窗口, 即电脑任务栏的图标会多出来一个 // 如果指定了父窗口, 则不会创建一个独立的窗口, 即电脑任务栏处的图标不会增加 HelloCOSBrowser(QWidget *parent = nullptr); ~HelloCOSBrowser();private slots: void showDialog();private: Ui::HelloCOSBrowser *ui;};#endif // HELLOCOSBROWSER_H Qt 工程文件解释文件列表 文件名称 描述 pro 文件 该文件是 Qt 的项目文件，qmake工具可以根据此文件生成 Makefile pro.user 文件 该文件包含和用户相关的项目信息（用户不需要关注此文件） ui 文件 Qt 的设计师界面文件 .cpp 文件 C++ 源文件 .h 文件 C++ 头文件 MOC编译器MOC(Meta-Object Compiler)编译器C++ 编译器本身不支持 Qt 的某些机制，Qt 希望对 C++ 代码进行自动扩展，这里就需要用到宏（例如：Q_Object）和继承。此外为了方便用户使用，希望用户无感知，可以将这一操作直接集成到框架中。 Qt 编译过程12345预编译 -&gt; 编译 -&gt; 汇编 -&gt; 链接 -&gt; 目标 ↑ +-------------------------+ ↑拓展代码 -&gt; MOC编译器 -&gt; 新CPP代码 通过上述方式，实现 Qt 的某些特性。我们可以发现，当我们写完代码进行编译后，会产生一个 debug 文件夹，此时我们进入该文件夹，会看到一些元对象编译器编译的文件，如 moc_xxxx.cpp 或 moc_xxx.h 等文件。 MOC 的使用方法 MOC 编译工具由 Qt 框架自动调用 扫描 C++ 头文件，寻找 Q_OBJECT 宏 生成拓展 C++ 代码，再进行预编译 程序员在使用时，需要继承 QObject 类或者是 QObject 子类，并且包含 Q_OBJECT 宏。 Qt应用程序开发为窗口添加图标 准备图标文件 调用 setWindowIcon 方法 为应用程序添加图标（一般使用这种方式） 准备图标文件 logo.ico 修改 pro 工程文件 RC_ICONS = &lt;Path&gt; 通过此种方式修改图标，可执行程序 .exe 的图标会修改，且不需要额外单独设置窗口图标。 部署产品的三种方式 手动部署（不常用，比较繁琐）进入 .exe 文件所在的文件夹（debug目录），双击运行 .exe 文件，会提示缺少的文件（包括dll动态库、plugin插件等），然后找到对应的文件移动到 .exe 文件的同级目录下即可，如下：如果配置了环境变量则大概率不会出现报错提示缺少库的问题，那么这种方式就会失效。 使用 windeployqt 部署① 查找 windeployqt.exe 程序② 将 windeployqt.exe 加入环境变量③ 再命令行界面执行命令 windeployqt.exe &lt;exe_file_dir&gt; 完成操作 使用creator 部署① 项目导航窗口→运行→部署→添加自定义部署② 输入 windeployqt.exe 程序及对应的命令行参数③ 执行部署命令","link":"/2024/05/22/Better-QT/"},{"title":"Evaluation Indicators in AI","text":"","link":"/2024/05/27/Evaluation-Indicators-in-AI/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"软件源","slug":"软件源","link":"/tags/%E8%BD%AF%E4%BB%B6%E6%BA%90/"},{"name":"开发工具","slug":"开发工具","link":"/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"Axure RP 9","slug":"Axure-RP-9","link":"/tags/Axure-RP-9/"},{"name":"闲聊","slug":"闲聊","link":"/tags/%E9%97%B2%E8%81%8A/"},{"name":"计划","slug":"计划","link":"/tags/%E8%AE%A1%E5%88%92/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"VSCode","slug":"VSCode","link":"/tags/VSCode/"},{"name":"大数据技术","slug":"大数据技术","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"ECharts","slug":"ECharts","link":"/tags/ECharts/"},{"name":"Tomcat","slug":"Tomcat","link":"/tags/Tomcat/"},{"name":"CppDev","slug":"CppDev","link":"/tags/CppDev/"},{"name":"QT6","slug":"QT6","link":"/tags/QT6/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"闲聊","slug":"闲聊","link":"/categories/%E9%97%B2%E8%81%8A/"},{"name":"软件源","slug":"Linux/软件源","link":"/categories/Linux/%E8%BD%AF%E4%BB%B6%E6%BA%90/"},{"name":"Axure RP 9","slug":"开发工具/Axure-RP-9","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Axure-RP-9/"},{"name":"计划","slug":"闲聊/计划","link":"/categories/%E9%97%B2%E8%81%8A/%E8%AE%A1%E5%88%92/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Git","slug":"开发工具/Git","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Git/"},{"name":"VSCode","slug":"开发工具/VSCode","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/VSCode/"},{"name":"PyTorch","slug":"深度学习/PyTorch","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/"},{"name":"大数据技术","slug":"大数据技术","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"HBase","slug":"大数据技术/HBase","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/HBase/"},{"name":"数据结构&#x2F;算法","slug":"数据结构-算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/"},{"name":"哈希(Hash)","slug":"数据结构-算法/哈希-Hash","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E5%93%88%E5%B8%8C-Hash/"},{"name":"栈(Stack)","slug":"数据结构-算法/栈-Stack","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%88-Stack/"},{"name":"树(Tree)","slug":"数据结构-算法/树-Tree","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%91-Tree/"},{"name":"Kaggle","slug":"深度学习/Kaggle","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Kaggle/"},{"name":"ECharts","slug":"大数据技术/ECharts","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/ECharts/"},{"name":"CppDev","slug":"CppDev","link":"/categories/CppDev/"},{"name":"QT6","slug":"CppDev/QT6","link":"/categories/CppDev/QT6/"}],"pages":[]}