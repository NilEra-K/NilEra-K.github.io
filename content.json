{"posts":[{"title":"CentOS7更换阿里&#x2F;清华源","text":"Blog网址：https://www.cnblogs.com/wswind/p/10173591.html","link":"/2024/04/23/CentOS7%E6%9B%B4%E6%8D%A2%E9%98%BF%E9%87%8C-%E6%B8%85%E5%8D%8E%E6%BA%90/"},{"title":"Axure RP 9 产品设计入门","text":"🚪Axure RP 9 产品设计入门 Axure RP 9 软件的安装可以直接官网下载，然后去某宝买一个账号的激活码，详情不再赘述，可以自行 Bing。下载网址：https://www.axure.com/release-history/rp9学习参考视频：https://www.bilibili.com/video/BV1hU4y1L77u/ Axure 软件的一些基础设置 偏好设置 文件 → Preferences（首选项、偏好设置） 网格设置 视图设置 → 标尺·网格·辅助线 → 网格设置 我个人认为舒服的设置：网格对齐，间距 10 像素，样式更改为线段，颜色 #F2F2F2 如何预览、发给别人可以浏览的文件发布 → 生成 HTML 文件（选择一个文件夹，会生成很多文件，因此最好选择一个独立的文件夹） 常用的设计尺寸手机端：365*667 开始设计绘制一个简单的UI界面 用矩形绘制一个背景：使用线性方式填充 用水平线绘制隔断 用矩形绘制按钮 如何绘制按钮使用矩形绘制一个按钮 绘制矩形 右键矩形 → 交互样式 美化输入框 绘制矩形 Ctrl + D 复制一份，调整大小 添加输入框和一个搜索图标 母版选择绘制好的组件，右键选择转换为母版 单选框的使用和自定义单选框（美化操作）使用单选框时，需要对单选框进行编组。编组方式：框选对应组的单选框，右键选择指定单选按钮的组，然后命名组名即可。自定义单选框操作步骤： 使用圆形绘制一个大圆以及一个小圆 内部的小圆的线条和填充设置为透明 右键添加交互样式，设置当选中时，内部小圆的填充和线条更改颜色，外部大圆线条更改颜色 添加文字后进行编组，对于编组内容设置交互（和交互样式不同），在菜单栏的右侧，交互面板 效果如下： 常用工具页面工具 页面工具中可以创建多个页面 页面工具中可以按住 Ctrl + 左右方向键 来调整页面的级别 页面工具中可以按住 Ctrl + 上下方向键 来调整页面的顺序 概要工具 概要工具可以看到整个页面全部的元件 当存在遮罩关系时，可以选择概要工具中需要修改的元件，不需要破坏遮罩关系 可以通过筛选按钮来对元件进行筛选 建议给元件起好名字 钢笔工具 世上无难事，只怕有心人 使用钢笔工具来绘制自定义图案 绘制复杂的UI界面 常用的交互事件及操作页面跳转 页面跳转原理：事件 → 动作 → 目标 对于跳转来说：事件（点击） → 动作（跳转） → 目标（页面2） 事件的选择在“交互”面板处（注：不是交互样式） 实现时点击右侧交互面板新建交互，选择单击时，动作设置为打开链接，目标设置为要切换到的界面 当点击预览时候，会发现元件的显示往往出现在屏幕中心位置，如果想要让元件按照画布的绝对位置显示，可以点击页面，选择样式中的“页面排列”，在其中调整相关显示样式。 热区 为什么使用热区？ 对于图片中的情况，如果我们想点击按钮完成跳转操作，可以直接给使用方式 ① 按钮添加一个交互事件。 但是当我们想要点击这个区域时进行跳转，一个元件一个元件添加事件过于繁琐（技巧：可以通过 Ctrl + C 在右侧交互面板直接复制整个事件，然后点击想要添加该事件的元件，通过 Ctrl + V 粘贴到元件上），而且不方便后续的修改。 于是可以使用方式 ② 将要整个部分编组，然后为整个组添加事件。但是当我们因为某种原因破坏了整个组时，可能会遗忘添加事件，导致整个事件消失。 所以热区就是为了解决上面的问题，我们可以通过方式 ③ 给组件添加热区，只要点击这个区域即可完成指定事件。 且热区占用内存更小。 显示和隐藏设置文字设置某元件上的文字 设置选中设置一个元件为选中状态 启用或禁用 启用/禁用是针对于一个元件 常用场景是：当满足某条件时启用/禁用某元件 例如：当用户输入账号和密码后启用登录按钮（瞎编的），当用户同意协议才可进行下一步这种 移动 移动一个元件，可以在“移动”处设置经过（相对位置）、到达（绝对位置）来移动元件 等待 有时候等待看起来并不生效，需要观察是否是等待动作前的动作设置了动画。 例如：移动动作设置了 500ms 的动画，其实移动动作是瞬间完成的，动画只是一种呈现的方式。所以动画呈现的效果和等待是几乎同时进行的（“移动”动作设置 500ms 的动画和 500ms 的“等待”，动画开始播放时“等待”同时进行，动画结束播放时，“等待”结束）。 旋转设置尺寸设置透明度制作更复杂的动效 Axure RP 9 的动画效果是可以叠加的。 等待可以分割两个本来融合的叠加效果。 使用该上图所示的操作，会导致“移动”和“旋转”的融合无法叠加，先执行“移动”，再执行“旋转”。 文本框的长度是可以固定或者跟随的，当文本框样式如下时，则说明文本框此时的长度是固定的。此时可以双击两侧中间的锚点来使文本框变为跟随模式。下图为固定长度：下图为跟随长度：双击角上的锚点可以设置是否跟随高度、宽度（若不跟随，则此时文本框高度和宽度完全固定）。 复杂动效总体流程： 滚动到元件想要动画展示，一般设置动画的时间为 500ms，但是对于用户来说，500ms 看起来有些卡顿，所以 350ms 往往是个不错的选择 更多事件 事件 条件 备注 单击时 点击形状时 一般单击事件不能和“鼠标按下”/“鼠标松开” 事件同时使用 双击时 双击形状时 鼠标右击时 形状的上下文菜单被触发时（右键点击形状时候） 鼠标按下时 鼠标在形状上按下时 鼠标松开时 鼠标在形状上松开时 鼠标移动时 鼠标移动到形状上时 鼠标移入时 鼠标指针进入形状元件区域中时 鼠标移出时 鼠标指针离开形状元件区域中时 鼠标停放时 鼠标指针在停放在形状上超过 1s 时 鼠标长按时 鼠标指针在按压在形状上超过 1s 时 按键按下时 当焦点在形状上并按下键盘上的任意按键时 按键松开时 当焦点在形状上并按下键盘上的任意按键松开时 移动时 当形状发生移动时 旋转时 当形状发生旋转时 尺寸改变时 当形状尺寸发生改变时 显示时 当形状发生显示时 隐藏 当形状发生被隐藏时 获取焦点时 当形状获取焦点时 失去焦点时 当形状失去焦点时 选中 当形状被选中时 取消选中时 当形状取消选中时 选中改变时 当形状的选中状态发生改变时 载入时 形状已加载时（首次显示页面时） 更多动作内部框架 若概念不明确，往往会将内部框架和快照混淆。 快照往往用于在一个页面预览所有的页面，点击快照后跳转到快照包含的页面，并且在跳转后的页面进行修改。 而内部框架和快照不同，内部框架相当于将需要包含的页面复制了一份，粘贴到了内部框架中，因此可以直接在内部框架中修改内容，修改内部框架中的内容不会影响原页面内容。 动态面板 动态面板也是一个元件，也可以显示或隐藏。 我们可以将动态面板形象的理解为一个盒子。在盒子里没有东西时，动态面板“无色无味”，看不见摸不到，双击后可以键入盒子内部。 盒子内部也可以添加很多层： 我们可以通过一个按钮，添加“设置动态面板”动作来切换层，因此我们可以想到动态面板常用于实现轮播图（当然还有很多其他的玩法） 动态面板案例 1：遮罩使用动态面板可以实现遮罩的一些操作，如： 超出指定范围的元件可以使用动态面板，隐藏超出部分，右键直接将元件转换为动态面板即可： 将元件一直固定再某一位置选择固定到浏览器，并且进一步选择固定位置，当浏览器页面过长需要滚动时，动态面板的位置不会发生改变。 通栏选择 “100%宽度（仅浏览器中有效）”，即可将动态面板的宽度按照浏览器的宽度进行设置，也就是所谓的通栏（Banner，Banner也可以用来表示轮播图） 动态面板是可以嵌套的 动态面板案例 2：浮窗 动态面板案例 3：弹窗 动态面板案例 4：轮播 动态面板案例 5：Tab切页 动态面板案例 6：列表切页 动态面板案例 7：拖动 使用动态面板实现拖动的难点在于使用“动态面板的嵌套”和“设置边界” 使用动态面板的嵌套是为了防止直接拖动最外层的动态面板，而是拖动动态面板内部的一个面板。 使用边界的难点在于 Axure RP 9 在设计这个功能时的让人难以理解。这里简单的理解是： 对于“左侧”，$-35 &lt;= x &lt;= 0$ 对于“左侧”可以移动 0 像素或移动 -35 像素 对于“左侧”，可以不移动，或者向左移动 35 像素 用不熟练的时候多尝试 中继器 添加局部变量的方式： 首先点击添加局部变量 变量全局变量可以跨页面传参 判断 添加判断当添加交互事件时，不进一步选择动作，而是选择“启用情形”，在启用情形中添加判断。 局部变量获取元件位置，结合判断可以用 [[元件局部变量.x]] 获取元件的 x 位置 多条件判断多条件判断可以通过在“添加情形”部分添加行来增加判断条件，“匹配所有”和“匹配任何”两个情况，分别代表的是 and 和 or。 嵌套判断 案例：绘制注册 中间页","link":"/2024/04/12/Axure-RP-9-%E4%BA%A7%E5%93%81%E8%AE%BE%E8%AE%A1%E5%85%A5%E9%97%A8/"},{"title":"[20240412] 下一步计划","text":"[Date: 20240412] 下一步计划[TOC] 📕 计划 1: 学习 Axure RP 9[*注] 2024.04.20 该计划已经完成主要是为了计划2做准备 📕 计划 2: 开发计划开发一个好玩的小工具这个其实是一个奇思妙想啦，目前还不知道能不能实现，总的来说就是希望用户上传一张文件，然后我们转化成命令行图片。就例如我用的网站博客的框架，他能用符号绘制一个很好看终端欢迎界面，我觉得是一个很有意思的东西，所以我准备在接下来的一年时间里实现这个小工具。目前连文件夹都没有创建，哈哈哈 :P 12345678INFO ======================================= ██╗ ██████╗ █████╗ ██████╗ ██╗ ██╗███████╗ ██║██╔════╝██╔══██╗██╔══██╗██║ ██║██╔════╝ ██║██║ ███████║██████╔╝██║ ██║███████╗ ██║██║ ██╔══██║██╔══██╗██║ ██║╚════██║ ██║╚██████╗██║ ██║██║ ██║╚██████╔╝███████║ ╚═╝ ╚═════╝╚═╝ ╚═╝╚═╝ ╚═╝ ╚═════╝ ╚══════╝============================================= 使用 QT 6 开发一个好玩的桌面端应用刷抖音的时候看到了，一些很好玩的评论，因为大家好像觉得小红书是一个给女孩子交流的平台，所以男孩子也希望有一个自己交流的平台，大概就是小蓝书的样子。这也是个很大的项目了，需要很长时间设计，不过最起码要等我学完Axure RP 9 吧，当然肯定不是全部实现，我就是想边学边开发，能做多少是多少的样子。 📕 计划 3: 读书📕 计划 4: 复习数据结构与算法复习的列表会开一篇新博客来归档","link":"/2024/04/12/20240412-%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AE%A1%E5%88%92/"},{"title":"PyTorch快速上手指南","text":"PyTorch 深度学习框架快速上手指南PyTorch 可以说是目前最常用的深度学习框架 , 常应用于搭建深度学习网络 , 完成一些深度学习任务 (CV、NLP领域) 要想快速上手 PyTorch , 你需要知道什么 : 一个项目的完整流程 , 即到什么点该干什么事 几个常用 (或者说必备的) 组件 剩下的时间你就需要了解 , 完成什么任务 , 需要什么网络 , 而且需要用大量的时间去做这件事情 $^{(e.g.)}$例如 : 你现在有一个图像分类任务 , 完成该任务需要什么网络, 你需要通过查找资料来了解需要查找什么网络。 需要注意的是 , 有一些常识性的问题你必须知道 , 例如: 图像层面无法或很难使用机器学习方法 , 卷积神经网络最多的是应用于图像领域等 下面我将通过一个具体的分类项目流程来讲述到什么点该干什么事一个完整的 PyTorch 分类项目需要以下几个方面: 准备数据集 加载数据集 使用变换(Transforms模块) 构建模型 训练模型 + 验证模型 推理模型 准备数据集 一般来说 , 比赛会给出你数据集, 不同数据集的组织方式不同 , 我们要想办法把他构造成我们期待的样子 分类数据集一般比较简单, 一般是将某个分类的文件全都放在一个文件夹中, 例如: 二分类问题 : Fake(文件夹) / Real(文件夹) 多分类问题 : 分类 1(文件夹) / 分类 2(文件夹) / … / 分类 N(文件夹) 当然有些时候他们会给出其他方式 , 如 UBC-OCEAN , 他们将所有的图片放在一个文件夹中 , 并用 csv 文件存储这些文件的路径(或者是文件名) , 然后在 csv 文件中进行标注(如下): 以后你可能还会遇到更复杂的目标检测的数据集, 这种数据集会有一些固定格式 , 如 VOC格式 , COCO格式等 在数据集方面 , 需要明确三个概念——训练集、验证集和测试集 , 请务必明确这三个概念 , 这是基本中的基本 训练集(Train) : 字如其名 , 简单来说就是知道数据 , 也知道标签 的数据 , 我们用其进行训练 验证集(Valid) : 验证集 和 测试集 是非常容易混淆的概念 , 简单来说 , 验证集就是我们也知道数据和标签 , 但是我们的一般不将这些数据用于训练 , 而是将他们当作我们的测试集 , 即我们已经站在了出题人的角度 , 给出参赛者输入数据 , 而我们知道这个数据对应的输出 , 但是我们不让模型知道 测试集(Test) : 测试集就是 , 我们不知道输入数据的输出标签 , 只有真正的出题人知道 , 一般来说 , 我们无法拿到测试集 , 测试集是由出题人掌控的 需要注意的是 , 如果你通过某种途径知道了所有的测试集的标签时 , 不可使用测试集进行训练 , 这是非常严重的学术不端行为 , 会被学术界和工业界唾弃 1234567891011# 现在我们已经有了一个数据集 , 我将以 FAKE_OR_REAL 数据集为例 , 展示我们数据集的结构# D:\\REAL_OR_FAKE\\DATASET# ├─test --------- 测试集路径, 这里可以放你自己的数据, 你甚至可以将他们分类, 但是请注意, 实际情况下你只能通过这种方式来“得到”测试集# │ ├─fake ------ 你自己分的类, 开心就好# │ └─real ------ 同上# ├─train -------- 训练集路径, 这里面放的是题目给出的数据, 下面有 fake 和 real 两个文件夹, 这两个文件夹中就是两个类别, 我们要用这里面的图片进行分类 # │ ├─fake# │ └─real# └─valid -------- 验证集路径, 这里面放的是题目给出的数据, 下面有 fake 和 real 两个文件夹, 这两个文件夹中就是两个类别, 这里面的图片不需要进行训练# ├─fake# └─real 加载数据集 请务必记住 , 不管是什么数据集 , 数据集是如何构成的 , 在使用 PyTorch 框架时 , 我们都要像尽办法将他们加载入 Dataset 类中 简单来说 , Dataset 类就是描述了我们数据的组成的类 需要注意 , PyTorch 实现了许多自己的 Dataset 类 , 这些类可以轻松的加载特定格式的数据集 , 但是我强烈建议所有的数据集都要自己继承Dataset类 , 自行加载 , 这样我们可以跟清晰的指导数据集的组成方式 , 也可以使得我们加载任意格式的数据集 实现 DataSet 类需要我们先继承 Dataset 类 , 在继承 Dataset 类后, 我们只需要实现其中的__init__、__len__和__getitem__三个方法 , 即可完成对数据集的加载 , 这三个方法就和他的名字一样 : __init__ 方法是构造函数 , 用于初始化 __len__ 方法用于获取数据集的大小 __getitem__ 方法用于获取数据集的元素 , 我将从下面的代码中进行更详细的解释 有些数据集并不分别提供 Train训练集 和 Valid验证集, 我们可以使用 random_split() 方法对数据集进行划分 需要注意的是, 每次重新划分数据集时, 必须重新训练模型, 因为 random_split() 方法随机性, 划分后的数据不可能和之前的数据完全重合, 因此会导致数据交叉的情况, 下面一段使用 random_split() 进行划分的 Python 代码示例 : 12345678# 下面演示使用 random_split 来划分数据集的操作# 我们假设已经定义了 CustomImageDataSetsplit_ratio = 0.8 # 表示划分比例为 8 : 2dataset = CustomImageDataSet(fake_dir, real_dir) # 定义 CustomImageDataSet 类, 假设此时没有划分训练集和验证集train_dataset_num = int(dataset.lens * split_ratio) # 定义训练集的大小valid_dataset_num = dataset.lens - train_dataset_num # 定义验证集的大小# random_split(dataset, [train_dataset_num, valid_dataset_num]) 表示将 dataset 按照 [train_dataset_num: valid_dataset_num] 的比例进行划分train_dataset, valid_dataset = random_split(dataset, [train_dataset_num, valid_dataset_num]) 当数据集不是很大的时, 推荐人为的将数据集进行划分, 可以写一个 Python 脚本(.py) 或者 批处理脚本(.bat) 来完成这个操作 完整的数据集加载代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torchfrom torch.utils.data import Datasetimport osfrom PIL import Image# 这里我们定义了一个 CustomImageDataset(...) 类, 括号中的内容表示我们继承了 ... 类# 因此我们这里 CustomImageDataset(Dataset): 表示我们定义了一个“自定义图片”类, 这个“自定义图片”类继承自 Dataset 类class CustomImageDataset(Dataset): # 这里我们实现 __init__ 方法, __init__ 方法其实就是一个类的构造函数, 他也分有参构造和无参构造, 只是在这里我们说无参构造基本没啥意义 # 因此我们常常实现这个类, 使得可以指定这个类的输入输出 # 比如下面我们写的 def __init__(self, fake_dir, real_dir, transform=None): # self : 自己, 我一般直接理解为 this 指针, 如果有兴趣了解更深层的东西可以查阅一些资料, 这个是必填的 # fake_dir : 用于指定 fake 类型图片的位置的 # real_dir : 用于指定 real 类型图片的位置的 # transform : 用于指定变换, 简单来说就是对输入进行某些操作, 我会在下面的板块中进行详细叙述 def __init__(self, fake_dir, real_dir, transform=None): self.fake_dir = fake_dir # 这里表示这个类内定义了一个 fake_dir, 其值为传入的 fake_dir self.real_dir = real_dir # 这里表示这个类内定义了一个 real_dir, 其值为传入的 real_dir self.transform = transform # 这里表示这个类内定义了一个 transform, 其值为传入的 transform, 当没有传入时, 这个变量为 None self.fake_images = os.listdir(fake_dir) # 传入的 fake_dir 是一个路径, 我们使用 os.listdir(fake_dir) 可以加载 fake_dir 文件夹下的内容, 也就是所有 fake 图片 self.real_images = os.listdir(real_dir) # 传入的 real_dir 是一个路径, 我们使用 os.listdir(real_dir) 可以加载 real_dir 文件夹下的内容, 也就是所有 real 图片 self.total_images = self.fake_images + self.real_images # 总图片列表, 就是将 fake 图片列表和 real 图片列表进行组合 self.labels = [0]*len(self.fake_images) + [1]*len(self.real_images) # 对图片打标签, fake 为 0, real 为 1 # [0] * 10 得到的结果为 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # [1] * 10 得到的结果为 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # 这里我们实现 __len__ 方法, 这个方法用于获取数据集的大小 def __len__(self): return len(self.total_images) # 这里我们直接返回总图片列表的长度即可, 这里的实现方式不唯一, 只要能做到表示数据集大小即可 # 这里我们实现 __getitem__ 方法, 这个方法用于获取数据集中的某个元素 # 其中 idx 表示索引, 这个参数是必须的, 当然可以起其他名字, 不过最好还是使用 idx # __getitem__(self, idx) 表示获取 idx 位置的元素 def __getitem__(self, idx): # 这里表示获取一个元素的逻辑 # 当 idx 位置的标签为 0 时, 图片的路径为 fake_dir + self.total_images[idx], idx 即为图片的索引位置 # 当 idx 位置的标签为 1 时, 图片的路径为 real_dir + self.total_images[idx] image_path = os.path.join(self.fake_dir if self.labels[idx] == 0 else self.real_dir, self.total_images[idx]) # 使用 PIL 库加载图片, 通过 image_path 打开图片, 并且将图片转化为 RGB 格式 image = Image.open(image_path).convert('RGB') # 这里是 transform, 表示变换, 当其值为 None 时不进行操作, 当传入自己的 transform 时即为非空, 即对输入数据进行变换 if self.transform: # 我们将变换后的图片直接保存在原位置 image = self.transform(image) # 最后函数的返回值为 image 和 self.labels[idx], 即表示索引位置 idx 处的图片和标签 return image, self.labels[idx] 使用 Transforms 不要简单的使用原始图片进行训练 , 当然如果一定要使用原始图片进行训练, 也可以使用 transforms 模块 一般来说, 训练集和验证集的 transforms 是不同的, 因为我们希望验证集和测试集的图片贴合真实的情况 下面的代码演示了如何定义 transforms 在定义完 transforms 我们就可以完全定义我们的 Dataset 和 Dataloader 了 123456789101112131415161718192021222324252627282930import torchfrom torchvision import transforms# 定义transform# transforms.Compose(transforms) 实际上就是将多个 transform 方法变为逐步执行, 一般我们直接使用这种方式来对图片进行连续的变换train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(), # 随机水平翻转 transforms.RandomVerticalFlip(), # 随机垂直翻转 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # 改变图像的属性, 将图像的brightness亮度/contrast对比度/saturation饱和度/hue色相 随机变化为原图亮度的 10% transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # 对图片先进行随机采集, 然后对裁剪得到的图像缩放为同一大小, 意义是即使只是该物体的一部分, 我们也认为这是该类物体 transforms.RandomRotation(40), # 在[-40, 40]范围内随机旋转 transforms.RandomAffine(degrees=0, shear=10, scale=(0.8,1.2)), # 随机仿射变换 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 色彩抖动 transforms.ToTensor(), # [重点] 将图片转化为 Tensor 张量, 在 PyTorch 中, 一切的运算都基于张量, 请一定将你的输入数据转化为张量 # 请理解什么是张量 : 我们在线性代数中有向量的概念, 简单来说就是张量就是向量, 只不过张量往往具有更高的维度 # 而大家一般习惯将高于三维的向量称为张量, 某些人(比如我)也习惯所有的向量统称为张量 # 可以简单的将数组的维数来界定张量的维度 # 例如 [ ] 为一维张量, [[ ]] 为二维张量, [[[ ]]]为三维张量, [[[[ ]]]]为四维张量 # 对于图像来说, jpg 图像实际为三维矩阵, png 图像实际为四维矩阵, 这个维数是根据图像的通道数进行划分的 # 例如 jpg 有 R、G、B三个通道, png 具有 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 归一化, 可以对矩阵进行归一化 # 详细查看这个Blog : https://blog.csdn.net/qq_38765642/article/details/109779370 transforms.RandomErasing() # 随机擦除])valid_transform = transforms.Compose([ transforms.Resize((256, 256)), # Resize 操作, 将图片转换到指定的大小 transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) 12345678910111213141516171819from torch.utils.data import DataLoader# 定义 Dataset 实例train_dataset = CustomImageDataset(fake_dir=&quot;./dataset/train/fake&quot;, real_dir=&quot;./dataset/train/real&quot;, transform=train_transform)valid_dataset = CustomImageDataset(fake_dir=&quot;./dataset/valid/fake&quot;, real_dir=&quot;./dataset/valid/real&quot;, transform=valid_transform)# 创建 DataLoader 实例# 这里将要涉及到超参数的概念, 什么是超参数: 简单来将, 超参数就是我们自己能指定的一些数据, 超参数的选择将很大程度上影响模型的性能# 因此 深度学习领域的工程师 常称自己为 炼丹师、调参师等batch_size = 32 # batch_size 就是一个超参数, batch 即为 “批次”, 表示一次使用 DataLoader 加载多少张图片进行运算 # 这个数值并不是越大越好, 也不是越小越好, 但是往往大一些比较好, 这个数字最大能选择多大和你的图片大小和显卡显存有很大的关系 # 当出现 [Out Of Memery] 错误时往往表明你选取了过大的 batch_size, 导致显卡出现了爆显存的问题# batch_size : 每次训练时，模型所看到的数据数量。它是决定训练速度和内存使用的重要参数。# shuffle : 是否在每个训练周期之前打乱数据集的顺序。这对于许多模型（如卷积神经网络）是很有帮助的，因为它可以帮助模型避免模式识别。# sampler : 定义如何从数据集中抽样。默认情况下，它使用随机采样。但你可以使用其他更复杂的采样策略，如学习率调度采样。# batch_sampler : 与sampler类似，但它在批处理级别上进行采样，而不是在整个数据集上。这对于内存使用效率更高的场景很有用。# num_workers : 定义了多少个工作进程用于数据的加载。这可以加快数据加载的速度，但需要注意内存的使用情况。train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) 1234567891011# 查看Dataloader数据# 为了了解Dataloader中的数据, 我们可以使用以下方法来查看:# 使用 Python 的 len() 函数 : 我们可以直接通过 len() 函数获取 Dataloader 的长度, 即数据集中数据块的数量# 使用 torch.utils.data.DataLoader.len() 方法 : 这个方法也会返回Dataloader的长度。# 使用 iter() 函数：Dataloader是一个可迭代对象，我们可以直接通过iter()函数对其进行迭代，以获取每个批次的数据。# 使用torchvision.utils.save_image()函数 : 如果我们正在处理的是图像数据集，那么可以使用这个函数来保存Dataloader中的图像数据。 len(train_loader) # 401len(valid_loader) # 100images, labels = next(iter(train_loader))print(images)print(labels) 构建模型 构建模型是比较重要的一部分, 一般来说做好数据集之后, 最重要的事情就是修改模型, 通过训练结果改进模型, 判断自己的模型的正确性, 这里就是整个你要用到的神经网络的部分 , 需要注意的是 , 这里指定什么输入 , 推理的时候就要指定什么输入 简单用几个符号说明一下就是: $^{Train} model (inputX, inputY, …)$ → $^{Valid} model (inputX, inputY, …)$ 如何确定输入是什么: 看 forward() 的输入是啥模型的输入就是啥 我下面展现了我复现的 ResNet50 , 用这种方式可以顺便教你如何复现网络结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140import torch.nn as nnfrom torch.nn import functional as F# 这里是对 ResNet50 的实现, 请对照论文来进行对照阅读# 定义 ResNet50Basic类, 这里并不是完整的模型, 而是模型的一个部分class ResNet50BasicBlock(nn.Module): def __init__(self, in_channel, outs, kernerl_size, stride, padding): # super(ResNet50BasicBlock, self).__init__() 这里是干什么的? # 1. 首先找到 ResNet50BasicBlock 的父类, 这里是 nn.Module # 2. 把类 ResNet50BasicBlock 的对象self转换为 nn.Module 的对象 # 3. &quot;被转换&quot;的 nn.Module 对象调用自己的 init 函数 # 简单理解一下就是 : 子类把父类的 __init__ 放到自己的 __init__ 当中, 这样子类就有了父类的 __init__ 的那些东西 super(ResNet50BasicBlock, self).__init__() # 这里只是定义部分, 在这里的定义并不一定会在推理过程中使用 self.conv1 = nn.Conv2d(in_channel, outs[0], kernel_size=kernerl_size[0], stride=stride[0], padding=padding[0]) self.bn1 = nn.BatchNorm2d(outs[0]) self.conv2 = nn.Conv2d(outs[0], outs[1], kernel_size=kernerl_size[1], stride=stride[0], padding=padding[1]) self.bn2 = nn.BatchNorm2d(outs[1]) self.conv3 = nn.Conv2d(outs[1], outs[2], kernel_size=kernerl_size[2], stride=stride[0], padding=padding[2]) self.bn3 = nn.BatchNorm2d(outs[2]) # 输入是啥看 forward(), 例如这里是 forward(self, x), 则表示输入是 x, 也就是一个 def forward(self, x): # nn.Conv2d 是卷积层, 请了解[1]什么是卷积层, 以及[2]卷积层是干啥用的, [3]卷积后会变成什么 # 卷积运算的目的是提取输入的不同特征, 第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级, 更多层的网路能从低级特征中迭代提取更复杂的特征 out = self.conv1(x) # [*] 什么是 ReLU, ReLU是激活函数, 请了解 [1]什么是激活函数, [2]为什么要使用激活函数 # [*] 什么是 Batch Normalization层, BN 层是批次归一化层 out = F.relu(self.bn1(out)) out = self.conv2(out) out = F.relu(self.bn2(out)) out = self.conv3(out) out = self.bn3(out) return F.relu(out + x)# 定义 ResNet50DownBlock类, 这里并不是完整的模型, 而是模型的一个部分class ResNet50DownBlock(nn.Module): def __init__(self, in_channel, outs, kernel_size, stride, padding): super(ResNet50DownBlock, self).__init__() self.conv1 = nn.Conv2d(in_channel, outs[0], kernel_size=kernel_size[0], stride=stride[0], padding=padding[0]) self.bn1 = nn.BatchNorm2d(outs[0]) self.conv2 = nn.Conv2d(outs[0], outs[1], kernel_size=kernel_size[1], stride=stride[1], padding=padding[1]) self.bn2 = nn.BatchNorm2d(outs[1]) self.conv3 = nn.Conv2d(outs[1], outs[2], kernel_size=kernel_size[2], stride=stride[2], padding=padding[2]) self.bn3 = nn.BatchNorm2d(outs[2]) self.extra = nn.Sequential( nn.Conv2d(in_channel, outs[2], kernel_size=1, stride=stride[3], padding=0), nn.BatchNorm2d(outs[2]) ) def forward(self, x): x_shortcut = self.extra(x) out = self.conv1(x) out = self.bn1(out) out = F.relu(out) out = self.conv2(out) out = self.bn2(out) out = F.relu(out) out = self.conv3(out) out = self.bn3(out) return F.relu(x_shortcut + out)class ResNet50(nn.Module): def __init__(self): super(ResNet50, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # Sequential 类是 torch.nn 模块中的一个容器, 可以将多个层封装在一个对象中, 方便顺序连接 self.layer1 = nn.Sequential( ResNet50DownBlock(64, outs=[64, 64, 256], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(256, outs=[64, 64, 256], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(256, outs=[64, 64, 256], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ) self.layer2 = nn.Sequential( ResNet50DownBlock(256, outs=[128, 128, 512], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50BasicBlock(512, outs=[128, 128, 512], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(512, outs=[128, 128, 512], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(512, outs=[128, 128, 512], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.layer3 = nn.Sequential( ResNet50DownBlock(512, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50BasicBlock(1024, outs=[256, 256, 1024], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50BasicBlock(1024, outs=[256, 256, 1024], kernerl_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(1024, outs=[256, 256, 1024], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.layer4 = nn.Sequential( ResNet50DownBlock(1024, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 2, 1, 2], padding=[0, 1, 0]), ResNet50DownBlock(2048, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]), ResNet50DownBlock(2048, outs=[512, 512, 2048], kernel_size=[1, 3, 1], stride=[1, 1, 1, 1], padding=[0, 1, 0]) ) self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1, ceil_mode=False) # self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) self.fc = nn.Linear(2048, 10) # 使用卷积代替全连接 self.conv11 = nn.Conv2d(2048, 10, kernel_size=1, stride=1, padding=0) def forward(self, x): out = self.conv1(x) out = self.maxpool(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) # avgpool 平均池化层, 了解什么是平均池化层 out = self.avgpool(out) out = self.conv11(out) out = out.reshape(x.shape[0], -1) # out = self.fc(out) return out# 这里展现了对 ResNet 的一个具体的应用# x = torch.randn(1, 3, 224, 224) # 这个是我们 ResNet50 期待的输入样子, 可以看到他是 [1] 个 [3] 通道, 宽度为[224], 高度为 [224]的张量 image_path = './dataset/test/fake/test_fake_1.png'image = Image.open(image_path).convert('RGB') # 图片加载transform = transforms.ToTensor() # 将图片转化为张量, 此时的 张量的形状为[3, 1024, 1024]# 当输入数据的维度不足时, 我们可以通过 unsqueeze() 添加维度, 这个东西简单理解一下就是, 在某个维度外面加括号[], 即可拓展出更高的维度img_tensor = transform(image).unsqueeze(dim=0)# print(x.shape) 我们可以使用 shape 来查看一个张量的形状# print(img_tensor.shape)# 这里加载我们的网络架构net = ResNet50()# 这里进行输入, 输入 img_tensor, 进入 forword() 部分, 然后得到最终输出的结果out = net(img_tensor)print(out) 1234567891011121314151617181920212223242526import torchfrom torchvision import models# 这里为了方便, 我们直接加载 PyTorch 预训练好的 ResNet50 的模型# PyTorch 已经为我们提供了不少已经预训练好的模型, 我们只需要加载他们与训练好的模型即可# 但是我还是希望你可以掌握上面这种自定义模型的方法, 这样遇到 PyTorch 未提供的模型, 我们也可以尝试自己实现该模型model = models.resnet50(pretrained=True)# 冻结参数 : 即不更新模型的参数# 可以看到下面的代码, 这里表示冻结了所有层for param in model.parameters(): param.requires_grad = False# 但是我们可以通过替换层来接触某些层的冻结num_ftrs = model.fc.in_features # 这里是获取 ResNet50 的 fc 层的输入特征数model.fc = torch.nn.Linear(num_ftrs, 2) # 这里是对 fc 层进行修改, Linear(input_feather_num, output_feather_num) # 这里输入特征数是 num_ftrs, 输出特征数为 2 # 这一行很重要, 指定了模型的位置, cuda 可以理解为 GPU 设备, cuda: 0 表示使用编号为 0 的GPU进行训练# 当有多块 GPU 时, 可以用其他的方式指定 GPU# model = torch.nn.DataParallel(model, device_ids=[0, 1, 2]), 当然向我们这种小白(穷B), 当然还是单卡为主# 为了避免出现多卡的情况, 我在下面放入两篇博客, 有兴趣可以参考这两篇文章进行多卡训练# https://zhuanlan.zhihu.com/p/102697821# https://blog.csdn.net/qq_34243930/article/details/106695877device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model = model.to(device) 训练模型 + 验证模型 这里需要直接对模型进行训练 , 一般来说 , 在训练的过程中我们会加入 tqdm 库使得训练过程可视化 , 有时我们还会在训练过程中保存更好的训练结果 , 并且设置断点训练等操作 , 我只使用最简单的方式进行预测 train 部分的代码因人而异, 基本上每个人的写法都可能不同, 没有固定的写法 对于训练完的模型我们需要对其进行评价, 一般来说, 训练和验证都是放在一起的, 不可分开的 记得保存一下训练后的模型, 使用如下代码保存/加载整个模型123456# 保存模型model_path = &quot;xxxx.pth&quot; # xxxx 表示一个你喜欢的名字torch.save(model, model_path) # 使用 torch.save(model, model_path) 保存模型# 加载模型model = torch.load(model_path) # 使用 torch.load(model_path) 即可加载模型 完整的”训练模型 + 验证模型”代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from tqdm import tqdmfrom sklearn.metrics import f1_score, accuracy_score# 定义损失函数和优化器# 这里包含了 PyTorch 的 19 种损失函数 https://blog.csdn.net/qq_35988224/article/details/112911110criterion = torch.nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters())# 计算 F1 值和 准确率def evaluate(loader, model): preds = [] targets = [] loop = tqdm(loader, total=len(loader), leave=True) for images, labels in loop: images, labels = images.to(device), labels.to(device) with torch.no_grad(): outputs = model(images) _, predicted = torch.max(outputs, 1) preds.extend(predicted.cpu().numpy()) targets.extend(labels.cpu().numpy()) # Update the progress bar loop.set_description(&quot;Evaluating&quot;) return f1_score(targets, preds), accuracy_score(targets, preds)# 训练循环best_f1 = 0.0loss_values = []num_epochs = 10 # 定义训练的轮次for epoch in range(num_epochs): model.train() # 将模型设置为训练模式 loop = tqdm(train_loader, total=len(train_loader), leave=True) print(loop) for images, labels in loop: images, labels = images.to(device), labels.to(device) # 前向推理 outputs = model(images) loss = criterion(outputs, labels) # 反向传播及优化 # 在用 PyTorch训练模型时, 通常会在遍历 Epochs 的过程中依次用到 # optimizer.zero_grad() : 先将梯度归零 # loss.backward() : 反向传播计算得到每个参数的梯度值 # optimizer.step() : 通过梯度下降执行一步参数更新 # 对于这三个函数, 这篇博客写的很好 : https://blog.csdn.net/PanYHHH/article/details/107361827 # 可以简单阅读一遍 optimizer.zero_grad() loss.backward() optimizer.step() # 保存该批次的损失 loss_values.append(loss.item()) # 更新进度条 loop.set_description(f&quot;Epoch [{epoch + 1}/{num_epochs}]&quot;) loop.set_postfix(loss=loss.item()) # 在每轮之后验证模型 model.eval() # 将模型设置为推理模式, 此时模型中的参数不会进行更新, 即完全用于推理/验证 f1_value, accuracy = evaluate(valid_loader, model) print(f'F1 score: {f1_value:.4f}, Accuracy: {accuracy:.4f}') # 保存 F1 值最高的模型 if f1_value &gt; best_f1: best_f1 = f1_value # 这里和上面 Markdown 的保存方式不同, model.state_dict(), 表示模型的参数, 简单来说呢我们仅仅保存了模型的参数, 但是我们并没有保存模型的结构 # 上面 Markdown 的保存方式是即保存了整个模型的结构, 也保存了模型的参数 torch.save(model.state_dict(), 'best_model.pth')print('训练结束') 当然我们也可以使用绘图函数，来展示过程中的相关数据。 12345678910import matplotlib.pyplot as pltplt.figure(figsize=(12, 8))plt.plot(loss_values, label='Train Loss')plt.title('Loss values over epochs')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.grid(True)plt.show() 推理模型 很高兴, 如果你到这一步, 你的水平肯定已经有了质的飞跃, 这里已经是最后一步了, 结束这个部分, 你就要开始自己的探索之路了 推理模型很简单, 我在上面说过, 构造模型时指定什么输入 , 推理的时候就要指定什么输入, 这里就是对应的部分了 1234567891011121314151617181920212223242526272829303132from torchvision.transforms import ToTensor, Resize, Normalize# predict_by_file 表示推理一个文件, 我们需要传入文件路径以及模型def predict_by_file(file_path, model): # image = Image.open(file_path).convert('RGB') # 这里的 transform 有与没有都无所谓, 纯看心情 transform = transforms.Compose([ Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) image = transform(image) # 这里和上面一样, 表示在最外面加一层括号, 使 [3, 256, 256] 变为 [1, 3, 256, 256] image = image.unsqueeze(0).to(device) model.eval() with torch.no_grad(): outputs = model(image) # 模型推理 # torch.max(...) # input (Tensor) – 输入张量 # dim (int) – 指定的维度 _, predicted = torch.max(outputs, 1) # 返回指定维度的最大值, 其实这里只有一维 print(outputs) # tensor([[0.7360, 0.2668]], device='cuda:0') print(outputs.shape) # torch.Size([1, 2]) return &quot;Fake&quot; if predicted.item() == 0 else &quot;Real&quot;path = './dataset/test/real/test_real_7.jpg'print(predict_by_file(path, model))","link":"/2024/03/20/PyTorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"title":"Git常用指令","text":"[TOC] Git 常用命令 初始化仓库:git init 查看当前仓库的状态git status 切换分支 切换到已有 branchName 分支git checkout branchName 创建新分支，创建的同时切换到该分支git checkout -b newBranch 查看所在目录的分支git branch -a 拉取请求git pull 从 Gitee 拉取请求git pull 上传到远程仓库(GitHub) 在 GitHub 配置 SSH_KEY 查看本地是否具有已存在 ssh_key : ll ~/.ssh 如果不存在则生成 ssh_key : ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot; 如果存在则复制 ssh_key 的内容 : cat ~/.ssh/id_rsa.pub 在 GitHub 上添加公钥 : Settings → SSH and GPG Keys → New SSH Key 验证是否成功 : ssh -T git@github.com 克隆远程仓库 : git clone &lt;SSH_Addr&gt; 添加远程仓库地址:git remote add origin &lt;git_addr&gt; 添加文件 git add &lt;files&gt; 添加注释 git commit -m &quot;f&quot; 推送请求 git push -u origin main Git 常见错误解决方案Failed To Push Some Refs To … 参考链接问题原因：远程库与本地库不一致，在 hint 中也有提示把远程库同步到本地库就可以了解决办法：使用命令行： 1git pull --rebase origin main","link":"/2024/03/26/Git%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/03/17/hello-world/"},{"title":"VSCode C++ 的安装和配置","text":"VSCode C++ 的安装和配置 下载 VSCodeVSCode的安装地址 : https://code.visualstudio.com/ 安装和配置 Remote SSH 连接远程的 Linux 服务器 在 VSCode 安装插件 Remote-SSH 点击 左下角→连接到主机→配置SSH主机→选择要更新的SSH的配置文件(C:\\Users\\UserName\\.ssh\\config) 配置文件123Host 服务器的 IP 地址HostName 服务器的 IP 地址User 用户名 连接远程服务器, 按照要求选择操作系统, 输入用户名和密码等即可(第一次连接会较慢) 配置免密登录 Windows 系统下查看是否具有 ssh-key : cd C:/Users/UserName/.ssh 如果没有 ssh-key 则生成 ssh-key : ssh-keygen -t rsa -b 4096 如果有 ssh-key 则上传至 Windows : scp id_rsa.pub UserName@IPAddr:FilePath 在 Linux 系统下进行签名: cat pubPath &gt;&gt; authorized_keys","link":"/2024/03/27/VSCode-C-%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"},{"title":"在 Hadoop 高可用的基础上搭建 HBase 高可用","text":"在 Hadoop 高可用的基础上搭建 HBase 高可用当Hadoop高可用搭建完成后，需要进一步再Hadoop高可用集群上搭建HBase高可用时，过程如下： 前提说明： 我的集群为三台机器，每台机器上都有ZooKeeper，使用用户名和主机名（Username@Hostname）分别如下： 123master@mastermaster@slaver01master@slaver02 首先保证 ZooKeeper 正常部署 1zkServer.sh start 需要保证Hadoop正常部署 12[master@master ~]$ start-dfs.sh[master@slaver01 ~]$ start-yarn.sh 解压HBase 配置环境变量 生效环境变量 修改配置文件 修改 hbase-site.xml 文件 123456789101112131415161718192021222324252627282930313233343536&lt;configuration&gt; &lt;!-- HBase数据在HDFS中的存放的路径 --&gt; &lt;!-- 这里 ns 是 Hadoop 的 nameservice的值, 指向的是一个高可用的通道 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase 的运行模式 --&gt; &lt;!-- false是单机模式, 若为 false, HBase 和 ZooKeeper 会运行在同一个 JVM 里面 --&gt; &lt;!-- true是分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slaver01,slaver02&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper快照的存储位置 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.6/data&lt;/value&gt; &lt;/property&gt; &lt;!-- V2.1版本，在伪分布式情况下, 设置为 false --&gt; &lt;!-- 当使用 hdfs 时, 设置为 true --&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 需要注意的是： 这里使用了ns这个高可用通道，因此需要将Hadoop的 core-site.xml 与 hdfs-site.xml移动到/opt/module/hbase-2.1.0/conf下 修改 regionserver 文件 123masterslaver01slaver02","link":"/2024/05/04/Hadoop2HBase/"},{"title":"Hash","text":"哈希(Hash)[TOC] 哈希的基本概念 哈希（Hash）在我的理解中是一种映射关系，例如，将学生映射到学号上、将口红的颜色映射到一个色号上。 哈希函数（Hash Function）就是将一种你想要查询的关键字，比如说姓名、手机号码、数字或者字符串等，映射成便于查找的东西(一般来说是一个数字)的函数。 一般而言，一个好的哈希函数可以帮我们将我们想要查找的东西，从一个较大集合映射到一个较小集合，然后我们可以将这个较小集合中的信息存储下来，例如存入一个数组，这个用于存储较小集合的数组就称之为哈希表。 一张格式如下的表： 学号 姓名 0001 张三 0002 李四 0003 王五 0004 赵六 … … 这张表就可以理解为一个姓名和学号的哈希表，我们通过学号就可以获得学号对应的人的姓名，即：学号[0001] -&gt; &quot;张三&quot;，反映到代码中，可以理解为一个一维数组通过下标直接访问。 而在一些加密工作中，可能需要将要简单的组合复杂化，比如密码组合，这时会有一种类似反向哈希（加密）的过程。 比较常见的哈希函数的例子：$$H(x) = x ; mod ; 11$$ 这个函数可以让我们将任意的整数映射到 0 ~ 10 的范围中 哈希的基本操作 定义哈希函数 1234567const int modnum = 11;int hashTable[modnum];// 定义哈希函数int hash(int x) { return x % modnum;} 在哈希表中插入元素 12345// 在哈希表中插入元素void insert(int x) { int addr = hash(x); hashTable[addr] = x;} 在哈希表中查找元素 12345// 在哈希表中查找元素bool isExist(int x) { int addr = hash(x); return hashTable[addr] == x;} 哈希表中解决冲突的方式不同关键字通过相同哈希函数计算出相同的哈希地址，该种现象称为哈希冲突或哈希碰撞，或者称为哈希冲突。哈希表在存放数据时有可能出现冲突的情况，以上文中的哈希函数为例我们分别向其中插入元素，如下：因为希望哈希表底层数组的容量小于实际要存储的关键字的数量，这就会导致一个问题：冲突的发生是必然的，我们能做的是尽量的降低冲突率。 冲突的解决方式一般有以下两种： 方式 1：顺延一种很好想到的解决方式是将哈希表中插入时产生冲突的元素向后顺延，直到找到一个空位置再进行插入。这种方式称为顺延，也有说法称之为线性探测。此时插入和查找的代码也要发生相应的改变，插入时需要我们需要找到一个空位置来执行插入操作；相对应的查找方式也要做出改变，当我们查询一个数时，也要查询哈希函数对应的位置，并依次比较连续的非空的哈希表中的值： 插入操作 1234567void insert(int x) { int addr = hash(x); while(hashTable[addr] NOT NULL) { // 当哈希表的插入位置不为空 addr = (addr + 1) % modnum; } hashTable[addr] = x;} 查找操作 12345678910void isExist(int x) { int addr = hash(x); while(hashTable[addr] NOT NULL) { // 当哈希表的查询位置不为空(查询一段连续的哈希表) if(hashTable[addr] == x) { // 如果查询到指定元素, 返回 true return true; } addr = (addr + 1) % modnum; } return false;} 可以定义多种解决冲突的顺延函数，即addr = (addr + 1) % modnum，实际使用中可以是每次$+k$，或者$+1^2$，$+2^2$，$+3^2$，…等。 但是这种顺延方式会存在一定的问题：插入时可能会出现多次冲突，当哈希表即将满的时候，插入操作的冲突可能会出现的更多，此时插入和查询操作都会变成一个非常耗时的操作。 方式 2：哈希链表我们可以通过链表的方式，来实现在一个位置放置多个元素的操作。在 C++ 的 STL 库中，我们可以使用 STL 库中提供的 vector 来简单的替代链表。通过这种方式，每次查找元素时，先找到对应的链表头，然后遍历这个位置的整张链表即可。 此时的哈希表的定义、插入操作和查询操作要发生相应的变化： 定义哈希函数 12345678910#include &lt;iostream&gt;#include &lt;vector&gt;const int modnum = 11;vector&lt;int&gt; hashTable[modnum];// 定义哈希函数int hash(int x) { return x % modnum;} 插入操作 1234void insert(int x) { int addr = hash(x); hashTable[addr].push_back(x);} 查询操作 12345678910111213void isExist(int x) { int addr = hash(x); int tableSize = hashTable[addr].size(); // 这里不使用 for(int i = 0; i &lt; hashTable[addr].size(); i++) 的写法, 而是首先计算出 hashTable[addr].size() // 因为 vector 的 size() 是一个比较耗时的操作, 他是通过将 vector 按照一个一个数出来的方式来进行计数的 // 在数据量小的时候可能并不明显, 当数据量大的时候可能就会出现较为严重的耗时问题 for(int i = 0; i &lt; tableSize; i++) { if(hashTable[addr][i] == x) { return true; } } return false;} 但是这种方式还是不能彻底解决我们的问题。对于插入操作来说，时间复杂度可以看作是$O(1)$，对于查询操作来说，时间复杂度和其冲突次数相关联。 哈希函数的设计面对上面的问题，设计好哈希函数才是解决问题的关键。哈希函数在设计的时候，一般要注意几个原则： 设计哈希函数时，我们需要尽可能让查询的值均匀地存储在哈希表中，或者说尽量分散再哈希表中。 在手搓哈希函数时，我们会要求 $H(x) = x ; mod; p$，其中的 $p$ 为素数 哈希函数中的对 $p$ 取摸的操作，会使得哈希值落在 $0 &lt;= value &lt;= p-1$ 的范围内，这个哈希表的长度 $p$，一般被称为哈希表的容量（Capacity）。 插入哈希表的元素总数除以哈希表的容量得到的数，称为负载因子，这里可以用 $\\alpha$ 表示，即：$$\\alpha = ElumNum \\div p$$ 当负载因子$\\alpha$达到一定程度时（一般认为是$0.7\\sim0.8$），则说明再对哈希表继续进行操作，就会面临大量冲突的情况，这时就要考虑增大哈希表的容量以避免出现更多的冲突。 哈希函数的冲突率和负载因子的关系一般如下： 字符串哈希字符串哈希是学习或者工作中经常遇到的一种操作，常用于比较两组字符串的内容等操作。通过比较两个字符串的哈希值就可以完成字符串的比较。当然这个操作也可以用于数组的哈希，因为字符串本质就是一个字符数组。$$s = s_1s_2s_3 \\dots s_n\\qquad s_i \\in a, b \\dots z$$ 一个字符串 $s$ 由 $n$ 个字符组成，每个字符 $s_i$ 属于 $a \\sim z$。其哈希函数为：$$\\begin{aligned}H(S)&amp;=(\\sum_{i=1}^{n} c_i × base^{n-i});mod ;p\\&amp;=(c_1 × base ^ {n-1} + c_2 × base ^ {n-2} + \\dots + c_{n-1} × base ^ {1}) ; mod ; p\\&amp;=base^{n-(n-1)}(base\\dots(base(base × c_1 + c_2)));mod ;p\\&amp;=base^{1}(base\\dots(base(base × c_1 + c_2)+c_3)+\\dots + c_n)\\end{aligned};mod ;p$$ 其中 $c_i$ 是一个和 $s_i$ 有关的数字，我们可以将字符映射到数字，例如：$a → 1$、$b → 2$ 等。这里不将 a 映射为 0，因为如果将 a 映射为 0，字符串 a 和 ab 的哈希值是相等的。$base$ 是一个可以自己指定的数字，其值一般是大于字符集中的字符数量（$c_i$的最大值）的素数，这里可以取 31，常见的选择是 9999971。$p$ 是一个素数，常见的选择是 101 或 137。 用代码实现为： 12345678int hash(char s[], int n) { int res = 0; for(int i = 0; i &lt; n; i++) { // 为什么是 res * base, 见上文的描述的公式推导 res = (res * base + (s[i] - 'a' + 1)) % p; } return res;} 当 $base$ 为 31，$p$ 为 101 时。当 $s$ 为 $a$ 时，hash(char s[], int n) 的值为 1。过程可以描述如下： 12[1] res = (0 * base + ('a' - 'a' + 1)) % p&gt;&gt;&gt; res = 1 当 $s$ 为 $ab$ 时，hash(char s[], int n) 的值为 1。过程可以描述如下： 1234[1] res = (0 * base + ('a' - 'a' + 1)) % p&gt;&gt;&gt; res = 1[2] res = (1 * base + ('b' - 'a' + 1)) % p&gt;&gt;&gt; res = 33 当我们定义好一个良好的哈希函数之后，因为哈希函数的值相等的概率比较小，当两个字符串的哈希值相同的时候，我们可以认为两个字符串也相同，从而避免使用按位比较，节约时间。 $base$ 为什么一般是一个大于字符集中的字符数量（$c_i$的最大值）的素数呢？当 $base$ 值过小时，此处假定为 $11$，会出现有可能两个字符运算完后还没有超过字母表的范围。如：字母表的范围为 $1 \\sim 26$，字符串 ba 运算完成后的结果为 $23$，两个字符串的运算结果小于 $26$，字符串w运算完成后的结果为 $23$，可以发现 ba 和 w 出现了冲突。 我们字符串的定义方式有一种好处，可以使得我们借助类似前缀和的思想，快速得到任意一段字符串 $s$ 的字串的哈希值。 我们可以使用一个数组 $a$ 来记录我们计算 hash(s) 的中间过程，即：1234a[1] = hash(c1)a[2] = hash(c1c2)...a[i] = hash(c1c2...ci) 那么字符串 $s$ 的任意子串 $s_{l, r} = s_ls_{l+1}\\dots s_r$ 的哈希值为：$$hash(s_{l, r}) = (a[r] - a[l-1] × base^{r - l + 1});mod;p$$ 双哈希冲突往往是不可避免的，纵使我们的哈希函数再好，也有可能会出现冲突的情况，那么我们如何尽可能避免这个问题呢？这时候，我们可以使用双哈希的方法来避免冲突。在上文字符串哈希中，我们的字符串哈希函数为： $$H(S) = (\\sum_{i=1}^{n} c_i × base^{n-i});mod ;p$$ 那么如何实现双哈希呢？我们可以选取两组 $base$ 和 $p$，然后使用这分别使用这两组 $base$ 和 $p$，分别来求取哈希值。如果这两组参数得到的哈希值都相同，我们则认为两个字符串相等。在竞赛中，最好书写双哈希，甚至三哈希，因为这样出错误的概率会呈指数级的降低。 STL中的哈希 unordered_map在 C++ 的STL库中，有一个非常好用的数据结构 unordered_map，可以帮助我们实现大多数哈希表的操作。 1unordered_map&lt;K, V&gt; hash_table; 其中 K 为我们想要存储的关键字信息，V 表示与他关联的一些信息。例如： 1unordered_map&lt;string, int&gt; hash_table; 这个表会以string作为K，int作为V进行存储，基于此我们可以很方便的实现一些功能。例如我们现在要保存一个班级上学生的年龄信息: 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;int query(string name) { if(hash_table.find(name) == hash_table.end()) { cout &lt;&lt; &quot;Cannot Find...&quot; &lt;&lt; endl; } return hash_table[name];}int main(void) { unordered_map&lt;string, int&gt; hash_table; hash_table[&quot;zhangsan&quot;] = 2000; hash_table[&quot;lisi&quot;] = 2001; query(&quot;zhangsan&quot;); return 0;}","link":"/2024/05/05/Hash/"},{"title":"栈(Stack)","text":"栈(Stack)栈的基本概念 栈是一种 先进后出(First in Last Out, FILO) 的数据结构，其类似于我们生活中的衣娄，衣服只能从最顶部放入，也只能从最顶部拿出，要想拿出中间的某件衣服，就需要将顶部的衣服全部拿出，再进行后续的操作。 衣娄对应到栈上，就是以下的概念： 栈(Stack) 是一种类似于衣娄的数据结构，我们可以向其内部存入或者取出数据 栈按照 先进后出 的原则存储数据，每次新进入的数据都会被放在最上面，越先进入的越靠下，越后进入的数据越靠上。 我们只能对最上面的数据进行操作 栈的两大元素：栈的大小和栈顶指针Top（该指针指向栈最顶部的位置） 栈的基本操作 新建栈(题目简单时可以用数组模拟栈) 插入数据 删除栈顶数据 查询栈顶数据 清空栈","link":"/2024/05/04/Stack/"},{"title":"复习数据结构&#x2F;算法清单","text":"复习数据结构清单有日期则表示在该日期已经复习完毕，或者表示复习后的最新一次更新 数据结构 链接 备用链接(Backup Link) 日期 栈(Stack) - - - 队列(Queue) - - - 链表(Link List) - - - 二叉树(Binary Tree) https://hello-nilera.com/2024/05/07/Tree/ https://blog.csdn.net/NilEra/article/details/138624929?spm=1001.2014.3001.5501 2024.05.09 哈希(Hash) - - - 算法复习清单 算法 链接 备用链接(Backup Link) 日期","link":"/2024/05/04/ReviewAlgorithm/"},{"title":"Heap","text":"堆（Heap）[TOC] 堆的基本概念二叉堆的基本概念和基本性质堆是一种树形结构，有二叉树就有二叉堆。 二叉堆总是一棵完全二叉树 堆中某个结点的值总是不大于或不小于其父结点的值 根节点的值为整个堆中的最小/最大值。 父节点中的值大于等于两个子节点中的值，根节点的值最大的堆称为大根堆。 父节点中的值小于等于两个子节点中的值，根节点的值最小的堆称为小根堆。 我们可以在堆中插入和删除元素，然后通过调整元素的位置来维护堆的性质。 堆的操作 堆的初始化在建立堆之前，需要初始化一些东西： 一个空数组，用于存储堆中的元素 一个记录堆中元素个数的变量123const int MAXSIZE = 10000;int len = 0;int heap[MAXSIZE + 1]; 堆中元素的插入堆在插入时，需要首先将插入元素放在数组末尾，然后插入元素不断的和其父节点比较，直到位置合适。下面是对小根堆插入过程的模拟：小根堆插入的代码如下： 12345678910111213141516void insert(int x) { heap[++len] = x; up(len);}void up(int k) { while(k &gt; 1 &amp;&amp; heap[k] &lt; heap[k/2]) { swap(heap[k], heap[k/2]); k /= 2; }}int main(void) { insert(x); return 0;} 大根堆和小根堆插入元素的方式基本相同，只需要改变大于/小于符号，插入操作的时间复杂度为 $O(log;n)$ 堆顶元素的删除堆最常用的功能就是维护最小/最大值。在使用小根堆时，我们经常会求得最小的数字，然后让它出堆，这时就要从堆中删除堆顶数据。这时除了堆顶为空，它的左子树堆和右子树堆仍满堆结构。为了操作简单，一般选择将堆尾部元素放到堆顶，然后将其逐步下移的方式，下移时，如进行交换操作，交换的是该节点左右儿子中较小的一个与该节点。下图模拟小根堆删除堆顶元素的操作：小根堆删除元素的代码如下： 123456789101112131415void pop(void) { swap(heap[1], heap[len]); len--; down(1);}void down(void) { while(k * 2 &lt;= len) { int j = k * 2; if(k*2+1 &lt; len &amp;&amp; heap[j+1] &lt; heap[j]) j++; if(heap[k] &lt;= heap[j]) break; swap(heap[k], heap[j]); k = j; }} 堆中任意位置的删除删除堆中的任意一个元素时，我们可以发现这个时候这个元素下的左子树堆和右子树堆也满足堆结构，但是我们不可以像删除堆顶节点一样，和数组尾部元素互换，然后尝试下移，原因如下：此时无需向下调整，因为 $5&lt;8$ 且 $5&lt;9$，依旧满足小根堆的性质，但是其父节点 $6&gt;5$，破坏了小根堆的性质，因此此时需要上移。所以我们删除堆中的任意一个元素，跟数组尾元素互换时，不仅要考虑下移，还有可能会上移。小根堆中删除一个位于数组位置 pos 的元素的代码如下:1234567891011121314151617void deleteElem(int pos) { if (pos == len) { heap[len] = 0; len--; return; } int x = heap[pos], y = heap[len]; swap(heap[pos], heap[len]); len--; if (y &lt; x) { // 堆尾的数比原数大, 尝试上移 up(pos); } else { // 堆尾的数比原数小, 尝试下移 down(pos); }} STL 中的 priority_queue(优先队列)STL 库中的 priority_queue 是一个很类似于堆的结构，它包含如下操作： empty - 判断是否为空 size - 返回队列内元素个数 top - 访问队首元素 push - 往队列中插入一个元素 pop - 弹出队首元素 这里的 priority_queue 相当于堆，队首元素相当于堆顶元素。我们可以使用如下语句创建一个小根堆： 1priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 这段C++语句创建了一个优先队列 q，其中元素类型为 int，底层容器使用 vector&lt;int&gt;，并且使用 greater&lt;int&gt; 作为比较器。在优先队列中，当元素被插入队列时，会根据比较器的规则进行排序，从而实现堆的性质。 在这段语句中，greater&lt;int&gt; 是一个函数对象，代表使用“大于”运算符进行比较。因此，当要创建一个小根堆时，即希望队列中的元素按照从小到大的顺序排列，可以利用 greater 作为比较器，这样队列中的最小元素将位于队首。同样的，我们可以使用如下语句创建一个大根堆： 1234// 写法 1：priority_queue&lt;int&gt; q;// 写法 2：priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt; q; 当使用 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; 创建一个小根堆后，可以按以下方式操作这个小根堆： 插入元素：使用 push() 方法将元素插入小根堆。 123q.push(5); // 插入元素5q.push(3); // 插入元素3q.push(7); // 插入元素7 获取堆顶元素：使用 top() 方法获取小根堆的头部元素。 12int topElement = q.top(); // 获取小根堆的头部元素cout &lt;&lt; &quot;Top element of the min heap: &quot; &lt;&lt; topElement &lt;&lt; endl; 删除堆顶元素：使用 pop() 方法删除小根堆顶部的元素。 1q.pop(); // 删除小根堆的头部元素 查看堆是否为空：使用 empty() 方法检查小根堆是否为空。 12345if (q.empty()) { cout &lt;&lt; &quot;Min heap is empty&quot; &lt;&lt; endl;} else { cout &lt;&lt; &quot;Min heap is not empty&quot; &lt;&lt; endl;}","link":"/2024/05/07/Heap/"},{"title":"Tree","text":"树(Tree)[TOC] 树的基本概念树是一种非常重要的非线性数据结构，树的一个节点可能会生出多个分支。一般而言，一棵树会包含一个根节点，向下延伸出若干子节点，每个末端的节点被称为叶子节点。 有根树有根树存在一个根节点Root，如下：对于图中概念的一些补充： 节点拥有的子节点个数叫做节点的度。 具有相同深度的节点处于同一层，方便表示。 节点和节点之间的线叫做边。 路径：指从树上一点到另外一点所经过的不重合的点和边的集合，题目中有时会单指点或边的集合。 一颗 $n$ 个节点的树，一定有 $n-1$ 条边 无根树 二叉树二叉树是一种特殊的树。 所有节点的度都不超过2的树称为二叉树。 因为每个二叉树的节点最多只会有两个子结点，它的两个子节点一般会被称为左、右儿子，两棵子树一般会被称为左、右子树。 左、右儿子甚至根节点本身都有可能缺失（一个节点都没有可以称为空二叉树）。 满二叉树和完全二叉树二叉树也有两个比较特殊的类型：满二叉树和完全二叉树。 满二叉树：所有层的节点全满。 满二叉树的一些规律 第 $n$ 层的节点个数为 $2^{n-1}$ 深度为 $n$ 的满二叉树节点数为 $2^0 + 2^1 + 2^2 + \\dots + 2^{n-1}= 2^n-1$ 完全二叉树：除了最后一层以外，其他层的节点个数全满，而且最后一层的节点从左到右排满直到最后一个节点。 完全二叉树的一些规律 完全二叉树的节点个数不会少于 $(2^{n-1}-1)+1 = 2^{n-1}$ 完全二叉树的节点个数不会多于 $2^{n} - 1$ 一棵完全二叉树，设当前节点为 $t$，其父节点为 $t/2$，其左儿子为 $2t$，其右儿子为 $2t+1$，借助该规律，我们可以将完全二叉树使用数组进行存储。 完全二叉树的存储 完全二叉树由于它的特性，可以简单用数组来模拟其结构 一般会以数组$[1]$位置为根节点建立二叉树 数组$[t]$位置的左儿子和右儿子对应的位置分别为$[2t]$和$[2t+1]$，父节点的位置为$[t/2]$。 堆、线段树等数据结构的建立也会参考这个方式 完全二叉树的建立（使用数组），使用这种方法建立非完全二叉树，会导致空间的浪费： 12345678void build(int t) { // 添加数据 UpdateData(t); // 如果子节点存在 Build(2 * t); Build(2 * t + 1);} 为了解决这个问题，我们可以使用其他方法来完成一般二叉树的存储，可以用数组下标模拟节点编号，用多个数组来记录节点信息。为了方便，我们也可以使用结构体来存储这些信息： 12345// 使用结构体来实现上述操作struct TreeNode { int value; int l, r, fa;}a[100010]; 当然，作为一种树形结构，使用指针显然是更合适的方法： 123456789// 使用指针来实现上述操作struct TreeNode { int value; TreeNode* l; TreeNode* r; TreeNode* fa;};TreeNode* root; 使用指针的一些操作： 新建节点： 1234567struct TreeNode { int value; TreeNode *l, *r, *fa; // 初始为 NULL TreeNode(int x){ value = x; }};TreeNode* treeNode = new TreeNode(x); 根节点初始化： 12TreeNode* root;root = new TreeNode(v); 插入节点： 1234567891011void Insert(TreeNode* fa, TreeNode* p, int flag){ // flag = 0 插入到左边 // flag = 1 插入到右边 if (!flag) fa-&gt;l = p; else fa-&gt;r = p; p-&gt;fa = fa;}TreeNode* treeNode = new TreeNode(v);Insert(fa, treeNode, flag); 删除节点 1// 删除节点 二叉树的遍历二叉树的遍历可分为先序遍历、中序遍历和后序遍历，这三种方式以访问根节点的时间来区分。先序遍历（Degree-Left-Right, DLR）：根→左→右中序遍历（Left-Degree-Right, LDR）：左→根→右先序遍历（Left-Right-Degree, LRD）：左→右→根 在该图中，先序遍历的结果为 1 2 4 5 3 6 7，先序遍历代码如下： 1234567void preOrder(TreeNode* treeNode) { cout &lt;&lt; p-&gt;value &lt;&lt; endl; if(treeNode-&gt;l) preOrder(treeNode-&gt;l); if(treeNode-&gt;r) preOrder(treeNode-&gt;r);}preOrder(root); 在该图中，中序遍历的结果为 4 2 5 1 6 3 7，中序遍历代码如下： 1234567void inOrder(TreeNode* treeNode) { if(treeNode-&gt;l) inOrder(treeNode-&gt;l); cout &lt;&lt; p-&gt;value &lt;&lt; endl; if(treeNode-&gt;r) inOrder(treeNode-&gt;r);}inOrder(root); 在该图中，后序遍历的结果为 4 5 2 6 7 3 1，后序遍历代码如下： 1234567void postOrder(TreeNode* treeNode) { if(treeNode-&gt;l) postOrder(treeNode-&gt;l); if(treeNode-&gt;r) postOrder(treeNode-&gt;r); cout &lt;&lt; p-&gt;value &lt;&lt; endl;}postOrder(root); 除了上述的几种遍历方式，还有层级遍历（BFS）方式对树进行遍历。层级遍历是借助队列（Queue）来实现的，其过程可以描述如下： 层级遍历的代码如下： 1234567891011121314TreeNode* q[N];void bfs(TreeNode* root) { int front = 1, rear = 1; q[1] = root; while (front &lt;= rear) { TreeNode* p = q[front]; // 选取队列中最前面的节点 front++; cout &lt;&lt; p-&gt;value &lt;&lt;endl; if(p-&gt;l) q[++rear] = p-&gt;l; if(p-&gt;r) q[++rear] = p-&gt;r; }}bfs(root); 计算节点的深度我们可以在遍历树的时候同时进行节点深度的记录，简单来讲就是：$$depth_{儿子} = depth_{父亲} + 1$$ 有根树(Tree)这里不再是二叉树这种特殊的树，而是一般意义的树。 树的存储方式 vector/链表 123456789101112131415161718// vector 方式vector&lt;int&gt; nodes[N + 1];int n, father[N + 1];// 在 x 和 y之间构建一条边void addEdge(int x, int y) { nodes[x].push_back(y);}// 遍历 x 的所有儿子int l = nodes[x].size();for (int i = 0; i &lt; l; i++) { nodes[x][i];}for (auto i: nodes[x]) { ...} 123456789101112131415161718// 链表方式struct Node { int where; Node *next;} *head[N + 1], a[M];int n, father[N + 1], l = 0;void addEdge(int x, int y) { a[++i].where = y; a[l].next = head[x]; head[x] = &amp;a[l];}// 遍历 x 的所有儿子for (Node* p = head[x]; p; p-&gt;next) { p-&gt;where;} 有根树遍历遍历一棵树一般有 DFS 和 BFS 两种方式。DFS：深度优先搜索，从一个节点开始，选择一条路径并走到底，并通过回溯来访问所有节点。BFS：广度优先搜索，也称层级顺序探索，从一个节点开始，遍历该节点的所有子节点，或称按照深度从小到大的顺序依次遍历所有点。 有根树的DFS序有根树的 DFS 序是指，从根节点开始的深度优先搜索过程中，依次记录的点所生成的序列。对于上图，所生成的 DFS 序即为 ABCDEFGHIJKLMN。当然这个只是其中一种 DFS 序，因为 A 可以走向 B，也可以走向 E，当然也可以走向 F。不同的走向会有不同的 DFS 序。 1234567891011vector&lt;int&gt; dfn; // 用于存储 DFS 序, 常用 DFN 表示 DFS 序 // dfn 中的元素即为 DFS 序void dfs(int x) { dfn.push_back(x); // for x的所有儿子y { dfs(y); } // for (Node* p = x; p; p-&gt;next){ dfs(p-&gt;next) }}dfs(root); 有根树的BFS序有根树的 BFS 序是指，从根节点开始的广度优先搜索过程中，依次记录的点所生成的序列。对于上图，所生成的 BFS 序即为 ABENCDFMGJHIKL。当然这个只是其中一种 BFS 序，因为同一深度可能会有不同的遍历顺序，如深度为 $2$ 时，BEN、BNE、EBN、…都是可能出现的顺序，不同的顺序会有不同的 BFS 序。 12345678910111213void bfs(int root) { // 将 root 加入队列 q; q.push(root); // 遍历队列 q while(队列 q 非空) { x = q.top(); // 取队首元素 q.pop(); // x 出队 for x的所有儿子y { y 入队; } }} 无根树(Unrooted Tree)无根树即没有固定根结点的树，树中的节点只有相邻关系而没有父子关系。无根树有几种等价的形式化定义（建议搭配图论一起学习）： 有 $n$ 个结点， $n−1$ 条边的连通无向图 无向无环的连通图 任意两个结点之间有且仅有一条简单路径的无向图 任何边均为桥的连通图 没有圈，且在任意不同两点间添加一条边之后所得图含唯一的一个圈的图 如下图所示，即一棵无根树：无根树中的任意一个节点可以被指定为根，变成一棵有根树。 无根树的遍历遍历一棵无根树一般也有 DFS 和 BFS 两种方式。遍历无根树时，可以从任意一个节点开始，以类似有根树的方式，遍历整棵树。唯一的区别是在进入一个新节点时，需要记录这个节点的来源节点，在遍历新节点的相邻节点时，避免重复访问来源节点即可。 无根树的 DFS 123456789void dfs(int from, int x) { for x的所有响铃节点y { if (y != from) { dfs(x, y); } }}dfs(-1, x); 无根树的 BFS 12345678910111213141516void bfs(int x) { // 将 x 加入队列 q，x 的来源为空 while (队列 q 非空) { x = q.top(); from = x的来源节点; q.pop; for x的所有相邻节点 y { if (y != from) { y 入队; 记录 y 的来源节点为 x; } } }}bfs(x); 树的直径 树的直径是指树上任意两个节点之间最长（路径的长度一般指的是路径经过的边的数量）的路径。 一棵树可以存在很多条直径，他们的长度相等。 树的直径的中间节点被称为树的中心（图中C节点），如果直径上有偶数个节点，那么中间的两个节点都可以是树的中心。 树的中心到其它点的最长路径最短。","link":"/2024/05/07/Tree/"},{"title":"BELKA_2024","text":"[TOC] Leash Bio - Predict New Medicines with BELKA用 BELKA 预测新药 Predict small molecule-protein interactions using the Big Encoded Library for Chemical Assessment (BELKA) 使用化学评估大编码库（BELKA）预测小分子蛋白质相互作用 OverviewIn this competition, you’ll develop machine learning (ML) models to predict the binding affinity of small molecules to specific protein targets – a critical step in drug development for the pharmaceutical industry that would pave the way for more accurate drug discovery. You’ll help predict which drug-like small molecules (chemicals) will bind to three possible protein targets. 在这场比赛中，你将开发机器学习（ML）模型来预测小分子与特定蛋白质靶标（目标蛋白）的结合亲和力——这是制药行业药物开发的关键一步，将为更准确的药物发现铺平道路。你将帮助预测哪种药物样的小分子（化学物质）将与三种可能的蛋白质靶点结合。 DescriptionSmall molecule drugs are chemicals that interact with cellular protein machinery and affect the functions of this machinery in some way. Often, drugs are meant to inhibit the activity of single protein targets, and those targets are thought to be involved in a disease process. A classic approach to identify such candidate molecules is to physically make them, one by one, and then expose them to the protein target of interest and test if the two interact. This can be a fairly laborious and time-intensive process. 小分子药物是与细胞蛋白质机制相互作用并以某种方式影响该机制功能的化学物质。通常，药物旨在抑制单个蛋白质靶标的活性，而这些靶标被认为与疾病过程有关。识别这类候选分子的一种经典方法是一个接一个地进行物理制造，然后将其暴露于感兴趣的蛋白质靶点，并测试两者是否相互作用。这可能是一个相当费力和耗时的过程。 The US Food and Drug Administration (FDA) has approved roughly 2,000 novel molecular entities in its entire history. However, the number of chemicals in druglike space has been estimated to be 10^60, a space far too big to physically search. There are likely effective treatments for human ailments hiding in that chemical space, and better methods to find such treatments are desirable to us all. 美国食品药品监督管理局（FDA）已经批准了大约2000种新型分子实体在其整个历史. 然而，类药物领域的化学物质数量估计为$10^60$，这个空间太大了，无法进行物理搜索。在这个化学空间里，可能有有效的治疗人类疾病的方法，而找到更好的治疗方法对我们所有人来说都是可取的。 To evaluate potential search methods in small molecule chemistry, competition host Leash Biosciences physically tested some 133M small molecules for their ability to interact with one of three protein targets using DNA-encoded chemical library (DEL) technology. This dataset, the Big Encoded Library for Chemical Assessment (BELKA), provides an excellent opportunity to develop predictive models that may advance drug discovery. 为了评估小分子化学中潜在的搜索方法，比赛主办方Leash Biosciences使用DNA编码化学文库（DEL）技术对约133M个小分子进行了物理测试，以确定它们与三个蛋白质靶标之一相互作用的能力。该数据集，即化学评估大编码库（BELKA），为开发可能促进药物发现的预测模型提供了极好的机会。 Datasets of this size are rare and restricted to large pharmaceutical companies. The current best-curated public dataset of this kind is perhaps bindingdb, which, at 2.8M binding measurements, is much smaller than BELKA. 这种规模的数据集非常罕见，仅限于大型制药公司。目前这类最好的公共数据集可能是bindingdb，在2.8M的结合测量值下，比BELKA小得多。 This competition aims to revolutionize small molecule binding prediction by harnessing ML techniques. Recent advances in ML approaches suggest it might be possible to search chemical space by inference using well-trained computational models rather than running laboratory experiments. Similar progress in other fields suggest using ML to search across vast spaces could be a generalizable approach applicable to many domains. We hope that by providing BELKA we will democratize aspects of computational drug discovery and assist the community in finding new lifesaving medicines. 这项竞赛旨在通过利用ML技术彻底改变小分子结合预测。ML方法的最新进展表明，使用训练有素的计算模型而不是进行实验室 实验，通过推理搜索化学空间是可能的。其他 领域的类似进展表明，使用ML在广阔的空间中搜索可能是一种适用于许多领域的通用方法。我们希望通过提供BELKA，我们将使计算药物发现的各个方面民主化，并帮助社区寻找新的救命药物。 Here, you’ll build predictive models to estimate the binding affinity of unknown chemical compounds to specified protein targets. You may use the training data provided; alternatively, there are a number of methods to make small molecule binding predictions without relying on empirical binding data (e.g. DiffDock, and this contest was designed to allow for such submissions). 在这里，你将建立预测模型来估计未知化合物与特定蛋白质靶标的结合亲和力。您可以使用提供的培训数据；或者，有许多方法可以在不依赖经验结合数据的情况下进行小分子结合预测（例如DiffDock，而本次竞赛旨在允许此类提交）。 Your work will contribute to advances in small molecule chemistry used to accelerate drug discovery. 你的工作将有助于促进用于加速药物发现的小分子化学的进步。 EvaluationThis metric for this competition is the average precision calculated for each (protein, split group) and then averaged for the final score. Please see this forum post for important details. 这项比赛的指标是为每个（蛋白质、分组）计算的平均精度，然后为最终得分取平均值。请参阅此论坛帖子了解重要细节。 Here’s the code for the implementation. 这是代码以供实施。 Submission FileFor each id in the test set, you must predict a probability for the binary target binds target. The file should contain a header and have the following format: 对于测试集中的每个id，您必须预测二进制目标“绑定”目标的概率。该文件应包含一个标头，并具有以下格式： 12345id,binds295246830,0.5295246831,0.5295246832,0.5etc. Timeline April 4, 2024 - Start Date. July 1, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete. July 1, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams. July 8, 2024 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes First Prize: $12,000 Second Prize: $10,000 Third Prize: $10,000 Fourth Prize: $8,000 Fifth Prize: $5,000 Top Student Group: $5,000 to the highest performing student team. A team would be considered a student team if majority members (e.g. at least 3 out of a 5 member team) are students enrolled in a high school or university degree. In the case of an even number of members, half of them must be students. Competition HostLeash Biosciences is a discovery-stage biotechnology company that seeks to improve medicinal chemistry with machine learning approaches and massive data collection. Leash is comprised of wet lab scientists and dry lab scientists in equal numbers, and is proudly headquartered in Salt Lake City, Utah, USA. Additional DetailsChemical RepresentationsOne of the goals of this competition is to explore and compare many different ways of representing molecules. Small molecules have been [represented](https://pubs.acs.org/doi/10.1021/acsinfocus.7e7006?ref=infocus%2FAI_&amp; Machine Learning) with SMILES, graphs, 3D structures, and more, including more esoteric methods such as spherical convolutional neural nets. We encourage competitors to explore not only different methods of making predictions but also to try different ways of representing the molecules. We provide the molecules in SMILES format. 这场比赛的目标之一是探索和比较许多不同的分子表现方式。小分子已经用SMILES、图形、3D结构等表示，包括更深奥的方法，如球形卷积神经网络。我们鼓励竞争对手不仅探索不同的预测方法，还尝试不同的分子表示方法。 我们提供SMILES格式的分子。 SMILESSMILES is a concise string notation used to represent the structure of chemical molecules. It encodes the molecular graph, including atoms, bonds, connectivity, and stereochemistry as a linear sequence of characters, by traversing the molecule graph. SMILES is widely used in machine learning applications for chemistry, such as molecular property prediction, drug discovery, and materials design, as it provides a standardized and machine-readable format for representing and manipulating chemical structures. The SMILES in this dataset should be sufficient to be translated into any other chemical representation format that you want to try. A simple way to perform some of these translations is with RDKit. SMILES是一种简明的字符串表示法，用于表示化学分子的结构。它通过遍历分子图，将分子图（包括原子、键、连接性和立体化学）编码为线性字符序列。SMILES广泛用于化学的机器学习应用，如分子性质预测、药物发现和材料设计，因为它为表示和操纵化学结构提供了标准化和机器可读的格式。该数据集中的SMILES应该足以转换为您想要尝试的任何其他化学表示格式。执行其中一些翻译的一种简单方法是使用RDKit. Details about the experimentsDELs are libraries of small molecules with unique DNA barcodes covalently attachedTraditional high-throughput screening requires keeping individual small molecules in separate, identifiable tubes and demands a lot of liquid handling to test each one of those against the protein target of interest in a separate reaction. The logistical overhead of these efforts tends to restrict screening collections, called libraries, to 50K-5M small molecules. A scalable solution to this problem, DNA-encoded chemical libraries, was described in 2009. As DNA sequencing got cheaper and cheaper, it became clear that DNA itself could be used as a label to identify, and deconvolute, collections of molecules in a complex mixture. DELs leverage this DNA sequencing technology. These barcoded small molecules are in a pool (many in a single tube, rather than one tube per small molecule) and are exposed to the protein target of interest in solution. The protein target of interest is then rinsed to remove small molecules in the DEL that don’t bind the target, and the remaining binders are collected and their DNA sequenced. DEL是共价连接有独特DNA条形码的小分子库传统高通量筛选需要将单个小分子保持在单独的、可识别的管中，并且需要大量的液体处理来在单独的反应中针对感兴趣的蛋白质靶标测试其中的每一个。这些工作的后勤开销往往将筛选收藏（称为文库）限制在5000万至500万个小分子以内。这个问题的一个可扩展的解决方案，DNA编码的化学文库，在2009年描述. 随着DNA测序变得越来越便宜，很明显，DNA本身可以用作标签来识别和消除复杂混合物中分子的聚集。DELs这种DNA测序技术。这些条形码小分子在一个池中（许多在单管中，而不是每个小分子一管），并暴露于溶液中感兴趣的蛋白质靶标。然后冲洗感兴趣的蛋白质靶标，以去除DEL中不与靶标结合的小分子，收集剩余的结合物并对其DNA进行测序。 DELs are manufactured by combining different building blocksAn intuitive way to think about DELs is to imagine a Mickey Mouse head as an example of a small molecule in the DEL. We attach the DNA barcode to Mickey’s chin. Mickey’s left ear is connected by a zipper; Mickey’s right ear is connected by velcro. These attachment points of zippers and velcro are analogies to different chemical reactions one might use to construct the DEL. We could purchase ten different Mickey Mouse faces, ten different zipper ears, and ten different velcro ears, and use them to construct our small molecule library. By creating every combination of these three, we’ll have 1,000 small molecules, but we only needed thirty building blocks (faces and ears) to make them. This combinatorial approach is what allows DELs to have so many members: the library in this competition is composed of 133M small molecules. The 133M small molecule library used here, AMA014, was provided by AlphaMa. It has a triazine core and superficially resembles the DELs described here. DEL是通过组合不同的构建块来制造的一个思考DEL的直观方法是想象一个米老鼠的头作为DEL中一个小分子的例子。我们把DNA条形码贴在米奇的下巴上。米奇的左耳由拉链连接；米奇的右耳是用尼龙搭扣连接的。拉链和尼龙搭扣的这些连接点类似于可能用于构建DEL的不同化学反应。我们可以购买十个不同的米老鼠脸、十个不同拉链耳朵和十个不同尼龙搭扣耳朵，并用它们来构建我们的小分子库。通过创建这三者的每一个组合，我们将拥有1000个小分子，但我们只需要30个构建块（脸和耳朵）就可以制造它们。这种组合方法使DEL能够拥有如此多的成员：这场竞争中的文库由133M个小分子组成。这里使用的133M小分子文库AMA014由AlphaMa提供。它有一个三嗪核心，表面上类似于此处描述的DEL。 AcknowledgementsLeash Biosciences is grateful for the generous cosponsorship of Top Harvest Capital and AlphaMa. CitationAndrew Blevins, Ian K Quigley, Brayden J Halverson, Nate Wilkinson, Rebecca S Levin, Agastya Pulapaka, Walter Reade, Addison Howard. (2024). Leash Bio - Predict New Medicines with BELKA. Kaggle. https://kaggle.com/competitions/leash-BELKA Dataset DescriptionOverviewThe examples in the competition dataset are represented by a binary classification of whether a given small molecule is a binder or not to one of three protein targets. The data were collected using DNA-encoded chemical library (DEL) technology. 比赛数据集中的例子由给定小分子是否与三个蛋白质靶标之一结合的二元分类表示。使用DNA编码化学文库（DEL）技术收集数据。 We represent chemistry with SMILES (Simplified Molecular-Input Line-Entry System) and the labels as binary binding classifications, one per protein target of three targets. 我们用SMILES（简化分子输入 行输入系统)和二元绑定分类来表示化学，三个靶标中的每个蛋白质靶标都有一个。 Files[train/test].[csv/parquet] - The train or test data, available in both the csv and parquet formats. id - A unique example_id that we use to identify the molecule-binding target pair. buildingblock1_smiles - The structure, in SMILES, of the first building block buildingblock2_smiles - The structure, in SMILES, of the second building block buildingblock3_smiles - The structure, in SMILES, of the third building block molecule_smiles - The structure of the fully assembled molecule, in SMILES. This includes the three building blocks and the triazine core. Note we use a [Dy] as the stand-in for the DNA linker. protein_name - The protein target name binds - The target column. A binary class label of whether the molecule binds to the protein. Not available for the test set. sample_submission.csv - A sample submission file in the correct format [train/test].[csv/parquet] - 训练或测试数据，csv和parquet格式均可。 id - 我们用来识别分子结合靶标对的唯一示例_id。 buildingblock1_smiles - 第一个构建块的结构，以SMILES表示 buildingblock2_smiles - 第二个构建块的结构，以SMILES表示 buildingblock3_smiles - 第三个构建块的结构，以SMILES表示 molecule_smiles - 完全组装的分子的结构，以SMILES表示。这包括三个构建块和三嗪核心。请注意，我们使用[Dy]作为DNA连接子的替代。 protein_name - 蛋白质靶标名称 binds - 目标列。分子是否与蛋白质结合的二进制类标签。不适用于测试集。 Competition dataAll data were generated in-house at Leash Biosciences. We are providing roughly 98M training examples per protein, 200K validation examples per protein, and 360K test molecules per protein. To test generalizability, the test set contains building blocks that are not in the training set. These datasets are very imbalanced: roughly 0.5% of examples are classified as binders; we used 3 rounds of selection in triplicate to identify binders experimentally. Following the competition, Leash will make all the data available for future use (3 targets × 3 rounds of selection × 3 replicates × 133M molecules, or 3.6B measurements). 所有数据均由Leash Biosciences公司内部生成。我们为每种蛋白质提供了大约 98M 个训练实例，为每种蛋白提供了 200K 个验证实例，为每个蛋白质提供了 360K 个测试分子。为了测试可推广性，测试集包含不在训练集中的构建块。这些数据集非常不平衡：大约0.5%的示例被归类为绑定；我们使用了三轮一式三份的选择来实验鉴定粘合剂。比赛结束后，Leash将提供所有数据供未来使用（3个靶标×3轮选择×3个重复×3.33M个分子，或3.6B测量值）。 TargetsProteins are encoded in the genome, and names of the genes encoding those proteins are typically bestowed by their discoverers and regulated by the Hugo Gene Nomenclature Committee. The protein products of these genes can sometimes have different names, often due to the history of their discovery. We screened three protein targets for this competition. 蛋白质在基因组中编码，编码这些蛋白质的基因的名称通常由其发现者命名，并由雨果基因命名委员会监管。这些基因的蛋白质产物有时可能有不同的名称，通常是由于它们的发现历史。我们为这次比赛筛选了三个蛋白质靶点。 EPHX2 (sEH)The first target, epoxide hydrolase 2, is encoded by the EPHX2 genetic locus, and its protein product is commonly named “soluble epoxide hydrolase”, or abbreviated to sEH. Hydrolases are enzymes that catalyze certain chemical reactions, and EPHX2/sEH also hydrolyzes certain phosphate groups. EPHX2/sEH is a potential drug target for high blood pressure and diabetes progression, and small molecules inhibiting EPHX2/sEH from earlier DEL efforts made it to clinical trials. EPHX2/sEH was also screened with DELs, and hits predicted with ML approaches, in a recent study but the screening data were not published. We included EPHX2/sEH to allow contestants an external gut check for model performance by comparing to these previously-published results. We screened EPHX2/sEH purchased from Cayman Chemical, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino sequence is positions 2-555 from UniProt entry P34913, the crystal structure can be found in PDB entry 3i28, and predicted structure can be found in AlphaFold2 entry 34913. Additional EPHX2/sEH crystal structures with ligands bound can be found in PDB. 第一个靶标环氧化物水解酶2由EPHX2基因座编码，其蛋白产物通常被命名为“可溶性环氧化物水解酶”，或缩写为sEH。水解酶是催化某些化学反应的酶，EPHX2/sEH也水解某些磷酸基团。EPHX2/sEH是高血压和糖尿病进展的潜在药物靶点，早期DEL研究中抑制EPHX2/s EH的小分子已进入临床试验.EPHX2/sEH也用DEL进行了筛选，并用ML方法预测了命中率(https://blog.research.google/2020/06/unlocking-chemome-with-dna-encoded.html学习https://pubs.acs.org/doi/10.1021/acs.jmedchem.0c00452)但筛选数据没有公布。我们纳入了EPHX2/sEH，通过与之前公布的结果进行比较，让参赛者能够对模型性能进行外部检查。我们筛选了EPHX2/sEH购自开曼化学. 在PDB中可以发现具有结合配体的额外的EPHX2/sEH晶体结构。 BRD4The second target, bromodomain 4, is encoded by the BRD4 locus and its protein product is also named BRD4. Bromodomains bind to protein spools in the nucleus that DNA wraps around (called histones) and affect the likelihood that the DNA nearby is going to be transcribed, producing new gene products. Bromodomains play roles in cancer progression and a number of drugs have been discovered to inhibit their activities. BRD4 has been screened with DEL approaches previously but the screening data were not published. We included BRD4 to allow contestants to evaluate candidate molecules for oncology indications. We screened BRD4 purchased from Active Motif, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 44-460 from UniProt entry O60885-1, the crystal structure (for a single domain) can be found in PDB entry 7USK and predicted structure can be found in AlphaFold2 entry O60885. Additional BRD4 crystal structures with ligands bound can be found in PDB. ALB (HSA)The third target, serum albumin, is encoded by the ALB locus and its protein product is also named ALB. The protein product is sometimes abbreviated as HSA, for “human serum albumin”. ALB, the most common protein in the blood, is used to drive osmotic pressure (to bring fluid back from tissues into blood vessels) and to transport many ligands, hormones, fatty acids, and more. Albumin, being the most abundant protein in the blood, often plays a role in absorbing candidate drugs in the body and sequestering them from their target tissues. Adjusting candidate drugs to bind less to albumin and other blood proteins is a strategy to help these candidate drugs be more effective. ALB has been screened with DEL approaches previously but the screening data were not published. We included ALB to allow contestants to build models that might have a larger impact on drug discovery across many disease types. The ability to predict ALB binding well would allow drug developers to improve their candidate small molecule therapies much more quickly than physically manufacturing many variants and testing them against ALB empirically in an iterative process. We screened ALB purchased from Active Motif. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 25 to 609 from UniProt entry P02768, the crystal structure can be found in PDB entry 1AO6, and predicted structure can be found in AlphaFold2 entry P02768. Additional ALB crystal structures with ligands bound can be found in PDB. Good luck!","link":"/2024/05/19/BELKA-2024/"},{"title":"IDEA 2022 搭建 Tomcat 环境","text":"[TOC] Tomcat 环境的搭建参考教程 下载 TomcatTomcat官网地址在 Tomcat 官网中下载指定版本的 Tomcat，左侧 Download 处有相应版本，这里推荐 Tomcat 9 版本（因为Tomcat 10 在配置时会出现一定的问题）。下载后解压到指定位置即可。 配置环境变量即可配置 Tomcat 环境变量前一定要配置好 Java 的环境变量，尤其是JAVA_HOME，这里我一开始并没有配置 JAVA_HOME，我的环境变量是JAVA_HOME_180=xxx，这种方式Tomcat是找不到JAVA_HOME的，因此我又重新配置了JAVA_HOME。我的 JAVA_HOME 环境变量为： 1JAVA_HOME=D:\\JDK\\jdk1.8.0_231 下面是 Tomcat 的环境变量配置：新建 CATALINA_HOME 环境变量： 1CATALINA_HOME=D:\\tomcat\\apache-tomcat-9.0.89 修改Path，在 Path 后添加（新建）如下环境变量： 123%CATALINA_HOME%\\lib%CATALINA_HOME%\\bin%CATALINA_HOME%\\lib\\servlet-api.jar 验证是否配置成功在命令行中，执行命令：startup.bat，若正常打印相关配置变量、且 Tomcat 进程被阻塞，即证明环境搭建成功。访问localhost:8080，出现以下界面即证明成功搭建。使用 shutdown.bat 命令即可使阻塞的 Tomcat 进程被关闭，推荐使用这种方式关闭 Tomcat。 可能会出现的问题 协议处理程序初始化失败：参考教程这个问题有可能是由于8080端口被占用了，在Windows中可以使用如下命令查看端口的占用情况： 1netstat -aon|findstr &quot;8080&quot; 如果确实被占用了，可以使用如下命令杀死端口号为 &lt;PIDNUM&gt; 的进程。 1taskkill -PID &lt;PIDNUM&gt; -F 闪退可能原因是：环境变量配置不正确，仔细检查环境变量的配置。 乱码问题描述：打开startup.bat后汉字乱码解决方法：在.\\apache-tomcat-9.0.43\\conf下打开logging.properties文件将java.util.logging.ConsoleHandler.encoding = UTF-8替换为java.util.logging.ConsoleHandler.encoding = GBK 社区版 IDEA 如何配置 TomcatCSDN 上大多数教程使用 Maven 创建 Tomcat 项目，但是这种方法实在是过于麻烦，社区版和专业版又有些不同，找不到很多东西。 如何配置 IDEA 2022 社区版中的 Tomcat 安装插件在 File → Settings → Plugin 中安装插件，搜索 Tomcat，安装插件。 配置Tomcat路径安装插件后，在 File → Settings → Plugin → Tomcat Server添加配置如下： 完成","link":"/2024/05/22/Build-Tomcat/"},{"title":"Better QT","text":"[TOC] Qt 的一些常用技巧快捷键 快捷键 Ctrl + Tab 可以切换文件； 快捷键 Alt + ENTER 弹出代码生成提示，可以快速提示错误修改方案，类似于 IDEA 的 Alt + ENTER； 快捷键 Alt + 鼠标 同时输入； 快捷键 Ctrl + R 运行程序； 快捷键 Ctrl + M 创建书签（Bookmark），或者直接在某行代码前右键添加书签； 快捷键 Ctrl + ENTER 在当前行下方插入空行； 快捷键 Ctrl + Shift + ENTER 在当前行下方插入空行； 快捷键 Ctrl + I 代码对齐； 快捷键 Ctrl + ; 格式化代码； 快捷键 Shift + Delete 剪切当前行，可以当删除用； 快捷键 Ctrl + Shift + R 局部变量统一修改； 快捷键 Ctrl + Shift + V 复制历史； 用键盘模拟鼠标操作： 功能键 方向键 备注 Ctrl Shift Alt 左/右 上/下 Home/End 方向键具有移动光标的作用 × × × 字符 字符 行首/行尾 - √ × × 单词 滚动条 文件头/尾 - √ √ × 单词 移动 行首/行尾 Shift具有选中文本的作用 √ × √ - 向上/下复制选中部分 - - 快捷键 F1 查看帮助、文档 快捷键 F2 快速到变量或者函数间切换 快捷键 F4 快速在.cpp文件和.h文件间切换 快捷键 Ctrl + Shift + U 查找所有使用该符号的地方 快捷键 Ctrl + K 打开定位器 快捷键 Ctrl + L 跳转到某一行 快捷键 Ctrl + [Shift] + F 查找/替换当前文件[项目]当前选中的内容 快捷键 [Shift] + F3 查找下[上]一个 快捷键 Ctrl + B 编译工程 快捷键 Ctrl + R 运行工程 快捷键 F5 调试运行 快捷键 Ctrl + Shift + F5 重启调试 快捷键 F9 设置和取消断点 快捷键 F10 单步跳过 快捷键 F11 单步进入 Creator 片段片段简单理解一下就是已经写好的一些模式化的代码，用户可以使用内置片段或者根据自己的需要自定义片段。 自带片段示例 自定义片段一个用户的自定义片段需要以下几个内容：$$片段 = 一级标题 + 二级标题 + 片段文本$$需要通过：编辑（Edit）→首选项（Preferences）→文本编辑器（Text Editor）→片段（Snippets）进行设置比如我要添加一个自定义片段 note，用来表示文件注释，可以选择 Group 为 C++，然后选择 Add，添加指定的内容： Qt 代码/文件解释Qt的源代码和文件解释 Qt 代码 hellocosbrowser.h 1234567891011121314151617181920212223242526272829#ifndef HELLOCOSBROWSER_H#define HELLOCOSBROWSER_H#include &lt;QWidget&gt;#include &lt;QMessageBox&gt;QT_BEGIN_NAMESPACEnamespace Ui { class HelloCOSBrowser; }QT_END_NAMESPACEclass HelloCOSBrowser : public QWidget // QWidget 是所有应用程序窗口的基类{ Q_OBJECT // Qt的宏, 支持 Qt 的特性, 如信号与槽、对象树、元对象等public: // 这里 HelloCOSBrowser 指定父窗口指针为 nullptr, 则它会作为一个独立的窗口进行展示, 否则则会作为父窗口的一个控件 // 关于这个父窗口指针, 一个很典型的应用就是 微信 // 当我们创建新窗口的时候, 如果不指定父窗口, 就会弹出一个独立的新窗口, 即电脑任务栏的图标会多出来一个 // 如果指定了父窗口, 则不会创建一个独立的窗口, 即电脑任务栏处的图标不会增加 HelloCOSBrowser(QWidget *parent = nullptr); ~HelloCOSBrowser();private slots: void showDialog();private: Ui::HelloCOSBrowser *ui;};#endif // HELLOCOSBROWSER_H Qt 工程文件解释文件列表 文件名称 描述 pro 文件 该文件是 Qt 的项目文件，qmake工具可以根据此文件生成 Makefile pro.user 文件 该文件包含和用户相关的项目信息（用户不需要关注此文件） ui 文件 Qt 的设计师界面文件 .cpp 文件 C++ 源文件 .h 文件 C++ 头文件 MOC编译器MOC(Meta-Object Compiler)编译器C++ 编译器本身不支持 Qt 的某些机制，Qt 希望对 C++ 代码进行自动扩展，这里就需要用到宏（例如：Q_Object）和继承。此外为了方便用户使用，希望用户无感知，可以将这一操作直接集成到框架中。 Qt 编译过程12345预编译 -&gt; 编译 -&gt; 汇编 -&gt; 链接 -&gt; 目标 ↑ +-------------------------+ ↑拓展代码 -&gt; MOC编译器 -&gt; 新CPP代码 通过上述方式，实现 Qt 的某些特性。我们可以发现，当我们写完代码进行编译后，会产生一个 debug 文件夹，此时我们进入该文件夹，会看到一些元对象编译器编译的文件，如 moc_xxxx.cpp 或 moc_xxx.h 等文件。 MOC 的使用方法 MOC 编译工具由 Qt 框架自动调用 扫描 C++ 头文件，寻找 Q_OBJECT 宏 生成拓展 C++ 代码，再进行预编译 程序员在使用时，需要继承 QObject 类或者是 QObject 子类，并且包含 Q_OBJECT 宏。 Qt应用程序开发Qt Designer 设计师界面使用① Qt 控件编辑模式② Qt 信号与槽编辑模式③ Qt 伙伴关系编辑模式④ Qt Tab 顺序编辑模式：可以设置按下 Tab 键的高亮顺序 Qt 核心——信号与槽信号与槽的基本概念 Qt 中的信号和槽是支持多对多的，即一个信号可以对应多个槽，一个槽可以由多个信号触发。 Qt 中的信号无需实现，可以由函数（普通函数或者槽函数）通过 emit 关键字发送信号传递参数。 Qt中如何定义信号 继承 QObject 类或其派生类，同时包含 Q_OBJECT 宏 使用关键字 signals 声明函数信号函数，不需要具体实现信号函数 使用 emit 关键字发送信号 Qt中如何定义槽函数 必须包含 Q_OBJECT 宏 使用关键字 [public/protected/private] slots 声明函数 需要具体实现声明的槽函数 Qt中如何连接信号与槽（三种写法） SIGNAL/SLOT 宏写法：QObject::connect(this, SIGNAL(...), this, SLOT(...)); 函数指针写法：QObject::connect(this, &amp;SignalFunction, this, &amp;SlotFunction) lambda 表达式写法：QObject::connect(this, &amp;SignalFunction, this, [=]() { qDebug() &lt;&lt; &quot;...&quot;; }) 三种写法的比较： 连接信号与槽 宏 函数指针 编译 运行 编译 运行 参数类型 完全相同 √ √ √ √ 隐式转换 向上 √ × √ √ 向下 √ × √ √ 不可以隐式转换 √ × × × 参数个数 信号=槽 √ √ √ √ 信号>槽 √ √ √ √ 信号","link":"/2024/05/22/Better-QT/"},{"title":"Evaluation Indicators in AI","text":"","link":"/2024/05/27/Evaluation-Indicators-in-AI/"},{"title":"QuickPassHBase","text":"快速上手HBase[TOC] ⚙ 1. HBase简介1.1 HBase的定义Apache HBase 是以 HDFS 为数据存储的，一种分布式、可扩展的 NoSQL 数据库。 HBase 的设计理念依据 Google 的 BigTable 论文，论文中对于数据模型的首句介绍。 BigTable是一个稀疏的、分布式的、持久的多维排序映射(Map)。该映射由行键、列键和时间戳索引作为键(Key)，映射中的每个值(Value)都是一个未解释的字节数组。 HBase 使用与 BigTable 非常相似的数据模型。用户将数据行存储在带标签的表中。数据行具有可排序的键和任意数量的列。该表存储稀疏，因此如果用户喜欢，同一表中的行可以具有疯狂变化的列。 1.2 HBase的数据模型1.2.1 HBase 的逻辑结构12345678910111213141516171819202122232425{ &quot;row_key1&quot;: { &quot;personal_info&quot;: { &quot;name&quot;: &quot;ZhangSan&quot;, &quot;city&quot;: &quot;Beijing&quot;, &quot;phone&quot;: &quot;156****0000&quot; }, &quot;office_info&quot;: { &quot;tel&quot;: &quot;010-1234567&quot;, &quot;address&quot;: &quot;Shandong&quot; } }, &quot;row_key11&quot;: { &quot;personal_info&quot;: { &quot;city&quot;: &quot;Shanghai&quot;, &quot;phone&quot;: &quot;133****0000&quot; }, &quot;office_info&quot;: { &quot;tel&quot;: &quot;010-1234567&quot;, } }, &quot;row_key2&quot;: { ... }} 列族→ personal_info office_info RowKey↓ name city phone tel address row_key1 ZhangSan Beijing 156****0000 010-1234567 Shandong row_key11 Shanghai 131****0000 010-1234567 row_key2 ... ... ... ... ... 在上面的表格中： personal_info、office_info称为列族 name、city、phone、tel、address称为列 row_key1、row_key11称为行键。 将一整张大表按照行进行拆分，拆分为多个表，拆分后的每个表称为**块(Region)**，用于实现分布式结构。 将一整张大表按照列族进行拆分，拆分为多个**存储(Store)**，用于在底层存储到不同的文件夹中，便于文件对应。 存储数据稀疏，数据存储多维，不同的行具有不同的列。数据存储整体有序，按照RowKey的字典序排列，RowKey为一个Byte数组。 1.2.2 HBase 的物理结构物理存储结构即为数据映射关系，而在概念视图的空单元格，底层实际根本不存储。 在HDFS中划分好的存储Store如下： personal_info RowKey name city phone row_key1 ZhangSan Beijing 156****0000 row_key11 Shanghai 131****0000 row_key2 ... ... ... 其底层一定是以映射(Map)的方式进行存储的，格式为**(Key, Value)，Value一定是“ZhangSan”**这种字段。那么Key是什么呢？ 为了确定Value值**”ZhangSan”，我们需要用Key对应到Value**，于是得到存储如下： Row Key Column Family Column Qualifier Timestamp Type Value row_key1 personal_info name t1 Put ZhangSan row_key1 personal_info city t2 Put Beijing row_key1 personal_info phone t3 Put 156****0000 row_key1 personal_info phone t4 Put 156****0001 row_key1 personal_info phone t5 Delete 156****0001 … … … … … … 因为 HDFS 是无法修改数据的，而 HBase 需要修改数据，那么就需要解决这一问题，于是就有了**时间戳(Timestamp)**。不同版本（version）的数据根据 Timestamp 进行区分，读取数据默认读取最新的版本。 在上面的表格中，t4相对于t3来说就是进行了修改，将t3时的**phone从156****0000修改为t4时的156****0001，读取时默认读取t4时的phone**值，通过这种方式完成了修改。 同样的，我们也不好删除数据，因此我们只需要插入一条**Type**为Delete的数据即可。 1.2.3 数据模型 Name Space 命名空间 类似于关系型数据库的 Database 概念，每个命名空间下有多个表。HBase 两个自带的命名空间，分别是 hbase 和default，hbase 中存放的是 HBase 内置的表，default表是用户默认使用的命名空间。 Table 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需要声明具体的列。因为数据存储时稀疏的，所有往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。 需要注意的是，列族的存在是动态添加列（或称字段）的基础。 Row HBase 表中的每行数据都由*一个行键(RowKey)和多个列(Column)组成，数据是按照 RowKey的字典顺序存储的，*并且查询数据时只能根据 RowKey进行检索**，所以RowKey的设计十分重要。 Column HBase 中的每个列都由列族(Column Family)和列限定符(Column Qualifier)进行限定，例如info:name, info:age。建表时，只需指明列族，而列限定符无需预先定义。列限定符听起来很高端，其实就是列名的意思。 Time Stamp 用于标识数据的**不同版本(Version)**，每条数据写入时，系统会自动为其加上该字段，其值为写入 HBase 的时间。 Cell 由 {rowkey, Column Family: Column Qualifier, Timestamp} 唯一确定的单元，Cell 中的数据全部是字节码形式存储。 1.3 HBase 基本架构 Master 主要进程，具体实现类为HMaster，通常部署在NameNode上。 主要功能：负责通过 ZK 监控 RegionServer 进程状态，同时是所有元数据变化的接口，内部启动监控执行 region 的故障转移和拆分的线程。 功能的详细描述： 管理元数据表格 hbase:meta：接收用户对表格创建、修改、删除的命令并执行。 监控 RegionServer 是否需要进行负载均衡、故障转移和Region拆分。通过启动多个后台线程监控实现上述功能： LoadBalancer 负载均衡器 周期性监控 region分布在 RegionServer 上面是否均衡，由参数 hbase.balancer.period控制周期时间，默认5分钟。 CatalogJanitor元数据管理器 定期检查和清理hbase:meta中的数据。 MasterProcWAL Master 预写日志处理器 把Master需要执行的任务记录到预写日志WAL中，如果Master宕机，则让BackupMaster继续操作。 RegionServer 主要进程，具体实现类为HRegionServer，通常部署在DataNode上。 功能：主要负责数据 Cell 的处理，同时在执行区域的拆分和合并的时候，由 RegionServer 来实际执行。 功能的详细描述： 负责数据 Cell 的处理，例如写入数据put，查询数据get等。 拆分合并 region 的实际执行者，有 Master 监控，有RegionServer 执行。 ZooKeeper HBase 通过 ZooKeeper 来做 Master的高可用、记录 RegionServer 的部署信息、并且存储有 meta 表的位置信息。HBase 对于数据的读写操作时是直接访问 ZooKeeper 的，在 2.3 版本推出 Master Registry 模式，客户端可以直接访问 Master。使用此功能，会加大对 Master的压力，减轻对 ZooKeeper 的压力。 HDFS HDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高容错的支持。 上图中的Region由三个RegionServer随机管理，尽量均衡。表名hbase:meta是一个特例，他存储在HDFS，但是由Master管理。 🔧 2. 快速上手2.1 安装部署2.1.1 分布式部署 至少 3 台虚拟机 123hadoop101hadoop102hadoop103 保证 ZooKeeper 正常部署，并且启动 ZooKeeper 1zkServer.sh start 保证 Hadoop 正常部署，并且启动 Hadoop 1start-dfs.sh 配置 HBase 环境 ① 下载 HBase 安装包（压缩包），这里假设为hbase-2.4.11-bin.tar.gz ② 解压 HBase 安装包到一个文件夹 1tar -zxvf /path/to/hbase-2.4.11-bin.tar.gz -C /path/to/module ③ 在用户目录下，添加用户环境变量 1vim .bashrc 123#HBase_HOMEexport HBASE_HOME = /path/to/module/hbase-2.4.11export PATH = $PATH:$HBASE_HOME/bin ④ 使环境变量生效 1source .bashrc ⑤ 修改配置文件 hbase-env.sh 123# 表示是否需要 HBase 管理维护一个自带的 ZooKeeper, 默认为 true# 我们需要使用本机已经配置好的 ZooKeeper, 所以修改为 Falseexport HBASE_MANAGES_ZK = false hbase-site.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!-- ZooKeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop101,hadoop102,hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase数据在HDFS中的存放路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hadoop101:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase的运行模式 --&gt; &lt;!-- false为单机模式, HBase和ZooKeeper会运行在同一个JVM虚拟机中 --&gt; &lt;!-- true 为分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- ZooKeeper快照的存储位置 --&gt; &lt;!-- 这里替换为自己的 /path/to/ZooKeeperDir --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.6/data&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase 安全模式 --&gt; &lt;!-- 在分布式模式下, 设置为 false --&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers 123hadoop101hadoop102hadoop103 ⑥ 解决 log4j 不兼容的问题，移除 HBase或者 Hadoop的 .jar包 ⑦ 使用 scp 命令同步 HBase 配置，需要提前设置好免密登录。或者使用 xsync 启动 HBase 服务 单点启动 1234#单点启动HMasterhbase-daemon.sh start master#单点启动HRegionServerhbase-daemon.sh start regionserver 集群启动 1start-hbase.sh 停止服务 1stop-hbase.sh 2.1.2 高可用服务 如果 HBase 已经启动，先关闭HBase 1stop-hbase.sh 添加配置文件 backup-masters 123#使用touch命令或者echo命令均可touch /path/to/hbase-2.1.4/conf/backup-mastersvim /path/to/hbase-2.1.4/conf/backup-masters 添加内容：hadoop102 使用 scp 命令分发配置文件 启动HBase，正常启动进程如下： 123hadoop101 -&gt; HMaster HRegionServerhadoop102 -&gt; HMaster HRegionServerhadoop103 -&gt; HRegionServer 其中，hadoop101 的 HMaster 先启动作为主节点，hadoop102 的 HMaster后启动，作为**备用节点(Backup-Master)**。 2.2 使用操作2.2.1 Shell操作使用命令 hbase shell 启动 HBase 的 Shell 命令界面，所有命令均可以使用 help 查到。 当我们在 hbase shell中输入help命令时，将会弹出HBase的使用提示： 1hbase shell 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667hbase(main):001:0&gt; helpHBase Shell, version 2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Type 'help &quot;COMMAND&quot;', (e.g. 'help &quot;get&quot;' -- the quotes are necessary) for help on a specific command.Commands are grouped. Type 'help &quot;COMMAND_GROUP&quot;', (e.g. 'help &quot;general&quot;') for help on a command group.COMMAND GROUPS: Group name: general Commands: processlist, status, table_help, version, whoami Group name: ddl Commands: alter, alter_async, alter_status, clone_table_schema, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, list_regions, locate_region, show_filters Group name: namespace Commands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables Group name: dml Commands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve Group name: tools Commands: assign, balance_switch, balancer, balancer_enabled, catalogjanitor_enabled, catalogjanitor_run, catalogjanitor_switch, cleaner_chore_enabled, cleaner_chore_run, cleaner_chore_switch, clear_block_cache, clear_compaction_queues, clear_deadservers, close_region, compact, compact_rs, compaction_state, flush, hbck_chore_run, is_in_maintenance_mode, list_deadservers, major_compact, merge_region, move, normalize, normalizer_enabled, normalizer_switch, split, splitormerge_enabled, splitormerge_switch, stop_master, stop_regionserver, trace, unassign, wal_roll, zk_dump Group name: replication Commands: add_peer, append_peer_exclude_namespaces, append_peer_exclude_tableCFs, append_peer_namespaces, append_peer_tableCFs, disable_peer, disable_table_replication, enable_peer, enable_table_replication, get_peer_config, list_peer_configs, list_peers, list_replicated_tables, remove_peer, remove_peer_exclude_namespaces, remove_peer_exclude_tableCFs, remove_peer_namespaces, remove_peer_tableCFs, set_peer_bandwidth, set_peer_exclude_namespaces, set_peer_exclude_tableCFs, set_peer_namespaces, set_peer_replicate_all, set_peer_serial, set_peer_tableCFs, show_peer_tableCFs, update_peer_config Group name: snapshots Commands: clone_snapshot, delete_all_snapshot, delete_snapshot, delete_table_snapshots, list_snapshots, list_table_snapshots, restore_snapshot, snapshot Group name: configuration Commands: update_all_config, update_config Group name: quotas Commands: list_quota_snapshots, list_quota_table_sizes, list_quotas, list_snapshot_sizes, set_quota Group name: security Commands: grant, list_security_capabilities, revoke, user_permission Group name: procedures Commands: list_locks, list_procedures Group name: visibility labels Commands: add_labels, clear_auths, get_auths, list_labels, set_auths, set_visibility Group name: rsgroup Commands: add_rsgroup, balance_rsgroup, get_rsgroup, get_server_rsgroup, get_table_rsgroup, list_rsgroups, move_namespaces_rsgroup, move_servers_namespaces_rsgroup, move_servers_rsgroup, move_servers_tables_rsgroup, move_tables_rsgroup, remove_rsgroup, remove_servers_rsgroupSHELL USAGE:Quote all names in HBase Shell such as table and column names. Commas delimitcommand parameters. Type &lt;RETURN&gt; after entering a command to run it.Dictionaries of configuration used in the creation and alteration of tables areRuby Hashes. They look like this: {'key1' =&gt; 'value1', 'key2' =&gt; 'value2', ...}and are opened and closed with curley-braces. Key/values are delimited by the'=&gt;' character combination. Usually keys are predefined constants such asNAME, VERSIONS, COMPRESSION, etc. Constants do not need to be quoted. Type'Object.constants' to see a (messy) list of all constants in the environment.If you are using binary keys or values and need to enter them in the shell, usedouble-quote'd hexadecimal representation. For example: hbase&gt; get 't1', &quot;key\\x03\\x3f\\xcd&quot; hbase&gt; get 't1', &quot;key\\003\\023\\011&quot; hbase&gt; put 't1', &quot;test\\xef\\xff&quot;, 'f1:', &quot;\\x01\\x33\\x40&quot;The HBase shell is the (J)Ruby IRB with the above HBase-specific commands added.For more on the HBase Shell, see http://hbase.apache.org/book.html 根据上述信息，我们可以进一步的操作 HBase 数据库。我们实际开发中常用的**命令组(COMMAND GROUPS)**有：general、namespace、ddl、dml等，下面依次介绍这些内容： 通用命令 general 查看 HBase 状态 status，提供 HBase 的状态，如服务器的数量等 123hbase(main):001:0&gt; status1 active master, 0 backup masters, 1 servers, 0 dead, 4.0000 average loadTook 0.5268 seconds 查看 HBase 版本 version，提供正在使用 HBase 版本 123hbase(main):002:0&gt; version2.1.8, rd8333e556c8ed739cf39dab58ddc6b43a50c0965, Tue Nov 19 15:29:04 UTC 2019Took 0.0002 seconds 表引用命令提供帮助 table_help 提供有关用户的信息 whoami 1234hbase(main):003:0&gt; whoaminilera (auth:SIMPLE) groups: nileraTook 0.0283 seconds 操作命名空间 Namespace **命名空间(Namespace)**，相当于MySQL数据库中的DataBase。Namespace 命令包括：alter namespace、create_namespace、describe_namespace、drop_namespace、list_namespace、list_namespace_tables。下面将对一些常用命令进行介绍： 查看全部命名空间 list_namespace 123456hbase(main):001:0&gt; list_namespaceNAMESPACEdefaulthbase2 row(s)Took 0.5484 seconds 创建命名空间 create_namespace 用法：create_namespace 'ns' 123456789hbase(main):001:0&gt; create_namespace 'bigdata'Took 0.0432 secondshbase(main):002:0&gt; list_namespaceNAMESPACEbigdatadefaulthbase3 row(s)Took 0.0224 seconds 删除命名空间 drop_namespace 用法：drop_namespace 'ns'，删除命名空间时，命名空间必须为空。 查看命名空间 describe_namespace 用法：describe_namespace 'ns' 12345hbase(main):001:0&gt; describe_namespace 'bigdata'DESCRIPTION{NAME =&gt; 'bigdata'}Took 0.0068 seconds=&gt; 1 查看命名空间下的表 list_namespace_tables 用法：list_namespace_tables 'ns' 1234567hbase(main):001:0&gt; list_namespace_tables 'default'TABLElogsuser2 row(s)Took 0.3790 seconds=&gt; [&quot;logs&quot;, &quot;user&quot;] 数据定义语言 ddl DDL(Data Definition Language)数据定义语言，主要是进行定义/改变表的结构、数据类型、表之间的链接等操作。ddl 相关命令如下：alter、alter_async、alter_status、clone_table_schema、create、describe、disable、disable_all、drop、drop_all、enable、enable_all、exists、get_table、is_disabled、is_enabled、list、list_regions、locate_region、show_filters。下面将对一些常用命令进行介绍： 创建表 create 常见用法： ① create 'ns:tb', {NAME =&gt; 'cf', VERSION =&gt; 5} ​ 在命名空间 ns 下，创建一张表 tb，定义一个列族 cf。 ② 当在默认命名空间default下创建表时，可以省略 ns ③ create 'tb', 'cf1', 'cf2' ​ 在默认命名空间default下，创建一张表tb，并定义两个列族 cf1、cf2 ④ create 'tb', {NAME =&gt; 'cf1', VERSION =&gt; 5}, {NAME =&gt; 'cf2', VERSION =&gt; 5} ​ 在默认命名空间default下，创建一张表tb，并定义两个列族 cf1、cf2，并同时指定两个列族的版本为 5。 1234hbase(main):001:0&gt; create 'bigdata:person', {NAME =&gt; 'name', VERSIONS =&gt; 5}, {NAME =&gt; 'msg', VERSIONS =&gt; 5}Created table bigdata:personTook 1.5638 seconds=&gt; Hbase::Table - bigdata:person 查看表的详细信息 describe 用法：describe 'tb' 123456789101112hbase(main):010:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'msg', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}2 row(s)Took 0.1536 seconds 修改表 alter 表名创建时写的所有和列族相关的信息，都可以后续通过alter修改，包括增加、删除列族。 ① 增加列族和修改信息都使用覆盖的方法 ​ 修改列族的版本，VERSIONS =&gt; 6： 12345hbase(main):001:0&gt; alter 'bigdata:person', NAME =&gt; 'name', VERSIONS =&gt; 6Updating all regions with the new schema...1/1 regions updated.Done.Took 4.0145 seconds ​ 添加列族 tel： 12345hbase(main):002:0&gt; alter 'bigdata:person', NAME =&gt; 'tel', VERSIONS =&gt; 6Updating all regions with the new schema...1/1 regions updated.Done.Took 2.4498 seconds ​ 查看修改后的数据： 1234567891011121314151617hbase(main):003:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'msg', VERSIONS =&gt; '6', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}{NAME =&gt; 'tel', VERSIONS =&gt; '6', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}3 row(s)Took 0.0795 seconds ② 删除列族 ​ 删除列族可以用以下两种方式： 12345hbase(main):001:0&gt; alter 'bigdata:person', NAME =&gt; 'tel', METHOD =&gt; 'delete'Updating all regions with the new schema...1/1 regions updated.Done.Took 2.1046 seconds 12345hbase(main):002:0&gt; alter 'bigdata:person', 'delete' =&gt; 'msg'Updating all regions with the new schema...1/1 regions updated.Done.Took 2.9721 seconds ​ 然后查询修改后的数据： 123456789hbase(main):003:0&gt; describe 'bigdata:person'Table bigdata:person is ENABLEDbigdata:personCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'name', VERSIONS =&gt; '5', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}1 row(s)Took 0.0391 seconds 禁用表 disable 用法： disable 'ns:tb'或disable 'tb' 12hbase(main):001:0&gt; disable 'bigdata:person'Took 0.9384 seconds 删除表 drop 用法： drop 'ns:tb'或drop 'tb'，删除表时需要保证表是禁用的，否则会出现以下错误： 1234567hbase(main):001:0&gt; drop 'bigdata:person'ERROR: Table bigdata:person is enabled. Disable it first.For usage try 'help &quot;drop&quot;'Took 0.0248 seconds ​ 禁用表后再删除表： 12hbase(main):001:0&gt; drop 'bigdata:person'Took 1.7106 seconds 数据操纵语言 dml DML(Data Manipulation Language)数据操纵语言，主要是对数据进行增加、删除、修改操作。 写入数据 put 在 HBase 中如果想要写入数据，只能添加结构中最底层的 Cell。可以手动写入时间戳指定 Cell 的版本，推荐不写，默认使用当前的系统时间。如果重复写入相同 rowKey，相同列的数据，会写入多个版本进行覆盖。所以他同时兼具写入和修改的功能。 用法： ① put 'ns:tb', 'rk', 'col', 'value' ​ 向命名空间ns中的tb表中的行键为rk，列为col的位置写入值value。其中col为cf:col（即列族:列名）的格式。 ​ 如果重复向相同行号rk，相同col写数据，则会进行覆盖。 12345678910111213hbase(main):001:0&gt; put 'bigdata:student', '1001', 'info:name', 'zhangsan'Took 0.2415 secondshbase(main):002:0&gt; put 'bigdata:student', '1001', 'info:name', 'lisi'Took 0.0121 secondshbase(main):003:0&gt; put 'bigdata:student', '1001', 'info:name', 'wangwu'Took 0.0342 secondshbase(main):004:0&gt; put 'bigdata:student', '1002', 'info:name', 'zhaoliu'Took 0.0082 secondshbase(main):005:0&gt; put 'bigdata:student', '1003', 'info:age', '10'Took 0.0050 secondshbase(main):006:0&gt; put 'bigdata:student', '1003', 'info:sex', 'male'Took 0.0054 seconds ② put 't1', 'r1', 'c1', 'value'用法同上。 读取数据 get/scan 读取数据的方法有两个：get 和 scan get最大范围是一行数据，也可以进行列的过滤，读取数据的结果为多行 Cell。 scan是扫描数据，能够读取多行数据，不建议扫描过多数据，推荐使用 startRow 和 stopRow 来控制读取的数据，默认范围左闭右开。 ① get命令 1234567891011121314Some examples: hbase&gt; t.get 'r1' #查看'r1'的数据 hbase&gt; t.get 'r1', {TIMERANGE =&gt; [ts1, ts2]} hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1'} #过滤单列, 只显示 'c1' hbase&gt; t.get 'r1', {COLUMN =&gt; ['c1', 'c2', 'c3']} #过滤多列, 只显示 'c1', 'c2', 'c3' hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1', TIMESTAMP =&gt; ts1} hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1', TIMERANGE =&gt; [ts1, ts2], VERSIONS =&gt; 4} hbase&gt; t.get 'r1', {COLUMN =&gt; 'c1', TIMESTAMP =&gt; ts1, VERSIONS =&gt; 4} hbase&gt; t.get 'r1', {FILTER =&gt; &quot;ValueFilter(=, 'binary:abc')&quot;} hbase&gt; t.get 'r1', 'c1' hbase&gt; t.get 'r1', 'c1', 'c2' hbase&gt; t.get 'r1', ['c1', 'c2'] hbase&gt; t.get 'r1', {CONSISTENCY =&gt; 'TIMELINE'} hbase&gt; t.get 'r1', {CONSISTENCY =&gt; 'TIMELINE', REGION_REPLICA_ID =&gt; 1} 1234567891011121314151617hbase(main):001:0&gt; get 'bigdata:student', '1001'COLUMN CELL info:name timestamp=1717580289267, value=wangwu1 row(s)Took 0.0645 secondshbase(main):002:0&gt; get 'bigdata:student', '1001', {COLUMN =&gt; 'info:name'}COLUMN CELL info:name timestamp=1717580289267, value=wangwu1 row(s)Took 0.0107 secondshbase(main):003:0&gt; get 'bigdata:student', '1003', {COLUMN =&gt; 'info:age'}COLUMN CELL info:age timestamp=1717580366636, value=101 row(s)Took 0.0185 seconds ② scan 命令 12345678910111213141516Some examples: hbase&gt; scan 'hbase:meta' hbase&gt; scan 'hbase:meta', {COLUMNS =&gt; 'info:regioninfo'} hbase&gt; scan 'ns1:t1', {COLUMNS =&gt; ['c1', 'c2'], LIMIT =&gt; 10, STARTROW =&gt; 'xyz'} hbase&gt; scan 't1', {COLUMNS =&gt; ['c1', 'c2'], LIMIT =&gt; 10, STARTROW =&gt; 'xyz'} hbase&gt; scan 't1', {COLUMNS =&gt; 'c1', TIMERANGE =&gt; [1303668804000, 1303668904000]} hbase&gt; scan 't1', {REVERSED =&gt; true} hbase&gt; scan 't1', {ALL_METRICS =&gt; true} hbase&gt; scan 't1', {METRICS =&gt; ['RPC_RETRIES', 'ROWS_FILTERED']} hbase&gt; scan 't1', {ROWPREFIXFILTER =&gt; 'row2', FILTER =&gt; &quot; (QualifierFilter (&gt;=, 'binary:xyz')) AND (TimestampsFilter ( 123, 456))&quot;} hbase&gt; scan 't1', {FILTER =&gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)} hbase&gt; scan 't1', {CONSISTENCY =&gt; 'TIMELINE'} hbase&gt; scan 't1', {ISOLATION_LEVEL =&gt; 'READ_UNCOMMITTED'} hbase&gt; scan 't1', {MAX_RESULT_SIZE =&gt; 123456} 123456789101112131415hbase(main):001:0&gt; scan 'bigdata:student'ROW COLUMN+CELL 1001 column=info:name, timestamp=1717580289267, value=wangwu 1002 column=info:name, timestamp=1717580320927, value=zhaoliu 1003 column=info:age, timestamp=1717580366636, value=10 1003 column=info:sex, timestamp=1717581149533, value=male3 row(s)Took 0.0338 secondshbase(main):025:0&gt; scan 'bigdata:student', {STARTROW =&gt; '1001', STOPROW =&gt; '1003'}ROW COLUMN+CELL 1001 column=info:name, timestamp=1717580289267, value=wangwu 1002 column=info:name, timestamp=1717580320927, value=zhaoliu2 row(s)Took 0.0118 seconds 删除数据 delete/deleteall 删除数据的方式有两个：delete和deleteall delete 表示删除一个版本的数据，即为 1 个 Cell，不填写版本默认删除最新的一个版本。 deleteall 表示删除所有版本的数据，即为当前行当前列的多个 Cell。执行命令会标记数据为要删除，不会直接彻底删除，删除只在特定时期清理磁盘时进行。 ① delete 123456789101112131415161718192021222324252627hbase(main):001:0&gt; put 'bigdata:student', '1001', 'info:name', 'zhangsan'Took 0.3910 secondshbase(main):002:0&gt; put 'bigdata:student', '1001', 'info:name', 'lisi'Took 0.2024 secondshbase(main):003:0&gt; put 'bigdata:student', '1001', 'info:name', 'wangwu'Took 0.1559 secondshbase(main):004:0&gt; scan 'bigdata:student'ROW COLUMN+CELL 1001 column=info:name, timestamp=1717584831277, value=wangwu 1002 column=info:name, timestamp=1717580320927, value=zhaoliu 1003 column=info:age, timestamp=1717580366636, value=10 1003 column=info:sex, timestamp=1717581149533, value=male3 row(s)Took 0.0083 secondshbase(main):005:0&gt; delete 'bigdata:student', '1001', 'info:name'Took 0.0055 secondshbase(main):006:0&gt; scan 'bigdata:student'ROW COLUMN+CELL 1001 column=info:name, timestamp=1717584831277, value=lisi 1002 column=info:name, timestamp=1717580320927, value=zhaoliu 1003 column=info:age, timestamp=1717580366636, value=10 1003 column=info:sex, timestamp=1717581149533, value=male3 row(s)Took 0.0087 seconds ② deleteall 1 2.2.2 API操作根据官方 API 介绍，HBase 的客户端连接由 ConnectionFactory 类来创建，用户使用完成之后需要手动关闭连接。同时连接是一个重量级的，推荐一个进程使用一个连接。对 HBase 的命令通过连接中的两个属性 Admin 和 Table 来实现。其中 Admin 主要管理 HBase 的元数据，如创建、修改表格信息，也就是 DDL 操作；Table 主要用于表格的增加、删除数据，也就是 DML 操作。 环境搭建 使用 IDEA 创建 Maven 项目，并修改 pom.xml 文件，添加 HBase 所需要用到的依赖。 1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.4.11&lt;/version&gt; &lt;!-- 如果报错, 需要排除 javax.el 拓展 --&gt; &lt;!-- 因为 2.4.11 对应的是一个测试版本的 javax.el 包 --&gt; &lt;!-- 需要先排除这个包后再添加正式版的 javax.el 包 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- 添加正式版的 javax.el 包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;version&gt;3.0.1-b06&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 单线程使用连接 下面展示了一种单线程使用连接的方式，实际开发中实际上很少这样做。 123456789101112131415161718192021222324252627282930313233package com.sdutcm;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.AsyncConnection;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.CompletableFuture;public class HBaseConnection { public static void main(String[] args) throws IOException { // 1. 创建连接配置对象 Configuration conf = new Configuration(); // 2. 添加配置参数 conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;bigdata&quot;); // 这些配置都写在 hbase-site.xml 中 // 3. 创建连接 // 默认创建同步连接 Connection connection = ConnectionFactory.createConnection(conf); // 也可以创建异步连接: 不推荐使用异步连接 CompletableFuture&lt;AsyncConnection&gt; asyncConnection = ConnectionFactory.createAsyncConnection(conf); // 4. 使用连接 System.out.println(connection); // 5. 关闭连接 connection.close(); }} 多线程使用连接 实际开发中，因为 HBase 的连接是重量级的，所以我们在每个客户端中一般只创建一个（类似于单例模式）。所以我们对代码进行修改，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.sdutcm;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.AsyncConnection;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.CompletableFuture;public class HBaseConnection { // 声明一个静态属性 public static Connection connection = null; static { // 1. 创建连接配置对象: 当完成 resources 目录的配置后, 我们可以直接注释掉创建配置的部分 // 直接进行创建连接操作 // Configuration conf = new Configuration(); // 2. 添加配置参数 // 实际开发中, 不应该在代码中显式的写参数, 而是将参数写在 resources 下的配置文件中 // 将虚拟机的 hbase-site.xml 放到 resources 目录下 // conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;bigdata&quot;); // 这些配置都写在 hbase-site.xml 中 // 3. 创建连接 // 默认创建同步连接 try { // 这里修改为无参构造 // connection = ConnectionFactory.createConnection(conf); // 这里通过查看 ConnectionFactory.createConnection() -&gt; 查看 create() -&gt; 可以发现 HBase 官方文档添加了两个配置文件 // 分别为 hbase-default.xml 和 hbase-site.xml // 所以我们可以直接复制虚拟机的 hbase-site.xml 添加到 resources 目录下, 并且将这里改为无参构造 // 无参则默认使用读取本地 hbase-site.xml 文件的方式添加参数 connection = ConnectionFactory.createConnection(); } catch (IOException e) { e.printStackTrace(); } } // 关闭连接方式 public static void closeConnection() throws IOException { // 判断连接是否为空 if (connection != null) { connection.close(); } } public static void main(String[] args) throws IOException { // 直接使用创建好的连接, 不要在 main 线程里面单独创建连接 System.out.println(HBaseConnection.connection); // 使用完连接后需要关闭连接 HBaseConnection.closeConnection(); }} 获取 Admin 123// 获取 Admin// Admin 的连接式轻量级的, 不是线程安全的, 不推荐池化或者缓存这个连接Admin admin = connection.getAdmin(); 创建命名空间 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.sdutcm;import org.apache.hadoop.hbase.NamespaceDescriptor;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import java.io.IOException;public class HBaseDDL { // 声明一个静态属性, 这样我们可以在不同的类中, 调用到同一个对象 public static Connection connection = HBaseConnection.connection; /** * @brief 创建命名空间 * @param namespace 命名空间名称 */ public static void createNamespace(String namespace) throws IOException { // 1. 获取 Admin // Admin 的连接式轻量级的, 不是线程安全的, 不推荐池化或者缓存这个连接 Admin admin = connection.getAdmin(); // 2. 调用方法创建命名空间 // 2.1 创建命名空间描述 NamespaceDescriptor.Builder builder = NamespaceDescriptor.create(namespace); // 2.2 给命名空间添加需求 builder.addConfiguration(&quot;user&quot;, &quot;sdutcm&quot;); // 2.3 使用 builder 构造出对应的添加完参数的对象, 完成创建 admin.createNamespace(builder.build()); // 关闭 admin admin.close(); } public static void main(String[] args) throws IOException { // 测试创建命名空间 createNamespace(&quot;sdutcm&quot;); // 其他代码 System.out.println(&quot;其他代码&quot;); // 关闭 HBase 连接 HBaseConnection.closeConnection(); }} ​ 结果如下： 12345678910111213hbase(main):001:0&gt; list_namespaceNAMESPACEdefaulthbasesdutcm &lt;&lt;&lt; 可以看到 sdutcm 已经被创建出来了3 row(s)Took 8.0120 secondshbase(main):002:0&gt; describe_namespace &quot;sdutcm&quot;DESCRIPTION{NAME =&gt; 'sdutcm', user =&gt; 'sdutcm'} &lt;&lt;&lt; 这里是我们添加的描述Took 0.7576 seconds=&gt; 1 多异常处理 1 判断表格是否存在 创建表格 📕 3. 底层原理3.1 进程架构3.1.1 Master架构3.1.2 RegionServer架构3.2 写流程3.2.1 写入顺序3.2.2 刷新机制3.3 读流程3.3.1 读取顺序3.3.2 合并数据优化3.4 文件合并3.4.1 大合并3.4.2 小合并Region拆分自定义预分区系统拆分🔧 企业开发TSDB模式基础表格模式自定义API整合框架Phoenix 读写数据Hive 分析数据","link":"/2024/06/02/QuickPassHBase/"},{"title":"ScalaQuickIN","text":"快速入门 Scala[TOC] 1. 学习目标 2. Scala 介绍2.1 Scala 简介Scala 创始人为 Martin Odersky，马丁·奥德斯基。JDK5 和 JDK8 版本的Java编译器 javac 都是由他和他的团队编写的。 2.2 Scala 的六大特征一句话总结：Scala 是一门以 JVM 为运行环境的静态类型编程语言，具备面向对象及函数是编程的特性。下面是官网对Scala特性的介绍： ① SEAMLESS JAVA INTEROP 无缝与 JAVA互操作 Java 和 Scala 可以混编，在写 Scala 代码时可以引用 Java 的类。 Scala 运行在 JVM 上，所以 Java 和 Scala 可以自由的混合使用。这也就是在日常编写代码的过程中，经常看到Scala引用了很多Java包的原因。 ② TYPE INFERENCE 类型推断（自动推测类型） 使类型系统不是那么的死板（静态）。不要为了类型系统工作，而是让类型系统为你工作。 Scala是一种弱类型语言，即它在编写代码时，不需要像强类型语言（如 Java、C++等）定义变量的类别。 Scala有Int、Long、Short、Byte、Float、Double、Char、Boolean这些基础数据类型。但与 Java 不同，在定义变量时，Scala用 var/val 来定义，让其自主判断数据类型。 其中val是常量，var是变量，一般来说，用val比较多（涉及资源的利用）。 12345// Java 中定义一个变量String name = &quot;ZhangSan&quot;;// Scala 中定义一个变量var name = &quot;ZhangSan&quot;; ③ CONCURRENCY &amp; DISTRIBUTION 并发和分布式（Actor） Scala 在操作集合时使用了数据并行操作。并且使用Actor（Actor是一种不共享数据，依赖于消息传递的并发编程模式， 有效的避免了死锁、资源争夺等情况）来解决并发和分布式中的一些问题。或者使用 futures 类进行异步编程。 使用 Scala 进行并发以及分布式场景下的开发时有得天独厚的优势。 ④ TRAITS 特性 将 Java 风格接口的灵活性与类的强大功能相结合。认为是一种多重继承。 其实这就相当于 Java 的接口，但实际上它比接口还功能强大。 与接口不同的是，它还可以定义属性和方法的实现。 简单来说：TRAITS = Interface + Abstract。 ⑤ PATTERN MATCHING 模式匹配 更强大的 Switch，它可以匹配类的层级结构、序列以及更多。 ⑥ HIGHER-ORDER FUNCTIONS 高阶函数 函数是一级对象。在保证类型安全的情况下编写它们。在任何地方使用它们，传递给任何东西，即函数可以作为参数传递到函数中。 2.3 具体应用 Kafka：分布式消息队列，内部代码经常用来处理并发的问题，用 Scala 可以大大简化其代码。 Spark：方便处理多线程场景，另外Spark主要用作内存计算，经常要用来实现复杂的算法，利用Scala这种函数式编程语言可以大大简化代码。 3. Scala 的安装和使用3.1 Windows 下安装 ScalaScala下载地址：The Scala Programming Language (scala-lang.org) ① 进入下载地址后，选择合适的Scala版本进行安装。记住安装的路径，方便配置环境变量。 ② 配置环境变量 SCALA_HOME ③ 验证是否安装成功：进入命令行CMD，输入 scala -version，显示版本号及表明配置成功。 3.2 IDEA 2022 中配置 Scala 插件① 进入IDEA，Files→Settings→Plugins ② 进入Marketolace界面，直接搜索Scala，安装即可。 ③ 在编写 Scala 代码时，双击 Shift，选择 Add Framework 后，添加 Scala 框架，即可编写 Scala 代码。 4. Scala 基础4.1 数据类型 数据类型 描述 Byte 8-bit 的有符号数字，范围在 -128 ~ 127 Short 16-bit 的有符号数字，范围在 -32768 ~ 32767 Int 32-bit 的有符号数字，范围在 -2147483648 ~ 2147483647 Long 64-bit 的有符号数字，范围在 -9223372036854775808 ~ 9223372036854775807 Float 32-bit IEEE 754 单精度浮点数 Double 64-bit IEEE 754 双精度浮点数 Char 16-bit Unicode 字符，范围 U+0000 ~ U+FFFF String 字符串 Boolean 布尔类型 Unit 表示空值，与其他语言中的 void 相同 Null 空值或者空引用 Nothing 所有其他类型的子类型，表示没有值 Any 所有类型的超类，任何类型都属于Any AnyRef 所有引用类型的超类 AnyVal 所有值类型的超类 Nil 表示长度为0的List 比较特殊的 None，是Option的两个子类之一，另一个是Some，用于安全的函数返回值。 Scala 推荐在可能返回空的方法使用 Option[X] 作为返回类型。如果有值就返回 Some[X]，否则返回 None。 123456def get(key: A): option[B] = { if (contains(key)) Some(getValue(key)) else None} 4.2 变量和常量的声明 变量用 var 定义，可修改 常量用 val 定义，不可修改 定义变量或常量的时候，也可以写上返回值的类型，一般省略，如 val a: Int = 10 常量不可以再赋值 1234567var name = &quot;zhangsan&quot;println(name) // zhangsanname = &quot;lisi&quot;println(name) // lisival gender = &quot;m&quot;gender = &quot;m&quot; // [Error] 不可以给常量赋值 4.3 键盘标准输入编程中可以通过键盘输入语句来接收用户输入的数据（也就是 Java 中的 Scanner 对象）。在 Scala 中只需要导入对应的包，比 Java 还要简单，不需要实例化对象。 1234567891011import scala.io.StdInobject Test { def main(args: Array[String]): Unit = { println(&quot;Please Input Your Name: &quot;) val name = StdIn.readLine() println(&quot;Please Input Your Age: &quot;) val age = StdIn.readInt() printf(&quot;Your Name is %s, Your Age is %d&quot;, name, age) }} 如上述代码一样，printf() 的用法和 Java 中一样，为格式化输出。注意使用规范即可。 符号 含义 %d 十进制数字 %s 字符串 %c 字符 %e 指数浮点数 %f 浮点数 4.4 类Class 和 对象Object 在编写 Scala 代码时，一般不加分号;，如果一行中有多条语句，则可以用分号隔开，如：var a = 10; var b = 20。 class 默认实现了 getter/setter 方法。 class 中如果有参数传入，那么这个构造器就是这个类的默认构造器。 class 在被 new 新建对象的时候，除了方法内部不执行，其他地方的代码都会执行，类似于Java中的工具类。 Object 里面不能传递参数，Object 里面的属性和方法都是静态的，类似于 Java 中 static 修饰的东西，类似于 Java 中的工具类。 伴生类和伴生对象，在一个 Scala 文件中，如果 Class 和 Object 的名字一样，则互为伴生类和伴生对象。他们可以互相访问到互相的私有成员变量。 123456789101112131415161718192021222324252627282930class Person(xname:String, xage: Int) { val name = xname val age = xage var money = 100 /** * 重写构造器, 必须调用类的默认构造器 * @param xname * @param xage * @param xmoney */ def this(xname: String, xage: Int, xmoney: Int) { this(xname, xage) money = xmoney } println(&quot;Checkpoint&quot;) // `class` 在被 `new` 新建对象的时候，除了方法内部不执行，其他地方的代码都会执行 def test: Unit = { println(&quot;Inside Function&quot;) // 方法内部不会执行 } println(&quot;Checkpoint&quot;) // `class` 在被 `new` 新建对象的时候，除了方法内部不执行，其他地方的代码都会执行}object Test { def main(args: Array[String]): Unit = { val person1 = new Person(&quot;zhangsan&quot;, 20) val person2 = new Person(&quot;lisi&quot;, 20, 1000) println(person.name + &quot;:&quot; + person.age) // 默认实现了 getter/setter 方法 }} 4.5 IF-ELSE 语句Scala 中的条件判断语句同 Java 中的条件判断语句，语法结构基本一致。 123456789101112131415import scala.io.StdInobject IfDemo { def main(args: Array[String]): Unit = { println(&quot;请输入年龄&quot;) val age = StdIn.readInt() if (age &gt;= 18 &amp;&amp; age &lt;= 100) { println(&quot;成年&quot;) } else if (age &gt;= 0 &amp;&amp; age &lt; 18) { println(&quot;未成年&quot;) } else { println(&quot;请输入 0 ~ 100 的数字&quot;) } }} 4.6 Loop 循环语句 to 和 until 语句的区别 1234567object ToUntilDemo { def main(args: Array[String]): Unit = { println(1 to 10) // to 表示 [1, 10], 输出: Range(1, 2, 3, ..., 10) println(1 until 10) // until 表示 [1, 10), 输出: Range(1, 2, 3, ..., 9) println(1 to (10, 2)) // 表示按照步长为`2`来输出数据, 输出: Range(1, 3, 5, 7, 9) }} for 循环 12345678910111213141516171819202122232425262728293031object ForDemo { def main(args: Array[String]): Unit = { for (i &lt;- 1 to 10) { print(i + ' ') // 输出: 1 2 3 4 5 6 7 8 9 10 } // Scala 中的 `for` 循环可以将 if 语句直接写在 for 循环中 for (i &lt;- 1 to 10; if i &gt; 5; if i % 2 == 0) { printf(&quot;%d &quot;, i) // 输出: 6 8 10 } // 上面的写法可以等同于下面的写法 for (i &lt;- 1 to 10) { if (i &gt; 5 &amp;&amp; i % 2 == 0) { printf(&quot;%d &quot;, i) } } // 双重 for 循环 for (i &lt;- 1 to 10; j &lt;- 1 to 5) { println(i + &quot;:&quot; + j) } // 上面的写法可以等同于下面的写法 for (i &lt;- 1 to 10) { for (j &lt;- 1 to 5) { println(i + &quot;:&quot; + j) } } }} Scala 中不能使用类似于 x++、x-- 的操作，需要使用 x += 1 或者 x -= 1 来完成自增或者自减操作。 for 循环使用 yield 关键字返回一个集合，for { 子句 } yield {变量或表达式}，for 循环中的 yield 会把当前的元素记下来，保存在集合中，循环结束后将返回该集合。Scala 中 for 循环是有返回值的。如果被循环的是 Map，返回的就是 Map，被循环的是 List，返回的就是 List，以此类推。 12345678910111213object Loop { def main (args: Array[String]): Unit = { val range = 1 to 10; for (num &lt;- range if num % 2 == 0 if num &gt; 5) { printf(&quot;%d&quot;, num); // 6 8 10 } val result_1 = for (num&lt;-range if num % 2 == 0 if num &gt; 5) yield num // yield 变量 println(result_1) // Vector(6, 8, 10) val result_2 = for (num&lt;-range if num &gt; 5) yield num % 2 // yield 表达式 println(result_2) // Vector(0, 1, 0, 1, 0) }} while 和 do...while 1234567891011121314object Loop { def main (args: Array[String]): Unit = { var index = 0 while (index &lt; 10) { println(&quot;第&quot; + index + &quot;次循环&quot;) index += 1 } do { index += 1 println(&quot;第&quot; + index + &quot;次循环&quot;) } while(index &lt; 20) }} 5. Scala 函数(方法)5.1 函数的定义12345678// 函数(方法) 的定义// def 函数名(参数x: 参数类型, 参数y: 参数类型): 返回类型 = {// 函数体// }def function_name(x: Int, y: String): Unit = { function_body} 注意事项： Scala 使用 def 关键字告诉编译器这是一个函数（方法） 我们可以通过在参数列表后面加一个冒号:和类型来显式地指定返回类型。 函数可以写返回类型，也可以不写，会自动推断（最后一行是什么类型，就被推断成什么类型）。有时候不能省略，必须写，比如在递归函数中或者函数的返回值是函数类型的时候。 Scala 中函数有返回值时，可以写 return，也可以不写 return，不写 return 时会把函数中最后一行当做结果返回。当写 return 时，必须要写函数的返回类型。 传递给方法的参数可以在方法中使用，并且 Scala 规定：方法的传过来的参数为常量 val 而不是变量 var 。 如果去掉函数体前面的等号=，那么这个函数返回类型必定是 Unit。这种说法无论函数体里面什么逻辑都成立，Scala 可以把任意类型转换为 Unit。假设，函数里面的逻辑最后返回了一个 String，那么这个返回值会被转换成 Unit，原本逻辑的值会被丢弃。这种方法往往适用于无返回值的函数中。 5.2 递归函数 12345678910111213141516object FunctionDemo { /** * 递归函数: * 关键点在于递归的定义, 终止条件(避免无休止的递归, 导致栈溢出问题) */ def f1(num: Int): Int = { if (num == 1) { return num } num * f1(num-1) // f1(5) = 5*f1(4) = 5*4*f1(3) = 5*4*3*f1(2) = 5*4*3*2*f1(1) } def main (args: Array[String]): Unit = { println(f1(5)) }} 5.3 包含参数默认值的函数 和其他语言没有任何区别 默认值的函数中，如果传入的参数个数与函数定义相同，则传入的数值会覆盖默认值。 如果不想覆盖默认值，且传入的参数个数小于定义的函数的参数，则需要指定参数名称。 123456789101112131415object FunctionDemo { /** * 包含参数默认值的函数 */ def f2(x: Int=5, y: Int=10): Int = { a + b } def main (args: Array[String]): Unit = { println(f2()) // 输出: 15 println(f2(10, 20)) // 输出: 30 println(f2(10)) // 输出: 20 println(f2(y=30)) // 输出: 35 }} 5.4 可变参数个数的函数123456789101112131415161718object FunctionDemo { /** * 可变参数个数的函数 * 传入多个参数时, 多个参数之间用逗号分隔 * 传入的参数其实就是不定长数组 */ def f3(elements: Int*): Int = { var sum = 0 for (element &lt;- elements) { sum += element } sum } def main (args: Array[String]): Unit = { println(f3(1, 2, 3, 4, 5)) // 输出 }} 5.5 匿名函数匿名函数有以下几种： 有参匿名函数 无参匿名函数 有返回值的匿名函数 在定义和使用匿名函数时： 可以将匿名函数返回给 val 定义的值 匿名函数不能显示声明函数的返回类型 在 Scala 中 ，大多数情况下 =&gt; 是匿名函数的显著标志。 123456789101112131415161718192021222324252627object FunctionDemo { /** * 匿名函数: 经常和高阶函数一起使用 * 1. 有参匿名函数 * 2. 无参匿名函数 * 3. 有返回值的匿名函数 */ def main (args: Array[String]): Unit = { // 有参数匿名函数 val value1 = (a: Int) =&gt; { println(a) } value1(1) // 无参数匿名函数 val value2 = () =&gt; { println(&quot;无参数匿名函数&quot;) } value2() // 有返回值的匿名函数 val value3 = (a: Int, b: Int) =&gt; { a + b } println(value3(4, 4)) }} 5.6 嵌套函数嵌套函数其实就是函数里套了函数。 1234567891011121314151617object FunctionDemo { def f5 (num: Int): Int = { @tailrec def f6(a: Int, b: Int): Int = { if (a == 1) { b } else { f6(a-1, a*b) } } f6(num, 1) } def main (args: Array[String]): Unit = { f5(5) }} 5.7 偏应用函数偏应用函数是一种表达式，不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。 12345678910111213141516171819202122object FunctionDemo { def main (args: Array[String]): Unit = { // 这里只是一个普通的函数 def log(date: Date, log: String) { println(&quot;Date is &quot; + date + &quot;, Log is &quot; + log) } // 按照普通函数的方法使用 val date = new Date() // 与 Java 混编 log(date, &quot;log1&quot;) log(date, &quot;log2&quot;) log(date, &quot;log3&quot;) // 我们发现, 上面的程序除了 log 参数在改变, date 参数没有变化 // 此时我们可以使用偏应用函数来优化 // _ 下划线可以理解为一个变化的参数, 而 date 可以理解为一个固定的参数(他本身可能是变化的) val logWithDate = log(date, _:String) logWithDate(&quot;log_11&quot;) logWithDate(&quot;log_12&quot;) logWithDate(&quot;log_13&quot;) }} 5.8 高阶函数高阶函数：函数的参数是函数，或者函数的返回类型是函数，或者函数的参数和函数的返回类型是函数的函数。 函数的格式为 (A)=&gt;B, 后面没有函数体, 此函数接收类型 A 的参数, 返回类型 B 的函数。 函数的参数是函数：其实就是定义一个传入参数和返回类型的模板参数，但是这个模板需要从其他地方实现。 1234567891011121314151617// 函数的参数是函数def f7(a: Int, func: (Int, Int) =&gt; Int): Int { val result = func(1, 2) // 这里将函数作为参数传入 f7, 而 result 使用了这个函数 // 这里我理解为模板, result 使用了一个 参数为(Int, Int), 返回类型为 Int 的函数模板 // 那么这个模板的具体实现是什么, 则需要从其他地方实现 a * result}def f8(x: Int, y: Int): Int = { // 这里就是对于模板的一种实现 x + y}println(f7(5, f8)) // 使用这种方式将 f8 传入, 并且打印输出, 结果为: 15// 当然我们也可以结合匿名函数一起实现println(f7(5, (x: Int, y: Int) =&gt; {x + y})) 函数的返回类型是函数 12345678910// 函数的返回类型是函数def f9(a: Int, b: Int): (String, String) =&gt; String = { def f10(c: String, d: String): String = { a + &quot; &quot; + b + &quot; &quot; + c + &quot; &quot; + d } f10 // 返回一个函数}// 像这种括号连着括号的`()()`, 往往代表着出现了函数的返回类型是函数的函数println(f9(1, 2)(&quot;3&quot;, &quot;4&quot;)) // 输出: 1 2 3 4 函数的参数和函数的返回类型是函数 12345678def f11(x: Int, f: (Int, Int) =&gt; Int): (Int, Int) =&gt; Int = { f}println(f11(1, (a: Int, b: Int) =&gt; { a + b } ) (100, 200)) // 输出: 300// _ 类似于 Java 中的 *. 通配符, 变量只使用一次的时候可以简写如下:println(f11(1, (_ + _))(100, 200)) // 输出: 300 5.9 柯里化函数柯里化函数，或称颗粒化函数，将参数变成颗粒散落简而言之就是将参数不断拆分。柯里化函数基本是在做这么一件事情：只传递给函数一部分参数来调用它，让它返回一个函数去处理剩下的参数。如果写成公式文字就是这样： 1234fn(a, b, c, d) =&gt; fn(a)(b)(c)(d)fn(a, b, c, d) =&gt; fn(a)(b, c, d)fn(a, b, c, d) =&gt; fn(a, b)(c)(d)fn(a, b, c, d) =&gt; fn(a, b, c)(d) 可以理解为高阶函数的简化，类似于返回类型为函数的函数。 12345678def f12(a: Int, b: Int, c: Int, d: Int) = { a + b + c + d}// 柯里化函数: 可以理解为高阶函数的简化def f13(a: Int, b: Int)(c: Int, d: Int) = { a + b + c + d } 6. Scala 字符串Scala 中的字符串和 Java 中的字符串用法几乎完全相同。 String 和 StringBuilder的区别：String 不可修改，StringBuilder 可修改。 6.1 String123456789101112package com.szy.inspur.subowen.rdd.baseobject StringDemo { def main(args: Array[String]): Unit = { val str_1: String = &quot;aabbccdd&quot; println(str_1.indexOf(&quot;a&quot;)) // 输出: 0 println(str_1.indexOf(98)) // 输出: 2, 这里输入的整数是 ASCII 码 val str_2: String = &quot;AABBCCDD&quot; println(str_1 == str_2) // 输出: false println(str_1.compareToIgnoreCase(str_2)) // 输出: 0, 如果 str != str_2, 则输出 `-1` }} 6.2 StringBuilder1234567891011121314151617181920212223package com.szy.inspur.subowen.rdd.baseimport scala.collection.mutableobject StringDemo { def main(args: Array[String]): Unit = { val stringBuilder = new mutable.StringBuilder stringBuilder.append(&quot;abc&quot;) println(stringBuilder) val result = 1 .+ (2) // Scala 中的运算操作其实都是调用函数 // 因此, `result 1 .+ (2)` 其实和 result = 1 + 2 一样 // StringBuilder 可以 `+=` Char 类型, 而不能 `+=` String 类型 // 普通的 `+` 只有 Char 类型追加在后面 stringBuilder ++= &quot;c&quot;; println(stringBuilder) // 输出: abcc stringBuilder += 'd'; println(stringBuilder) // 输出: abccd // stringBuilder += &quot;e&quot;; println(stringBuilder) // [ERROR] // stringBuilder ++= 'f'; println(stringBuilder) // [ERROR] stringBuilder + 'e'; println(stringBuilder) // 输出: abccde stringBuilder + &quot;f&quot;; println(stringBuilder) // 输出: abccde, 可以看到 &quot;f&quot; 没有被添加到字符串后面 }} 6.3 String 的操作方法万金油append，append()不受类型的限制。 7. 集合7.1 数组7.1.1 创建数组12345678910111213141516171819202122232425262728293031323334/** * 创建数组的两种方式 * 1. new Array[String](3) * 2. 直接 Array */object ArrayDemo { def main(args: Array[String]): Unit = { // 方式 1 // 创建一个 Array val array = Array(1, 2, 3) println(array(0)) // 用 for 循环遍历一个 Array for(x &lt;- array) { println(x) } // 用 foreach 遍历一个 array // foreach( function ) array.foreach(i =&gt; {println(i)} ) array.foreach(println(_)) array.foreach(println) println() // 方式 2 val array_use_length = new Array[Int](3) array_use_length.foreach(i =&gt; { println(i) }) // 0, 0, 0 println(array_use_length.isEmpty) // 输出: false println(array_use_length.length) // 输出: 3 }} 7.2 集合 Set7.3 集合 Map7.4 元组 Tuple7.4.1 元组的定义","link":"/2024/06/10/ScalaQuickIN/"},{"title":"OnlineTravelBigdataPlatform","text":"🌳 在线旅游大数据平台项目 完成度： 100% 日期任务清单 项目开始日期: 2024-06-11 了解项目的背景以及整个系统的架构 了解系统需要完成的主要功能 了解系统整个架构 完成数据服务端的部署 完成数据客户端的部署 了解数据集 认识消息队列 Kafka 完成消息队列 Kafka 的部署 了解消息队列 Kafka 的基本应用 使用 Flume 收集数据到 Kafka 2024-06-12 了解实时数据分析所用到的技术 了解 SparkStreaming 和 Flink 了解 SparkStreaming 的核心概念 了解数据源 借助netcat实践Kafka 了解转换操作 具体实施任务：处理数据 2024-06-13 了解数据库连接池 了解如何向 MySQL 数据库中写入数据 借助 alibaba Druid 库实现一个数据库连接工具类 编写案例：WordCount 具体实施任务，将代码中生成的数据写入MySQL。 2024-06-14 了解什么是 Kafka Offset 维护 Kafka Offset 具体实施任务，将 Kafka 的 Offset 配合 MySQL 用代码进行维护。 2024-06-17 进行后端开发 进行前端开发 2024-06-18 进行热力图的绘制 进行人流量柱状图的绘制 进行人流量趋势图的绘制 2024-06-19 项目结束日期 文件目录 [TOC] 📕 1. 项目概述1.1 项目背景随着信息技术的飞速发展，旅游行业正迅速融入数字化转型的浪潮中。旅游大数据的产生和积累为行业提供了前所未有的洞察力。然而，传统的数据处理方法往往难以应对数据的海量性和实时性需求。 1.2 项目介绍本项目旨在构建一个旅游大数据实时分析和监控系统，系统主要包括旅游数据分析和实时监控两大模块，旅游数据分析模块是基于Spark Streaming对济南各景点的人流量数据进行实时处理和分析，实时监控模块是基于SpringBoot对分析的结果进行可视化展示。 🚧 2. 系统功能及架构2.1 系统主要功能 数据实时收集：通过 Flume 实时采集手机移动信令数据（数据生成器生成的模拟数据），发送到 Kafka。 数据实时处理分析：通过Spark Streaming 消费 Kafka 数据，主要完成以下分析： 各景点人流量实时统计（热力图，每秒钟） 各景点人流量随时间增长情况/各景点人流量随时间变化趋势(每分钟) 实时监控：通过 SpringBoot + MyBatis 构建旅游监控系统，基于高德地图完成每秒钟人流量热力图展示，基于 Echarts 完成每分钟流量柱状图和每分钟人流量变化折线图。 2.2 系统结构与技术选型 项目开发工具：IntelliJ IDEA 2019 数据收集分析：Flume + Kafka + SparkStreaming + MySQL/Redis 数据展示：SpringBoot + MyBatis + WebSocket + MySQL + LayUI + Echarts + 高德地图API 🔧 3.项目收集功能3.1 数据服务端与数据客户端部署我的主机信息如下： 123192.168.26.110 bigdata192.168.26.111 webserver01192.168.26.111 webserver02 3.1.1 数据服务端部署 将 logweb-1.0.jar 上传到服务器webserver01 以及 webserver02。 启动运行 logweb 程序 1nohup java -jar logweb-1.0.jar &amp; 运行成功后，查询日志文件结果，结果如下表示正常启动： 1234567891011121314151617181920 . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.7.10)2024-06-11 12:10:18.084 INFO 1288 --- [ main] c.s.i.w.s.web.logweb.LogwebApplication : Starting LogwebApplication v1.0 using Java 1.8.0_231 on webserver01 with PID 1288 (/home/subowen/serverJar/logweb-1.0.jar started by subowen in /home/subowen/serverJar)2024-06-11 12:10:18.090 INFO 1288 --- [ main] c.s.i.w.s.web.logweb.LogwebApplication : No active profile set, falling back to 1 default profile: &quot;default&quot;2024-06-11 12:10:20.604 INFO 1288 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 9527 (http)2024-06-11 12:10:20.666 INFO 1288 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2024-06-11 12:10:20.666 INFO 1288 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.73]2024-06-11 12:10:21.301 INFO 1288 --- [ main] o.a.c.c.C.[.[localhost].[/logweb] : Initializing Spring embedded WebApplicationContext2024-06-11 12:10:21.301 INFO 1288 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 3094 ms2024-06-11 12:10:23.099 INFO 1288 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 9527 (http) with context path '/logweb'2024-06-11 12:10:23.116 INFO 1288 --- [ main] c.s.i.w.s.web.logweb.LogwebApplication : Started LogwebApplication in 6.507 seconds (JVM running for 8.07)2024-06-11 12:14:14.608 INFO 1288 --- [nio-9527-exec-1] o.a.c.c.C.[.[localhost].[/logweb] : Initializing Spring DispatcherServlet 'dispatcherServlet'2024-06-11 12:14:14.608 INFO 1288 --- [nio-9527-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'2024-06-11 12:14:14.609 INFO 1288 --- [nio-9527-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 1 ms 3.1.2 数据客户端部署本次数据客户端直接部署在 Windosw 下，使用 IntelliJ IDEA 2019 进行开发。 IDEA 数据客户端结构如下： 修改 pom.xml 引入相应的包 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;travel_subowen423&lt;/artifactId&gt; &lt;groupId&gt;com.example.x.travel&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;logclient&lt;/artifactId&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- 配置 Spark 的版本 --&gt; &lt;spark.version&gt;3.0.1&lt;/spark.version&gt; &lt;httpclient.version&gt;4.5.12&lt;/httpclient.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.12.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;${httpclient.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 导入 logclient 代码，并且修改相应包名和部分代码 执行 ScenicAPP.java 随机生成数据。 3.1.3 数据格式说明 经度 纬度 景点名称 时间 117.024489 36.669213 曲水亭街 20240611155103 117.016089 36.661138 趵突泉 20240611155103 116.813744 36.541549 济南国际园博园 20240611155103 117.022959 36.668068 芙蓉街 20240611155103 117.034920 36.641749 千佛山 20240611155103 117.023837 36.674997 大明湖 20240611155103 117.023837 36.674997 大明湖 20240611155103 117.024489 36.669213 曲水亭街 20240611155103 117.016089 36.661138 趵突泉 20240611155103 117.021483 36.661473 济南泉城广场 20240611155103 … … … … 3.2 Kafka 消息队列3.2.1 Kafka 简介点击访问：Kafka官网 Apache Kafka 是一个开源分布式**事件(Event)**流平台，已被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用程序。 本项目主要使用发布（写入）和订阅（读取）事件流，包括从其他系统持续导入/导出数据；根据需要持久可靠地存储事件流；在事件发生时或回顾性地处理事件流。 3.2.2 Kafka 中的核心概念 名词 解释 Broker Kafka 集群包含一个或多个服务器，这些服务器称为 Broker Producer 生产者，负责将数据发送到 Kafka Consumer 消费者，负责从 Kafka 中读取数据 Consumer Group 消费者组，多个消费者组成的组 Topic 主题，每条发布到 Kafka 集群的消息都有一个类别，这个类别称为 Topic，可以理解为文件夹 Partition 分区，每个Topic包含一个或多个Partition 3.2.3 Kafka 部署 Kafka 的部署方式分为： 分布式部署（多节点多Broker） 单机部署（单节点单Broker/单节点多Broker） Kafka 是使用 Scala 编写的组件，依赖与Scala版本 Kafka 依赖于 ZooKeeper，必须要安装 ZooKeeper，再安装 Kafka 部署 Kafka 过程 解压 kafka 安装包 配置环境变量 修改配置 config/server.properties 文件 12345# 配置 Broker 的 ID, 在同一个集群上, 这个值必须是一个独一无二的整数值broker.id=0# 配置日志文件的路径log.dirs=/home/subowen/apps/kafka_2.12-2.8.0/kafka-logs 3.2.4 Kafka 的基本应用 启动 ZooKeeper 1[subowen@bigdata ~]$ zkServer.sh start 启动 Kafka 第一次启动：先使用前台启动，如果没有问题再使用后台启动 前台启动命令：kafka-server-start.sh $KAFKA_HOME/config/server.properties 后台启动命令：kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties 常用的 Kafka 命令 参考博客：Kafka 基础理论与常用命令详解 (超详细) Kafka常用命令和解释 - CSDN博客 Topics 常用命令 创建一个名为my_topic的主题： 1[subowen@bigdata ~]$ kafka-topics.sh --create --bootstrap-server bigdata:9092 --topic my_topic 查看名为my_topic的主题的详细信息： 1[subowen@bigdata ~]$ kafka-topics.sh --describe --bootstrap-server bigdata:9092 --topic my_topic Producer 常用命令 生产信息 1[subowen@bigdata ~]$ kafka-console-producer.sh --broker-list bigdata:9092 --topic my_topic Consumer 常用命令 3.2.5 使用 Flume 收集数据到 Kafka在之前的项目中，我学习了 离线类型 项目的 Flume 数据收集，因为本次需要进行在线的实时数据分析，所以按照之前的离线分析方式，使用 Flume 将数据收集到 HDFS 是不适合实时的数据分析环境的。将数据落地到 HDFS 则意味着数据进入磁盘，数据的读写会占用大量的磁盘 I/O，不适用于实时场景。因此在实时项目，考虑到数据的实时性，本次实时数据分析项目使用 消息队列（Kafka）进行数据的存放。 配置 webserver01 和 webserver02 上的 Flume 配置文件 taildir-avro-stream.conf 12345678910111213141516171819202122a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = TAILDIRa1.sources.r1.positionFile = /home/subowen/apps/apache-flume-1.9.0-bin/position/taildir_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /home/subowen/serverJar/logweb/logs/scenic.loga1.sources.r1.headers.f1.headerKey1 = inspur-szya1.sources.r1.fileHeader = truea1.sources.r1.maxBatchCount = 1000a1.sinks.k1.type = avroa1.sinks.k1.hostname = 192.168.26.110a1.sinks.k1.port = 4545a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels=c1a1.sinks.k1.channel=c1 配置 bigdata 上的 Flume 配置文件 avro-kafka-stream.conf 123456789101112131415161718192021222324252627282930313233343536# 配置 Agent a1各个组件的名称# Agent a1 的 source有一个, 叫做r1a1.sources = r1# Agent a1 的 sink也有一个, 叫做k1a1.sinks = k1# Agent a1 的 channel有一个, 叫做c1a1.channels = c1# 配置 Agent a1的source r1的属性a1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0# 监听的端口a1.sources.r1.port = 4545# 配置 Agent a1的sink k1的属性a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = subowena1.sinks.k1.kafka.bootstrap.servers = bigdata:9092a1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1a1.sinks.k1.kafka.producer.compression.type = snappy# 配置 Agent a1 的 channel c1 的属性, channel是用来缓冲Event数据的# channel 的类型是内存 channel, 顾名思义这个 channel 是使用内存来缓冲数据a1.channels.c1.type = memory# 内存channel的容量大小是1000, 注意这个容量不是越大越好, 配置越大, 一旦Flume 挂掉丢失的 event 也就越多a1.channels.c1.capacity = 1000# source 和 sink 从内存 channel 每次事务传输的event数量a1.channels.c1.transactionCapacity = 100# 把source和sink绑定到channel上# 与source r1绑定的channel有一个, 叫做c1a1.sources.r1.channels = c1# 与sink k1绑定的channel有一个, 叫做c1a1.sinks.k1.channel = c1 启动 webserver01 和 webserver02 的 Flume，这里为了简化操作，只启动一台机器上的 Flume 1[subowen@webserver01 config]$ ./start-flume.sh taildir-avro-stream.conf a1 启动 bigdata 上的 Flume 1[subowen@bigdata config]$ ./start-flume.sh avro-kafka-stream.conf a1 在 bigdata 上启动 **kafka-console-consumer**，使用消费者Shell进行测试消费，后期会更换为 SparkStreaming 进行消费。 1[subowen@bigdata ~]$ kafka-console-consumer.sh --topic subowen --bootstrap-server bigdata:9092 在 Windows 下运行客户端，模拟数据的产生，执行ScenicAPP代码，模拟数据的生成过程。 12345678910package com.example.x.data;import com.example.x.data.producer.DataProducer;public class ScenicApp { public static final String url = &quot;http://192.168.26.111:9527/logweb/upload&quot;; public static void main(String[] args) throws Exception{ DataProducer.producer(url); }} 📈 4. 数据实时分析Spark 3.0.1 官方文档入口：Overview - Spark 3.0.1 Documentation (apache.org) 4.1 SparkStreaming 概述SparkStreaming 官方文档入口：Spark Streaming - Spark 3.0.1 Documentation (apache.org) SparkStreaming 类似于之前学习的 SparkRDD 和SparkSQL，是 Spark API 的核心扩展，支持实时数据流的可扩展、高吞吐量和容错。数据可以从Kafka、Flume、Kinesis或TCP Socket 等许多来源中读取，并且可以使用复杂的算法进行处理，这些算法用高级函数（如 map、reduce、join 和 window）表示。最后，处理过的数据可以保存到文件系统、数据库和实时仪表板。事实上，您可以在数据流上应用 Spark 的机器学习和图形处理算法。 SparkStreaming 是 Spark 中用于处理实时数据的一个模块。 在内部，他的工作流程是：SparkStreaming 接收实时输入的数据流，并对数据进行分批（微批）处理，由 Spark 引擎进行处理，生成最终的批量结果流。 SparkStreaming 是微批处理（批次特别小，足以实现实时处理），不是真正的流处理。 这里也就可以更显著的得到离线批数据和实时数据之间的区别： 离线批数据：bound——有界的数据 实时数据：unbound——无界的数据 4.2 开发第一个SparkStreming案例类比 SparkRDD 开发的流程，我们给出SparkStreaming 开发的流程。 SparkRDD 编程模型： 创建 SparkContext 读取数据源 处理数据 输出结果 关闭 SparkContext SparkStreaming 编程模型： 创建 StreamingContext 读取数据 处理数据 输出结果 启动程序（阻塞） 等待程序关闭 由此，我们开发我们的第一个 SparkStreaming 实例。 在 IDEA 中添加 Maven 依赖 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 TcpStreamingAPP，编写代码 1234567891011121314151617181920212223242526272829303132333435package com.example.x.sparkstreamimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}import org.apache.spark.streaming.{Seconds, StreamingContext}object TcpStreamingAPP { def main(args: Array[String]): Unit = { // 第一步: 创建 StreamingContext val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;TcpStreaming&quot;) val streamingContext = new StreamingContext(conf, Seconds(1)) // 第二步: 读取数据: 接收 hostname:port 发送的数据 // SparkCore SparkContext RDD(弹性分布式数据集)——不可变的、可分区的、可并行计算 // SparkSQL SparkSession DataSet/DataFrame // SparkStreaming StreamingContext DStream val hostname = &quot;bigdata&quot; val port = 9999 var tcpDStream: ReceiverInputDStream[String] = streamingContext.socketTextStream(hostname, port) // 第三步: 处理数据 val wcDStream: DStream[(String, Int)] = tcpDStream.flatMap(_.split(&quot;,&quot;)) .map((_, 1)) .reduceByKey(_ + _) // 默认计算的是当前批次的数据, 不会与之前的批次进行累加操作 // 第四步: 累加结果 wcDStream.print() // 第五步: 启动程序 streamingContext.start() // 第六步: 等待程序关闭 streamingContext.awaitTermination() }} 在 bigdata 中安装和启动 netcat 生成数据 12[subowen@bigdata ~]$ sudo yum -y install nc[subowen@bigdata ~]$ nc –lk 9999 启动程序进行测试，需要注意的是：程序只计算当前时间点发送过来的数据（无状态）。 4.3 SparkStreaming 核心概念 概念 解释 StreamingContext SparkStreaming功能的主要入口点，它提供了用于从各种输入源创建 DStream 的方法。创建和转换DStream后，可以分别使用 start() 和 stop() 启动和停止流计算。awaitTermination() 等待执行停止，允许当前线程通过stop() 手动停止或通过一个异常等待StreamingContext的终止。 DStream DStream 是 SparkStreaming 提供的基本抽象，它表示一个连续的数据流，要么是从Source接收的输入数据流，要么是通过转换输入流生成的处理数据流。在内部，DStream由一系列连续的RDD表示，DStream中的每个RDD都包含一定时间间隔的数据。在DStream上应用的任何操作都转换为在底层 RDD上的操作。DStream中是由每个批次生成的RDD组成的。 Input DStreams 和 Receivers Input DStreams是表示从数据源接收的输入数据流的DStreams。每个Input DStream（文件流除外）都与一个Receiver对象相关联，接收来自源的数据并将其存储在Spark的内存中进行处理。 4.4 数据源数据源其实就是从哪里读取数据，区别就在于一些读取的写法有不同。 4.4.1 基本数据源 Socket套接字数据源：socketTextStream()。 File System文件系统数据源：textFileStream()。 4.4.2 高级数据源Spark Kafka官方文档：Spark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher) - Spark 3.0.1 Documentation (apache.org) 高级数据源：如 kafka 、Kinesis等。 用代码对接 Kafka 123456789101112131415161718192021222324252627282930313233343536373839package com.example.x.sparkstreamimport org.apache.spark.SparkConfimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.codehaus.jackson.map.deser.std.StringDeserializerobject KafkaStreamApp { def main(args: Array[String]): Unit = { // 第一步 // Spark 中遇到的一切序列化问题都需要 KryoSerializer val conf: SparkConf = new SparkConf().setMaster(&quot;Local[2]&quot;).setAppName(&quot;KafkaStreaming&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;subowen&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = &quot;subowen&quot; val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) // 指定订阅的 Topic ) kafkaStream.flatMap(x=&gt;x.value().split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _).print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 4.4.3 维护 Kafka Offset在前面的学习中，我们已经了解了两种消费者的消费策略，分别是 Lastest 和 Earliest，这两种消费策略都是存在问题的。 Lastest 消费策略：当消费者启动之后，从启动后产生的第一条数据开始消费 消费者第一次启动之前，topic中已经存在的数据是不会被消费。 消费者宕机的时间段内，topic中产生的数据不会被消费。 Earliest 消费策略： 如何手动维护Kafka Offset？ 我们可以将 Offset 持久化到一个数据库中，如MySQL、HDFS、ZooKeeper 中。下面将演示如何使用 MySQL 来维护 Kafka Offset。 设计表结构 字段名 约束 类型 备注 k_topic 联合主键 VARCHAR(50) 可以直接从代码中获得 k_groupid 联合主键 VARCHAR(50) 可以直接从代码中获得 k_partition 联合主键 INT - k_offset - BIGINT - 实现表结构 1234567CREATE TABLE IF NOT EXISTS t_offset ( k_topic VARCHAR(50) NOT NULL, -- 设置 Topic k_groupid VARCHAR (50) NOT NULL, -- 设置 groupid k_partition INT NOT NULL, -- 设置 Partition k_offset BIGINT NOT NULL, -- 设置 offset PRIMARY KEY(k_topic, k_groupid, k_partition) -- 设置联合主键); 开发工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.example.x.utilsimport java.sql.{Connection, PreparedStatement, ResultSet}import org.apache.kafka.common.TopicPartitionimport org.apache.spark.streaming.kafka010.OffsetRangeimport scala.collection.mutable.ListBufferobject KafkaOffsetManagerUtils { // 保存 Offset def saveOffset(offsetRanges: Array[OffsetRange], groupId: String) = { val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO t_offset(k_topic, k_groupid, k_partition, k_offset) VALUES(?, ?, ?, ?) |ON DUPLICATE KEY UPDATE k_offset=? |&quot;&quot;&quot;.stripMargin val pst: PreparedStatement = connection.prepareStatement(sql) offsetRanges.map(offsetRanges =&gt; { val topic = offsetRanges.topic val partition = offsetRanges.partition val offset = offsetRanges.untilOffset pst.setString(1, topic) pst.setString(2, groupId) pst.setInt(3, partition) pst.setLong(4, offset) pst.setLong(5, offset) pst.execute() }) ConnectionUtils.closeConnection(connection) } // 读 Offset def readOffset(topic: String, groupId: String): Map[TopicPartition, Long] = { val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;SELECT k_topic, k_groupid, k_partition, k_offset FROM t_offset WHERE k_topic=? AND k_groupid=?&quot; val pst: PreparedStatement = connection.prepareStatement(sql) pst.setString(1, topic) pst.setString(2, groupId) val resultSet: ResultSet = pst.executeQuery() val list = new ListBuffer[(TopicPartition, Long)] while(resultSet.next()) { val partition = resultSet.getInt(&quot;k_partition&quot;) val topicPartition = new TopicPartition(topic, partition) val offset = resultSet.getLong(&quot;k_offset&quot;) list.append((topicPartition, offset)) } list.toMap }} 代码案例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.example.x.quickstartimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.{ConnectionUtils, KafkaOffsetManagerUtils}import org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.{SparkConf, TaskContext}import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.{Seconds, StreamingContext}object KafkaOffset { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;WordCount&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val topics = Array(&quot;KafkaOffset&quot;) val groupId = &quot;szy&quot; val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; groupId, &quot;auto.offset,reset&quot; -&gt; &quot;earliest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) // 先读取数据库的 Offset, 设置到 Subscribe val offset: Map[TopicPartition, Long] = KafkaOffsetManagerUtils.readOffset(topics(0), groupId) println(&quot;&gt;&gt;&gt; [LOGS] The Offset Now Reading: &quot; + offset) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams, offset) // 指定订阅的 Topic ) // 测试数据: // a,a,a,a,a // b,b,b,c // c,c,d kafkaStream.foreachRDD { rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition { iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) /** * @param: o.topic: 从 topics 可以获取到 * @param: o.partition: 分区 * @param: o.fromOffset: 正在执行的 Offset * @param: o.untilOffset: 即将执行的 Offset */ println(s&quot;${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}&quot;) KafkaOffsetManagerUtils.saveOffset(offsetRanges, groupId) } } kafkaStream.print() streamingContext.start() streamingContext.awaitTermination() }} 测试 4.5 转换操作4.6 输出操作将数据输出到指定的数据仓库中，如MySQL、Redis、HBase等。下面通过一个简单的 WordCount 案例快速了解如何将数据写入 MySQL。本案例借助 创建数据库和表 123456CREATE DATABASE travel CHARSET utf8;CREATE TABLE wc( id BIGINT PRIMARY KEY AUTO_INCREMENT, word VARCHAR(50), count BIGINT ); 存放 Druid 的配置文件 1234567891011url = jdbc:mysql://bigdata:3306/travel?useSSL=false&amp;characterEncoding=UTF-8username = rootpassword = rootdriverClassName = com.mysql.jdbc.DriverinitialSize = 5maxActive = 20minIdle = 1maxWait = 60000validationQuery = SELECT 1testOnBorrow = truetestWhileIdle = true 编写数据库连接工具类ConnectionUtils.scala 1234567891011121314151617181920212223242526272829303132333435package com.example.x.utilsimport java.io.InputStreamimport java.sql.Connectionimport java.util.Propertiesimport com.alibaba.druid.pool.DruidDataSourceFactoryimport javax.sql.DataSourceobject ConnectionUtils { // 1. 创建 Druid 的 DataSource 对象 val dataSource: DataSource = { val properties = new Properties() val inputStream: InputStream = getClass.getClassLoader.getResourceAsStream(&quot;druid.properties&quot;) properties.load(inputStream) println(properties) DruidDataSourceFactory.createDataSource(properties) } // 2. 创建获取连接的方法, 向外提供数据库连接 def getConnection(): Connection = { dataSource.getConnection() } // 3. 创建连接回收方法 def closeConnection(connection: Connection): Unit = { if(null != connection) { connection.close() } } def main(args: Array[String]): Unit = { println(dataSource) }} 编写 WordCount 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.example.x.quickstartimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.ConnectionUtilsimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject WordCount { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;WordCount&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;WordCount&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = Array(&quot;WordCount&quot;) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) val wcDStream: DStream[(String, Int)] = kafkaStream.flatMap(x=&gt;x.value().split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _) // 测试数据： // a,b,a,c,a,d,c,c,a,m,e,f // (a, 4) // (b, 1) // (c, 3) // (d, 1) // (e, 1) // (m, 1) // (f, 1) wcDStream.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;INSERT INTO wordcount(word, count) VALUES (?, ?)&quot; // 在执行这个SQL语句之前需要创建 `wordcount` 表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { pst.setString(1, record._1) pst.setLong(2, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) streamingContext.start() streamingContext.awaitTermination() }} 4.7 任务实施 启动 WebServer01 的 Flume 1[subowen@webserver01 config]$ ./start-flume.sh taildir-avro-stream.conf a1 启动 Bigdata 的 Flume 1[subowen@bigdata config]$ ./start-flume.sh taildir-avro-stream.conf a1 启动 WebServer01 的生产者Producer，即logweb-1.0.jar 123456789[subowen@webserver01 serverJar]$ ll总用量 17608drwxrwxr-x. 3 subowen subowen 18 6月 11 12:10 logweb-rw-rw-r--. 1 subowen subowen 18014926 6月 11 12:08 logweb-1.0.jar-rw-------. 1 subowen subowen 5531 6月 12 13:41 nohup.out-rwxrw-r--. 1 subowen subowen 33 6月 11 12:09 start-logweb.sh[subowen@webserver01 serverJar]$ cat ./start-logweb.shnohup java -jar logweb-1.0.jar &amp;[subowen@webserver01 serverJar]$ ./start-logweb.sh 编写消费者程序，进行实时数据处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.example.x.sparkstreamimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentobject RealtimeScenicPA { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;RealtimeScenic&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;subowen&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = Array(&quot;subowen&quot;) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) // 1. 统计相同经纬度和时间的人流量数据 val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value) .map(line =&gt; { val splitData = line.split(&quot;,&quot;) (s&quot;${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}&quot;, 1) }) .reduceByKey(_ + _) // 2. 基于状态操作统计每个景点每分钟的人流量 val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line =&gt; { val keySplit = line._1.split(&quot;,&quot;) (s&quot;${keySplit(2)},${keySplit(3).substring(0, 12)}&quot;, line._2) }).reduceByKey(_ + _) // 输出结果 peopleCountPerLocationAndTime.print() peopleCountPerMinutePerLocation.print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 在 Windows 平台运行生产者程序ScenicAPP和消费者程序RealtimeScenicPA。结果如下： 结合 MySQL 进行数据持久化，首先建立好数据库和数据表。 12345678910111213141516171819202122-- SQL 建表语句如下CREATE DATABASE IF NOT EXISTS travel CHARSET UTF8;USE travel;CREATE TABLE IF NOT EXISTS people_count_per_location_and_time ( -- id BIGINT PRIMARY KEY AUTO_INCREMENT, -- id longitude DOUBLE, -- 经度 latitude DOUBLE, -- 纬度 scenic VARCHAR(20), -- 景点 sec_moment VARCHAR(15), -- 具体时刻 sec_quantity BIGINT, -- 具体数量 PRIMARY KEY(scenic, sec_moment) -- 设置联合主键);CREATE TABLE IF NOT EXISTS people_count_per_minute_per_location ( -- id BIGINT PRIMARY KEY AUTO_INCREMENT, -- id scenic VARCHAR(20), -- 景点 min_moment VARCHAR(15), -- 具体时刻 min_quantity BIGINT, -- 具体数量 PRIMARY KEY(scenic, min_moment) -- 设置联合主键); 集合之前 WordCount 案例中编写 ConnectionUtils.scala 代码，完成该项目的数据持久化任务。编写RealtimeScenicPA.scala代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.example.x.travelimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.ConnectionUtilsimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.KafkaUtilsimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.{Seconds, StreamingContext}object RealtimeScenicPA { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;RealtimeScenic&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) streamingContext.checkpoint(&quot;ckpt&quot;) val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;subowen&quot;, &quot;auto.offset,reset&quot; -&gt; &quot;latest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) val topics = Array(&quot;subowen&quot;) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) val updateFunc = (newValues: Seq[Int], state: Option[Int]) =&gt; { val currentCount = newValues.sum val previousCount = state.getOrElse(0) val newCount = currentCount + previousCount Some(newCount) } // 1. 统计相同经纬度和时间的人流量数据 val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value) .map(line =&gt; { val splitData = line.split(&quot;,&quot;) (s&quot;${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}&quot;, 1) }) .updateStateByKey(updateFunc) // 2. 基于状态操作统计每个景点每分钟的人流量 val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line =&gt; { val keySplit = line._1.split(&quot;,&quot;) (s&quot;${keySplit(2)},${keySplit(3).substring(0, 12)}&quot;, line._2) }).reduceByKey(_ + _) // 输出结果 peopleCountPerLocationAndTime.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO people_count_per_location_and_time(longitude, latitude, scenic, sec_moment, sec_quantity) |VALUES (?, ?, ?, ?, ?) |ON DUPLICATE KEY |UPDATE sec_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setDouble(1, splitRecord(0).toDouble) pst.setDouble(2, splitRecord(1).toDouble) pst.setString(3, splitRecord(2)) pst.setString(4, splitRecord(3)) pst.setInt(5, record._2) pst.setInt(6, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) peopleCountPerMinutePerLocation.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO people_count_per_minute_per_location(scenic, min_moment, min_quantity) |VALUES (?, ?, ?) |ON DUPLICATE KEY |UPDATE min_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setString(1, splitRecord(0)) pst.setString(2, splitRecord(1)) pst.setInt(3, record._2) pst.setInt(4, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) // peopleCountPerLocationAndTime.print() peopleCountPerMinutePerLocation.print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 结合 MySQL 对 t_offset 表进行维护。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145package com.example.x.travelimport java.sql.{Connection, PreparedStatement}import com.example.x.utils.{ConnectionUtils, KafkaOffsetManagerUtils}import org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.{SparkConf, TaskContext}import org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.{Seconds, StreamingContext}object RealtimeScenicPA { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;RealtimeScenic&quot;).set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;) val streamingContext = new StreamingContext(conf, Seconds(5)) streamingContext.checkpoint(&quot;ckpt&quot;) val topics = Array(&quot;subowen&quot;) val groupId = &quot;subowen&quot; val kafkaParams = Map[String, Object] ( &quot;&quot;-&gt;&quot;&quot;, &quot;bootstrap.servers&quot; -&gt; &quot;bigdata:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; groupId, &quot;auto.offset,reset&quot; -&gt; &quot;earliest&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) // 自动提交 offset(自动记录读到topic中的哪个部位) ) // 先读取数据库的 Offset, 设置到 Subscribe val offset: Map[TopicPartition, Long] = KafkaOffsetManagerUtils.readOffset(topics(0), groupId) println(&quot;&gt;&gt;&gt; [LOGS] The Offset Now Reading: &quot; + offset) val kafkaStream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams, offset) ) val updateFunc = (newValues: Seq[Int], state: Option[Int]) =&gt; { val currentCount = newValues.sum val previousCount = state.getOrElse(0) val newCount = currentCount + previousCount Some(newCount) } // 1. 统计相同经纬度和时间的人流量数据 val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value) .map(line =&gt; { val splitData = line.split(&quot;,&quot;) (s&quot;${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}&quot;, 1) }) .updateStateByKey(updateFunc) // 2. 基于状态操作统计每个景点每分钟的人流量 val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line =&gt; { val keySplit = line._1.split(&quot;,&quot;) (s&quot;${keySplit(2)},${keySplit(3).substring(0, 12)}&quot;, line._2) }).reduceByKey(_ + _) // 输出结果 peopleCountPerLocationAndTime.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO t_heat(longitude, latitude, scenic, sec_moment, sec_quantity) |VALUES (?, ?, ?, ?, ?) |ON DUPLICATE KEY |UPDATE sec_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setDouble(1, splitRecord(0).toDouble) pst.setDouble(2, splitRecord(1).toDouble) pst.setString(3, splitRecord(2)) pst.setString(4, splitRecord(3)) pst.setInt(5, record._2) pst.setInt(6, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) peopleCountPerMinutePerLocation.foreachRDD(rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection: Connection = ConnectionUtils.getConnection() val sql = &quot;&quot;&quot; |INSERT INTO t_scenic(scenic, min_moment, min_quantity) |VALUES (?, ?, ?) |ON DUPLICATE KEY |UPDATE min_quantity = ?&quot;&quot;&quot;.stripMargin // 在执行这个SQL语句之前需要创建表 val pst: PreparedStatement = connection.prepareStatement(sql) partitionOfRecords.foreach(record =&gt; { val splitRecord = record._1.split(&quot;,&quot;) // println(splitRecord(0) + &quot; &quot; + splitRecord(1) + &quot; &quot; + splitRecord(2) + &quot; &quot; + splitRecord(3)) pst.setString(1, splitRecord(0)) pst.setString(2, splitRecord(1)) pst.setInt(3, record._2) pst.setInt(4, record._2) pst.execute() }) if(pst != null) { pst.close() } ConnectionUtils.closeConnection(connection) } ) // 维护 KafkaStream 的 Offset 表 kafkaStream.foreachRDD { rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition { iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) /** * @param: o.topic: 从 topics 可以获取到 * @param: o.partition: 分区 * @param: o.fromOffset: 正在执行的 Offset * @param: o.untilOffset: 即将执行的 Offset */ println(s&quot;${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}&quot;) KafkaOffsetManagerUtils.saveOffset(offsetRanges, groupId) } } // peopleCountPerLocationAndTime.print() peopleCountPerMinutePerLocation.print() // 启动 StreamingContext streamingContext.start() // 阻塞 StreamingContext streamingContext.awaitTermination() }} 🎥 5. 数据实时监控系统5.1 软件开发体系架构5.1.1 开发架构 架构 说明 B/S架构（浏览器/服务器） 只要用户安装浏览器（谷歌）就可以访问 C/S架构（客户端/服务器） 需要用户安装和更新客户端部分，比如微信 5.1.2 开发模式及相关技术 开发模式：MVC 模式 组件 说明 Model 模型 View 视图，login.html / index.html Controller 控制器，接受用户的请求，向用户做出响应 开发方式 方式 说明 前后端不分离 JSP/HTML + SSM 前后端分离 前端和后台不在同一个项目中 请求方法 请求方法 说明 常用方式 GET 常用于查询、删除操作 1. 浏览器的地址栏2. &lt;a href=&quot;...&quot;&gt;&lt;/a&gt;3. Ajax4. 异步请求 POST 常用于增加、修改操作 1. Form表单2. Ajax JavaWeb开发相关技术 Java后台技术 SSM：大量的配置文件 xml， SpringBoot+ MyBatis Servlet/JSP、Spring + Spring MVC、SpringBoot、SpringCloud（微服务） 数据库相关框架：MyBatis、Hibernate、Spring JPA 前端技术 HTML/HTML5、CSS/CSS3、JS 前端框架：jQuery、Vue、AngularJS、React、TS 5.2 SpringBoot + MyBatis 框架简介Spring Boot：入门篇 (cnblogs.com) MyBatis中文网 5.3 后台服务开发基本任务说明： 统计今天所有景点的人流量总和 统计当前时间（分钟）所有景点的人流量总和 统计每分钟每个景点的人流量（经纬度）-热力图 5.3.1 项目环境搭建 创建SpringBoot项目 修改必要配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;!-- 这里需要修改版本 --&gt; &lt;version&gt;2.7.15&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example.x.travel&lt;/groupId&gt; &lt;artifactId&gt;travelweb&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;travelweb&lt;/name&gt; &lt;description&gt;Travel Project Web&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.41&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.18&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 修改 application.xml 为 application.yml 并进行配置 123456789101112131415161718192021222324252627282930313233343536# 配置应用服务器端口以及根目录server: port: 9090 servlet: context-path: /travelserver# 配置应用名spring: application: name: travelserver # 配置数据源, 这里是 Druid 的一些配置 datasource: type: com.alibaba.druid.pool.DruidDataSource druid: url: jdbc:mysql://bigdata:3306/travel?useSSL=false&amp;characterEncoding=UTF-8 username: root password: root driver-class-name: com.mysql.jdbc.Driver initial-size: 1 minIdle: 1 maxActive: 5 maxWait: 6000 timeBetweenEvictionRunsMillis: 6000 minEvictableIdleTimeMillis: 30000 validationQuery: SELECT 1 FROM DUAL testWhileIdle: true testOnBorrow: false testOnReturn: false filters: stat,wall,log4j# 配置 MyBatis 的路径mybatis: # 这里是 DAO Mapper的路径, 表示 Resource/Mapper/ 的所有 *.xml 文件 mapper-locations: classpath:mapper/*.xml # 这里定义实体类的位置 type-aliases-package: com.example.x.travel.travelweb.entity configuration: map-underscore-to-camel-case: true 在 IDEA 中添加 Lombok 和 MyBatisX 的插件 配置 MyBatis 的 *.xml 模板 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper&gt; &lt;!-- 这里补全内容 --&gt;&lt;/mapper&gt; 开发 TravelConfig.java 类解决跨域问题 123456789101112131415161718192021// SpringBoot 中很多事情需要通过注解的方式来实现// @... 代表注解// @Configuration 注解表明这是一个 Spring Boot 的配置类// @EnableWebMvc 开启 WebMvc 的功能, 这样 Spring Boot 就能自动处理 Web 相关的配置@Configuration@EnableWebMvcpublic class TravelConfig implements WebMvcConfigurer { /** * @brief 设置服务器跨域访问策略: 所有 GET 和 POST 请求都可以跨域访问, 允许所有来源的请求, 即允许跨域请求 * 因为我们的项目是前后端分离的, 即前端和后端不在一个服务器上, 所以需要处理跨域问题 * @param registry */ @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) // addMapping 方法的参数代表需要进行 CORS 映射的 HTTP方法及其路径, 这里是&quot;/**&quot;, 意味着对所有HTTP方法进行映射 .allowedOriginPatterns(&quot;*&quot;) // allowedOriginPatterns 是一个正则表达式列表, 这里使用 &quot;*&quot; 表示任何来源都可以 .allowedMethods(&quot;GET&quot;, &quot;POST&quot;) // allowedMethods是允许的 HTTP 方法，这里是 &quot;GET,POST&quot; .allowCredentials(true) // 允许携带请求体的请求通过, 这里的 allowCredentials(true) 意味着在请求头中携带用户凭证信息是允许的 .maxAge(3600); // maxAge 设置 CORS 跨域配置的缓存时间, 这里是 3600 秒 }} 本项目（Java开发）中层与层之间的联系 5.3.2 后台开发 com.example.x.travel.travelweb.config.TravelConfig 1234567891011121314151617181920212223242526import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;// SpringBoot 中很多事情需要通过注解的方式来实现// @... 代表注解// @Configuration 注解表明这是一个 Spring Boot 的配置类// @EnableWebMvc 开启 WebMvc 的功能, 这样 Spring Boot 就能自动处理 Web 相关的配置@Configuration@EnableWebMvcpublic class TravelConfig implements WebMvcConfigurer { /** * @brief 设置服务器跨域访问策略: 所有 GET 和 POST 请求都可以跨域访问, 允许所有来源的请求, 即允许跨域请求 * 因为我们的项目是前后端分离的, 即前端和后端不在一个服务器上, 所以需要处理跨域问题 * @param registry */ @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) // addMapping 方法的参数代表需要进行 CORS 映射的 HTTP方法及其路径, 这里是&quot;/**&quot;, 意味着对所有HTTP方法进行映射 .allowedOriginPatterns(&quot;*&quot;) // allowedOriginPatterns 是一个正则表达式列表, 这里使用 &quot;*&quot; 表示任何来源都可以 .allowedMethods(&quot;GET&quot;, &quot;POST&quot;) // allowedMethods是允许的 HTTP 方法，这里是 &quot;GET,POST&quot; .allowCredentials(true) // 允许携带请求体的请求通过, 这里的 allowCredentials(true) 意味着在请求头中携带用户凭证信息是允许的 .maxAge(3600); // maxAge 设置 CORS 跨域配置的缓存时间, 这里是 3600 秒 }} com.example.x.travel.travelweb.controller.TravelController 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import java.util.HashMap;import java.util.List;import java.util.Map;@RestController // 注解: 定义这个类是一个 Controller@RequestMapping(&quot;/scenic&quot;) // 注解: 定义访问这个类的 URL// 访问 URL: http://ip:port/scenicpublic class TravelController { @Autowired private TravelService travelService; // @RequestMapping 表示既可以通过 GET 访问, 也可以通过 POST 访问 // @GetMapping 表示只可以通过 GET 访问 // @PostMapping 表示只可以通过 POST 访问 /** * @brief 表示获取一天访客的总数量 * @url http://localhost:9090/travelserver/scenic/getDaySum * @return */ @RequestMapping(&quot;/getDaySum&quot;) public long getDaySum() { long sum = travelService.getDaySum(); return sum; } @RequestMapping(&quot;/getCurrentSum&quot;) public long getCurrentSum() { long sum = travelService.getCurrentSum(); return sum; } @RequestMapping(&quot;/getHeatData&quot;) public List&lt;Scenic&gt; getHeatData() { return travelService.getHeatData(); } @RequestMapping(&quot;/getScenicMinuteData&quot;) public List&lt;Scenic&gt; getScenicMinuteData() { return travelService.getScenicMinuteData(); } @RequestMapping(&quot;/getScenicTrend&quot;) public HashMap&lt;String, Object&gt; getScenicTrend() { return travelService.getScenicTread(); } @PostMapping(&quot;/selectForm&quot;) @ResponseBody public Object receiveFormData(@RequestBody Map&lt;String, String&gt; formData) { // 在这里可以处理收到的formData // System.out.println(formData); String selectDate = formData.get(&quot;date&quot;); String selectFromTime = formData.get(&quot;fromTime&quot;); String selectToTime = formData.get(&quot;toTime&quot;); String selectTableName = formData.get(&quot;tableName&quot;); // System.out.println(selectDate + &quot; &quot; + selectFromTime + &quot; &quot; + selectToTime + &quot; &quot; + selectTableName); // 执行业务逻辑 if (selectDate.equals(&quot;&quot;)) { return null; } String fromDateAndTime = selectDate.replace(&quot;-&quot;, &quot;&quot;) + selectFromTime.replace(&quot;:&quot;, &quot;&quot;).replace(&quot;：&quot;, &quot;&quot;); String toDateAndTime = selectDate.replace(&quot;-&quot;, &quot;&quot;) + selectToTime.replace(&quot;:&quot;, &quot;&quot;).replace(&quot;：&quot;, &quot;&quot;); System.out.println(&quot;From Date And Time: &quot; + fromDateAndTime + &quot;\\nTo Date And Time: &quot; + toDateAndTime); switch (selectTableName) { case &quot;0&quot;: { System.out.println(&quot;请进行选择&quot;); break; } case &quot;1&quot;: { // 热力图 return travelService.getHeatDataByDateAndTime(fromDateAndTime, toDateAndTime); } case &quot;2&quot;: { // 人流量柱状图 return travelService.getScenicDataByDateAndTime(fromDateAndTime, toDateAndTime); } case &quot;3&quot;: { // 人流量趋势图 return travelService.getScenicTreadByDateAndTime(fromDateAndTime, toDateAndTime); } default: // 未定义图表 break; } return &quot;Data received successfully&quot;; }} com.example.x.travel.travelweb.dao.TravelDao 123456789101112131415161718import org.springframework.stereotype.Repository;import java.util.HashMap;import java.util.List;@Repositorypublic interface TravelDao { long getDaySum(); // 查询当天时间内的所有数据之和 long getCurrentSum(); // 查询当前时间内的所有数据之和 List&lt;Scenic&gt; getHeatData(); // 查询用于创建热力图的数据 List&lt;Scenic&gt; getScenicMinuteData(); // 查询用于获取每分钟景点人流量 List&lt;Scenic&gt; getScenicTread(); // 查询用于获取每分钟景点人流量趋势 List&lt;Scenic&gt; getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime); List&lt;Scenic&gt; getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime); List&lt;Scenic&gt; getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime);} com.example.x.travel.travelweb.entity.LineData 123456789import lombok.Data;import java.util.List;@Datapublic class LineData { private String name; private List&lt;Long&gt; data;} com.example.x.travel.travelweb.entity.Scenic 12345678910import lombok.Data;@Datapublic class Scenic { private double lng; private double lat; private long count; private String time; private String scenic;} com.example.x.travel.travelweb.services.TravelService 1234567891011121314151617181920package com.example.x.travel.travelweb.services;import com.example.x.travel.travelweb.entity.Scenic;import java.util.HashMap;import java.util.List;// 接口: 定义规范的public interface TravelService { long getDaySum(); long getCurrentSum(); List&lt;Scenic&gt; getHeatData(); List&lt;Scenic&gt; getScenicMinuteData(); HashMap&lt;String, Object&gt; getScenicTread(); List&lt;Scenic&gt; getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime); List&lt;Scenic&gt; getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime); HashMap&lt;String, Object&gt; getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime);} com.example.x.travel.travelweb.services.impl.TravelServiceImpl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.HashMap;import java.util.List;@Servicepublic class TravelServiceImpl implements TravelService { // 调用某个对象的方法, 需要先创建对象 @Autowired private TravelDao travelDao; // 相当于 TravelDap travelDao = new TravelDao(); @Override public long getDaySum() { return travelDao.getDaySum() ; } @Override public long getCurrentSum() { return travelDao.getCurrentSum(); } @Override public List&lt;Scenic&gt; getHeatData() { return travelDao.getHeatData(); } @Override public List&lt;Scenic&gt; getScenicMinuteData() { return travelDao.getScenicMinuteData(); } @Override public HashMap&lt;String, Object&gt; getScenicTread() { HashMap&lt;String, Object&gt; maps = new HashMap&lt;&gt;(); List&lt;String&gt; times = new ArrayList&lt;&gt;(); // 时间数组 List&lt;LineData&gt; series = new ArrayList&lt;&gt;(); // Series 数组 List&lt;Scenic&gt; heat = travelDao.getScenicTread(); // 调用 DAO 方法查询数据 HashMap&lt;String, List&lt;Long&gt;&gt; map = new HashMap&lt;&gt;(); // 将景点以及这个景点对应的所有数据整合成一条数据 for(Scenic scenic: heat) { String time = scenic.getTime(); // 获取数据库每条数据的时间 if (!times.contains(time)) { // 这个时间在 times 中是否存在 times.add(time); // 不存在则添加 } String scenic_name = scenic.getScenic(); // 获取景点名称 // 判断在 Map 中是否有 key, 并判断: // 如果有这个 key, 则把数据添加到 value 中 // 如果没有这个 key, 则添加这个 key, 新添加一个集合 map.computeIfAbsent(scenic_name, k-&gt;new ArrayList&lt;&gt;()).add(scenic.getCount()); // 上面一行等同于下面的逻辑 // if (map.containsKey(scienc)){ // map.get(scienc).add(scenic.getCount()); // } else { // List list=new ArrayList&lt;Long&gt;(); // list.add(scenic.getCount()); // map.put(scienc,list); // } } // System.out.println(times); // System.out.println(map); map.forEach((key, value) -&gt; { LineData data = new LineData(); data.setName(key); data.setData(value); series.add(data); }); maps.put(&quot;time&quot;, times); maps.put(&quot;series&quot;, series); return maps; } @Override public List&lt;Scenic&gt; getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime) { return travelDao.getHeatDataByDateAndTime(fromDateAndTime, toDateAndTime); } @Override public List&lt;Scenic&gt; getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime) { return travelDao.getScenicDataByDateAndTime(fromDateAndTime, toDateAndTime); } @Override public HashMap&lt;String, Object&gt; getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime) { return null; }} mapper/TravelMapper.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.inspur.szy.subowen.travel.travelweb.dao.TravelDao&quot;&gt; &lt;select id=&quot;getDaySum&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT IFNULL(SUM(min_quantity), 0) AS cnt FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 8)=DATE_FORMAT(NOW(), &quot;%Y%m%d&quot;) &lt;/select&gt; &lt;select id=&quot;getCurrentSum&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT IFNULL(SUM(min_quantity), 0) AS cnt FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 12)=DATE_FORMAT(NOW(), &quot;%Y%m%d%H%i&quot;) &lt;/select&gt; &lt;select id=&quot;getHeatData&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT longitude AS `lng`, latitude AS `lat`, SUBSTRING(`sec_moment`, 1, 12) AS today, SUM(`sec_quantity`) AS `count` FROM t_heat GROUP BY lng, lat, today HAVING today=DATE_FORMAT(NOW(), '%Y%m%d%H%i') &lt;/select&gt; &lt;select id=&quot;getScenicMinuteData&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT scenic, min_moment AS `time`, min_quantity AS `count` FROM t_scenic WHERE min_moment = (SELECT MAX(min_moment) FROM t_scenic WHERE &lt;![CDATA[min_moment &lt;= NOW()]]&gt;) GROUP BY scenic, min_moment; &lt;/select&gt; &lt;select id=&quot;getScenicTread&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT scenic, min_moment AS `time`, min_quantity AS `count` FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 8)=DATE_FORMAT(NOW(), '%Y%m%d') ORDER BY `time` &lt;/select&gt; &lt;select id=&quot;getHeatDataByDateAndTime&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT longitude AS `lng`, latitude AS `lat`, SUBSTRING(`sec_moment`, 1, 12) AS day, SUM(`sec_quantity`) AS `count` FROM t_heat GROUP BY lng, lat, day HAVING &lt;![CDATA[day &gt;= #{fromDateAndTime} AND day &lt;= #{toDateAndTime}]]&gt; &lt;/select&gt; &lt;select id=&quot;getScenicDataByDateAndTime&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; SELECT scenic, SUBSTRING(`min_moment`, 1, 12) AS `time`, min_quantity AS `count` FROM t_scenic WHERE &lt;![CDATA[min_moment &gt;= #{fromDateAndTime} AND min_moment &lt;= #{toDateAndTime}]]&gt; GROUP BY `scenic`, `min_moment`; &lt;/select&gt; &lt;select id=&quot;getScenicTreadByDateAndTime&quot; resultType=&quot;com.inspur.szy.subowen.travel.travelweb.entity.Scenic&quot;&gt; &lt;/select&gt;&lt;/mapper&gt; 5.4 前端开发所有代码在 GitHub 均有开源，我做的也就只是添加 Ajax 请求而已。 ❓ 问题汇总 序号 发生日期 问题描述 是否解决 解决措施 备注 0001 2024-06-13 执行代码：出现编译错误： 是 修改代码：val topics = &quot;...&quot; 为 val topics = Array(&quot;...&quot;)这里报错的原因是出现了无法重载方法的问题。因为错误的传递了所需要的参数，导致Subscribe方法出现了不期待的重载。修改代码到正确的格式即可。 老师协助解决 0002 2024-06-13 使用updateStatusByKey出现数据库数据不断增加的问题问题原因：1. MySQL数据表设计不合理，设置了一个列为 id 并将其设置为主键2. SQL语句有待优化 是 1. 修改MySQL表的结构，设置联合主键 2. 修改SQL语句，使用 ON DUPLICATE KEY 语法 老师协助解决 0003 2024-06-13 执行程序出现错误：ERROR [main] (Logging.scala:94) - Failed to bind SparkUI问题原因：每一个Spark任务都会占用一个SparkUI端口，默认为4040，如果被占用则依次递增端口重试。但是有个默认重试次数，为16次。16次重试都失败后，会放弃该任务的运行。 是 初始化 SparkConf 时，添加conf.set(&quot;spark.port.maxRetries&quot;, &quot;100&quot;)语句；使用 spark-submit提交任务时，在启动命令行中添加–conf spark.port.maxRetries=100 \\该参数设置向后递增100次寻找端口 自主解决 0004 2024-06-17 不是很严重的问题，当return 0 或未产生数据时，使用 Edge 浏览器会出现屏幕上没有输出值的情况 是 替换为Chrome即可解决问题，不过也没啥必要 水 0005 2024-06-18 在 TravelMapper.xml 中编写带 &lt; 和&gt; 的 SQL 代码，导致出现问题：Caused by: org.xml.sax.SAXParseException: 元素内容必须由格式正确的字符数据或标记组成。问题原因：XML文件会将&lt; 或 &gt; 当作是标记，导致错误 是 参考文章使用标记&lt;![CDATA[ ]]&gt;将包含 &lt; 或 &gt; 的语句包含住，如：&lt;![CDATA[min_moment &lt;= NOW()]]&gt; 自主解决 0006 2024-06-19 同学出现问题：生产者数据正常写入、客户端（Windows）和服务器端（Linux）时间正确，MySQL无法通过NOW() 获取到当前时间的数据。 是 MySQL时区问题。 协助同学解决 0007 2024-06-20 同学出现问题：虚拟机掉网卡 是 虚拟机网卡不见了,重新开机虚拟机网卡消失连接不上–虚拟机网卡掉了 协助同学解决","link":"/2024/06/11/OnlineTravelBigdataPlatform/"},{"title":"SparkQuickIN","text":"快速入门 Spark[TOC] ⛳︎ 1. 开始 SparkSpark官网：Apache Spark™ - Unified Engine for large-scale data analytics 1.1 什么是SparkSpark官网的解释：Apache Spark™ is a unified analytics engine for large-scale data processing. Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是加州大学伯克利分校的 AMP实验室 所开源的类 Hadoop MapReduce 的通用并行计算框架，Spark 拥有 Hadoop MapReduce 所具有的优点，但不同于 MapReduce 的是：Job 中间输出结果可以缓存在内存中，从而不再需要读写 HDFS，减少磁盘数据交互，因此 spark 能更好地适用于数据挖掘与机器学习等需要迭代的算法。 Spark是 Scala 编写，方便快速编程。 其特点是：高速、使用简单、通用、可以在多处运行。 1.2 总体技术栈讲解 Spark 提供了 Sparkcore RDD、Spark SQL、Spark Streaming、Spark MLlib、Spark GraphX等技术组件，可以一站式的完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见的任务。这就是Spark一站式开发的特点。 1.3 Spark 和 MapReduce 的区别1.3.1 MapReduce 的原理MapReduce 在运算时需要多次进行磁盘 I/O。下面是一个简单的 MapReduce 过程： 视频链接：https://www.bilibili.com/video/BV1TB4y1i7kk/ 在这个视频中，可以看出MapReduce 过程中需要多次磁盘 I/O，落地到HDFS上。 1.3.2 Spark 是如何做的 可以看到，MapReduce 的多个 Job 之间相互独立，每个 Job 完成后的数据都需要存储到文件系统中。每个 Job 中也可能会存在大量的磁盘 I/O ，这样会使得 MapReduce 的速度很慢。相比于 MapReduce，Spark使用了 DAG 有向无环图。使多个任务串联起来，将结果存储在内存中（当然内存不够还是要将数据缓存在磁盘中）直接进行运算，避免了大量的磁盘I/O。 1.3.3 Spark 和 MapReduce 的一些联系Spark 和 MapReduce 都是分布式计算框架，Spark 计算中间结果基于内存缓存，MapReduce 基于HDFS存储。也正因此，Spark处理数据的能力一般是 MapReduce的三到五倍以上，Spark 中除了基于内存计算这一个计算快的原因，还有DAG(DAG Schedule)有向无环图来切分任务的执行先后顺序。 1.4 Spark APISpark API 有多种语言支持，分别包括：Scala、Java、Python、R、SQL 等。 1.5 Spark 的运行模式 Local：多用于本地测试，如在：Eclipse、IDEA 中编写测试程序等。 Standalone：Spark 自带的资源调度框架，它支持完全分布式。Standalone模式也叫作独立模式，其自带完整的服务，可单独部署到一个集群中，无序依赖任何其他资源管理系统。 从一定程度上来说，该模式是 Local 模式和 Yarn 模式的基础。 Yarn： Hadoop 生态圈里的一种资源调度框架，Spark也是可以基于 Yarn 来计算的。 若要使用 Yarn 来进行资源调度，必须实现ApplicationMaster 接口，Spark 实现了这个接口，所以可以基于 Yarn 来进行资源调度。 Mesos：也是一种资源调度框架（了解即可）。 🥑 2. SparkCore2.1 RDD2.1.1 RDD 的概念RDD(Resilient Distribute Dataset)：弹性分布式数据集。 RDD → 算子 → Other RDD，RDD 经过算子的运算会变成其他的 RDD。 （重点）RDD的特点：① 分区的；② 并行操作的；③ 不可变的。 2.1.2 RDD 的五大特性 每个RDD 由一系列的 Partition 组成。 函数是作用在每一个 Partition (Split) 上的。 RDD 中有一系列的依赖关系，或者说每个RDD都会依赖其他一系列的RDD。 分区器是作用在 &lt;K, V&gt; 格式的 RDD 上，即：&lt;K, V&gt; 的 RDD 可以通过 Partitioner 进行自定义分区。 RDD提供一系列的最佳计算位置。数据在哪里，计算就在哪里，移动数据不如移动计算。 2.1.3 RDD 理解图 Spark 中读取文件是使用 SparkContext 对象调用 textFile 方法，实际上底层和 MapReduce 读取 HDFS 文件的方式是相同的，读取之前先要进行 split 切片。默认情况下 Split 的大小和 Block 块的大小相同。 一些问题： RDD的分布式体现在那些方面？ RDD 由一系列的 Partition 构成，并且 Partition 是分布在不同的节点上的。这就体现了 RDD 的分布式。 哪里体现了 RDD 的弹性？ RDD 由一系列的 Partition 组成，其大小和数量都是可以改变的。默认情况下，Partition 的个数和 Block 块的个数相同。 哪里体现了 RDD 的容错？ RDD 之间存在一系列的依赖关系，子RDD 可以找到对应的父RDD ，然后通过一系列计算得到得出响应的结果，这就是容错的体现。 RDD 提供计算最佳位置，体现了数据本地化，体现了大数据中”移动数据不如移动计算“的理念。 一些注意事项： textFile 方法底层封装的是 MapReduce 读取文件的方式，读取文件之前先进行 Split 切片，默认 Split 大小是一个 Block 的大小。 RDD 实际上不存储数据，但是为了方便理解，可以理解为存储数据。 什么是 &lt;K, V&gt; 格式的 RDD，如果 RDD 里面存储的数据都是二元组对象，那么这个 RDD 我们就叫做 &lt;K, V&gt; 格式的 RDD。 2.1.4 SparkRDD 编程模型 创建 SparkContext 对象 创建 RDD 计算 RDD 输出结果（如控制台打印测试，存储等） 关闭 SparkContext 2.1.5 WordCount 案例123456789101112131415161718192021222324252627object WordCount { def main(args: Array[String]): Unit = { // 第一步: 创建 SparkContext 对象 // 对于每个 Spark 程序来说, 最重要的就是两个对象: SparkConf 和 SparkContext val conf = SparkConf() conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;Local&quot;) val sparkContext = SparkContext(conf) // 创建 RDD val line: RDD[String] = sc.textFile(&quot;files/order.csv&quot;) // 计算 RDD val word: RDD[String] = sc.flatMap(x =&gt; x.split(&quot; &quot;)) val pair: RDD[(String, Int)] = word.map(x =&gt; (x, 1)) // val result: RDD[(String, Int)] = pair.reduceByKey((x, y) =&gt; {x + y}) val result: RDD[(String, Int)] = pair.reduceByKey((x, y) =&gt; { println(x + &quot;:&quot; + y) x + y }) // 输出 RDD result.foreach(println) // 关闭 SparkContext sparkContext.stop() }} 2.2 Spark 任务执行原理从下图中，我们可以看到 Spark 的主要角色： 一些名词的解释： Master Node 主节点 Worker Node 从节点 Driver 驱动程序 Executor 执行节点 Cluster Manager 集群管理者 2.2.1 Spark 架构的类比我们可以简单的将这个架构和 YARN 对比一下：Master 就相当于 YARN 中的 ResourceManager，Worker 就相当于 YARN 中的 NodeManager，Driver 相当于 YARN 中的 Application。 2.2.2 Spark 执行原理详细说明 Master 和 Worker 节点 搭建 Spark 集群的时候我们就已经设置好了 Master 节点和 Worker 节点，一个集群有多个Master节点和多个Worker节点。 Master 节点常驻 Master 守护进程，负责管理 Worker 节点，我们从 Master 节点提交应用。 Worker 节点常驻 Worker 守护进程，与 Master 节点通信，并且管理 Executor 进程。 一台机器可以同时作为 Master 和 Worker 节点（e.g. 有四台机器，可以选择一台设置为 Master节点，然后剩下三台设为 Worker节点，也可以把四台都设为 Worker 节点，这种情况下，有一个机器既是 Master 节点又是 Worker 节点）。 一个 Spark 应用程序分为一个驱动程序 Driver 和多个执行程序 Executors 两种。 Driver 和 Executor 进程Driver 进程就是应用的 main() 函数并且构建 SparkContext 对象，当我们提交了应用之后，便会启动一个对应的 Driver 进程，Driver 本身会根据我们设置的参数占有一定的资源（主要指 CPU Core 和 Memory）。 根据部署模式的不同，Driver 可以运行在 Master 上，也可以运行 Worker上，Driver 与集群节点之间有频繁的通信。上图展示了 Driver 在 Master 上的部署的情况。 如上图所示，Driver首先会向**集群管理者Cluster Manager**，如Standalone、Yarn、Mesos 申请 Spark 应用所需的资源，也就是Executor，然后集群管理者会根据 Spark 应用所设置的参数在各个 Worker 上分配一定数量的 Executor，每个 Executor 都占用一定数量的 CPU和 Memory。 在申请到应用所需的资源以后，Driver 就开始调度和执行我们编写的应用代码了。Driver 进程会将我们编写的 Spark 应用代码拆分成多个 Stage，每个Stage 执行一部分代码片段，并为每个 Stage 创建一批 Tasks，然后将这些 Tasks分配到各个 Executor中执行。这一步即 Driver ---Task--&gt; Worker。 Executor 进程宿主在 Worker 节点上，一个 Worker可以有多个 Executor。每个 Executor 持有一个线程池，每个线程可以执行一个 Task，Executor 执行完 Task 以后将结果返回给 Driver，每个 Executor 执行的 Task 都属于同一个应用。此外 Executor 还有一个功能就是为应用程序中要求缓存的 RDD 提供内存式存储，RDD 是直接缓存在 Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。这一步即 Worker ---Result--&gt; Driver。 Driver 负责任务 Tasks 的分发和结果Results的回收，即任务的调度。如果 Task 的计算结果非常大就不要回收了，会造成OOM（我们在执行程序时可以通过参数指定 Driver 的内存大小，如 1G，如果一个 Worker 的结果是 510M，那么两个接节点上的结果就会超过 1G，导致 OOM）。 2.2 RDD 算子RDD有两种操作算子：分别为转换算子(Transformation) 和 **行动算子(Action)**。算子其实就是函数，只不过在 Scala 中称为算子。 下面表格列出了部分 RDD 算子，完整内容可以查看 [SparkRDD](RDD Programming Guide - Spark 3.0.1 Documentation (apache.org)) 文档。 算子类型 算子方法 算子转换 Transformations map(f: T=>U) RDD[T] => RDD[U] filter(f: T=>Bool) RDD[T] => RDD[T] flatMap(f: T=>Seq[U]) RDD[T] => RDD[U] sample(fraction: Float) RDD[T] => RDD[T](Deterministic sampling) groupByKey() RDD[(K, V)] => RDD[(K, Seq[V])] reduceByKey(f: (V, V)=>V) RDD[(K, V)] => RDD[(K, V)] union() (RDD[T], RDD[T]) => RDD[T] join() (RDD[(K, V)], RDD[(K, W)]) => RDD[(K, (V, W))] cogroup() (RDD[(K, V)], RDD[(K, W)]) => RDD([K, (Seq[V], Seq[W])]) crossProduct() (RDD[T], RDD[U]) => (RDD[(T, U)]) mapValues(f: V=>W) RDD[(K, V)] => RDD[(K, W)](Preserves Partitioning) sort(c: Comparator[K]) RDD[(K, V)] => RDD[(K, V)] partitionBy(p: Partitioner[K]) RDD[(K, V)] => RDD[(K, V)] ... ... Actions count() RDD[T] => Long collect() RDD[T] => Seq[T] reduce(f: (T, T)=>T) RDD[T] => T lookup(k: K) RDD[(K, V)] => Seq[V](On hash/range partitioned RDDs) save(path: String) Outputs RDD to a Storage System, e.g. HDFS foreach(func) - ... ... 2.2.1 Transformation 转换算子点击快速跳转到转换算子列表：Transformations Transformation 转换算子有延迟执行的特点，具有**懒加载(Lazy)**的特性。 下面列出常用的行动算子及用法： map(func) 返回一个新的分布式数据集，由每个原元素经过func函数转换后组成。 12345678910111213141516object MapDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;MapDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(0, 10, 15, 20, 25, 30, 35, 40, 45, 50) val listRDD = sc.parallelize(list, 5) listRDD.foreach(println) println() val retRDD = listRDD.map(num =&gt; num * 7) retRDD.foreach(num =&gt; println(num)) sc.stop() }} mapPartition(func) 将函数用在每个RDD的分区上 1234567891011121314151617181920212223object MapPartitionDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;MapPartitionDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(0, 10, 15, 20, 25, 30, 35, 40, 45, 50) val listRDD = sc.parallelize(list, 3) // 设置三个分区 // 对比 map 和 mapPartition 的区别 listRDD.map( x =&gt; { println(&quot;Map 执行一次&quot;) x + 1 }).foreach(println) println() listRDD.mapPartitions( x =&gt; { println(&quot;MapPartition 执行一次&quot;) x.map(x =&gt; {println(&quot;mapPartition 里的map&quot;); x + 1}) // 这个 map 不是 RDD 里的 map, 而是 Iterator 中的 map }).foreach(println) }} filter(func)返回一个新的数据集，由经过func函数后返回值为true的元素组成。 12345678910111213object FilterDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FilterDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(0, 10, 15, 20, 25, 30, 35, 40, 45, 50) val listRDD = sc.parallelize(list) val retRDD = listRDD.filter(num =&gt; num % 2 == 0) retRDD.foreach(println) sc.stop() }} flatMap(func)类似于map，但是每一个输入元素，会被映射为 0 到多个输出元素（因此，func 函数的返回值是一个 Seq，而不是单一元素）。 12345678910111213object FlatMapDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FlatMapDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List(&quot;hello you&quot;, &quot;hello he&quot;, &quot;hello me&quot;) val listRDD = sc.parallelize(list) val wordsRDD = listRDD.flatMap(line =&gt; line.split(&quot; &quot;)) wordsRDD.foreach(println) sc.stop() }} sample(withReplacement, frac, seed)根据给定的随机种子 seed，随机抽样出数量为 frac 的数据。 1 union(otherDataset)返回一个新的数据集，由原数据集和参数联合而成。 1 groupByKey([numTasks])在一个由 &lt;K, V&gt; 对组成的数据集上调用，返回一个 &lt;K, Seq[V]&gt; 对的数据集。注意：默认情况下，使用 8 个并行任务进行分组，你可以传入 numTask 可选参数，根据数据量设置不同数目的 Task。使用该算子可以将相同Key的元素聚集到一起，最终把所有相同Key的元素合并成一个元素，该元素的Key不变，Value则聚集到一个集合中。 1 reduceByKey(func, [numTasks])在一个&lt;K, V&gt;对的数据集上使用，返回一个&lt;K, V&gt;对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和 groupByKey类似，任务的个数是可以通过第二个可选参数来配置的。 1 join(otherDataset, [numTasks])在类型为&lt;K, V&gt;和&lt;K, W&gt;类型的数据集上调用，返回一个&lt;K, &lt;V, W&gt;&gt;对，每个key中的所有元素都在一起的数据集。 sortByKey() 和 sortBy()，sortByKey() 函数需要在类型为 &lt;K, V&gt; 类型的数据集上调用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152object SortByKeyDemo { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SortByKeyDemo&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) val list = List( &quot;1,李 磊,22,175&quot;, &quot;2,刘银鹏,23,175&quot;, &quot;3,齐彦鹏,22,180&quot;, &quot;4,杨 柳,22,168&quot;, &quot;5,敦 鹏,20,175&quot; ) val listRDD: RDD[String] = sc.parallelize(list) listRDD.foreach(println) println() // 3,齐彦鹏,22,180 // 1,李磊,22,175 // 2,刘银鹏,23,175 val resultRDD = listRDD.map(x =&gt; { val fields = x.split(&quot;,&quot;) println(fields(0) + &quot; &quot; + fields(1) + &quot; &quot; + fields(2) + &quot; &quot; + fields(3) + &quot; &quot;) (fields(0), fields(1), fields(2), fields(3)) }).map(x =&gt; {(x._3, x._1)}) // 分区数会影响最终打印结果, 设置分区为 2, 所有分区最终会聚合为两个分区, 打印时显示每个分区的排序 // 这就可能导致打印输出时出现不同分区排序数据交叉的情况。 // 所以我们一般设置分区为 1, 表示排序结果聚合到一个分区 resultRDD.sortByKey(true, 2).foreach(println) println() val sortBy_RDD = listRDD.map(x =&gt; { val fields = x.split(&quot;,&quot;) // println(fields(0) + &quot; &quot; + fields(1) + &quot; &quot; + fields(2) + &quot; &quot; + fields(3) + &quot; &quot;) (fields(0), fields(1), fields(2), fields(3)) }) println(&quot;SortBy_001&quot;) sortBy_RDD.sortBy(_._1, true, 1).foreach(println) println(&quot;SortBy_002&quot;) sortBy_RDD.sortBy(_._2, true, 1).foreach(println) println(&quot;SortBy_003&quot;) sortBy_RDD.sortBy(_._3, true, 1).foreach(println) println(&quot;SortBy_004&quot;) sortBy_RDD.sortBy(_._4, true, 1).foreach(println) sc.stop() }} 2.2.2 Action 行动算子Action 行动算子具有触发执行的特点，一个 Application 应用程序有几个 Action 类算子执行，就有几个 Job 运行。 点击快速跳转到 SparkRDD Action 列表： Actions 下面列出常用的行动算子及用法： reduce(func) 通过函数 func 聚集数据集中的所有元素。func 函数接受 2 个参数，返回 1 个值。这个函数必须是关联性的，确保可以被正确的并发执行。关于 reduce 的执行过程，可以对比 Scala 中类似的 reduce函数。 不同于 Transformation 算子，执行后结果是RDD，执行 Action 算子之后，其结果不再是 RDD，而是一个标量。 1234567891011121314151617181920212223242526272829object Action_Reduce { def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;Action_Reduce&quot;) Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) val sc = new SparkContext(conf) // val list = List(0, 5, 15, 20, 25, 30) // 如果用这个 list 输出为: 95 val list = List( &quot;1,李 磊,22,175&quot;, &quot;2,刘银鹏,23,175&quot;, &quot;3,齐彦鹏,22,180&quot;, &quot;4,杨 柳,22,168&quot;, &quot;5,敦 鹏,20,175&quot; ) val listRDD = sc.parallelize(list) listRDD.foreach(println) println() val ret = listRDD.reduce( (v1, v2) =&gt; { println(&quot;[LOGS] v1 &gt; &quot; + v1 + &quot; &quot;) println(&quot;[LOGS] v2 &gt; &quot; + v2 + &quot; &quot;) println(&quot;[LOGS] v1 + v2 &gt; &quot; + (v1 + v2)) v1 + v2 } ) println(&quot;ret: &quot; + ret) sc.stop() }} 1234567891011121314151617181920输出如下：3,齐彦鹏,22,1801,李 磊,22,1754,杨 柳,22,1682,刘银鹏,23,1755,敦 鹏,20,175[LOGS] v1 &gt; 3,齐彦鹏,22,180 [LOGS] v2 &gt; 4,杨 柳,22,168 [LOGS] v1 + v2 &gt; 3,齐彦鹏,22,1804,杨 柳,22,168[LOGS] v1 &gt; 3,齐彦鹏,22,1804,杨 柳,22,168 [LOGS] v2 &gt; 5,敦 鹏,20,175 [LOGS] v1 + v2 &gt; 3,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175[LOGS] v1 &gt; 1,李 磊,22,175 [LOGS] v2 &gt; 2,刘银鹏,23,175 [LOGS] v1 + v2 &gt; 1,李 磊,22,1752,刘银鹏,23,175[LOGS] v1 &gt; 1,李 磊,22,1752,刘银鹏,23,175 [LOGS] v2 &gt; 3,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175 [LOGS] v1 + v2 &gt; 1,李 磊,22,1752,刘银鹏,23,1753,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175ret: 1,李 磊,22,1752,刘银鹏,23,1753,齐彦鹏,22,1804,杨 柳,22,1685,敦 鹏,20,175 collect 在 Driver 的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用 filter 或者其它操作后，返回一个足够小的数据子集再使用，直接将整个 RDD 集 Collect返回，很可能会让 Driver 程序 OOM，这点尤其需要注意。 1 count 1 take 1 first 1 saveAsTextFile 1 foreach 1 saveAsNewAPIHadoopFile 1 2.2.3 其他算子Spark中还有许多其他种类的算子，体现了 Spark 算子的灵活性，其中包括将数据进行持久化的算子。 cache：懒加载执行的，必须有一个 Action 触发算子触发执行。 persist：懒加载执行的，必须有一个 Action 触发算子触发执行。 checkpoint：算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系。 2.2.3 宽依赖和窄依赖 宽依赖 ① 子 RDD 的每个分区依赖于所有的父 RDD 分区 ② 对单个 RDD 基于 Key进行重组和 Reduce，如 groupByKey、reduceByKey ③ 对两个 RDD 基于 Key 进行 join 和重组，如 join ④ 经过大量 shuffle 生成的 RDD，建议进行缓存。这样避免失败后重新计算带来的开销。 窄依赖 ① 子RDD的每个分区依赖于常数个父分区（与数据规模无关） ② 输入输出一对一的算子，且结果RDD的分区结构不变。主要是map/flatmap ③ 输入输出一对一的算子，但结果RDD的分区结构发生了变化，如union/coalesce ④ 从输入中选择部分元素的算子，如 filter、distinct、substract、sample","link":"/2024/06/12/SparkQuickIN/"},{"title":"CppGamingDEV_PVZ_BASE_EASYX","text":"🎮 从零开始进行 C++ 游戏开发[TOC] 游戏场景概念 什么是场景?如果将要游戏程序比作是一场有玩家参与的盛大演出，那场景就是演出过程中的一幕。在不同的幕中，会有不同的剧本逻辑，也可能会有不同的角色登场，这些角色即游戏开发中常提到的 GameObject 的概念。不论是玩家、敌人还是子弹、道具等，这些从概念上讲都是 GameObject 的范畴。他们接受着不同的场景剧本的指挥，进行着不同逻辑的演出。了解了这些，我们就可以对程序的流程进行宏观的划分，游戏的主菜单是一个场景，玩家角色选择界面也是一个场景，游戏局内的逻辑也需要放置在一个单独的场景中。所以我们就可以定义一个 Scene 场景基类，主菜单、角色选择、局内游戏 作为新类分别继承 Scene 类。 游戏主循环框架概念 什么是游戏主循环框架?游戏程序的主体是一个永不停歇的死循环。在每次循环中，我们读取玩家的操作信息，并根据这些操作，处理玩家数据更新，在最后的绘图阶段将游戏画面根据这些更新后的数据渲染出来。1234567初始化();while(true) { 读取操作(); 处理数据(); 绘制画面();}施放资源; C++ 头文件 为什么需要头文件卫士?如果不使用头文件卫士，可能会导致出现重定义的问题。例如，我们有一个头文件 A.h ，有一个头文件 B.h，头文件 B.h 中使用了 A.h 的内容。当我们在主函数里面使用了 A.h 和 B.h 的时候，使用 #include 时会将头文件中的内容全部复制到 #include 的部分。这样的话如果不加头文件卫士，就会导致重定义的问题，如下： 1234567891011121314151617181920212223// A.hint a;// B.h#include &quot;A.h&quot;int b;// main.cpp#include &quot;A.h&quot; // int a;#include &quot;B.h&quot; // int a; // int b; // 出现重定义错误// 错误信息如下In file included from B.h:1, from main.cpp:2:A.h:1:5: error: redefinition of 'int a' 1 | int a; | ^In file included from main.cpp:1:A.h:1:5: note: 'int a' previously declared here 1 | int a; | ^ 在 MSVC 编译器中，头文件卫士如下： 1#pragma once 在其他的一些编译器中，常写为这样： 1234#ifndef __HEADER_H__#define __HEADER_H__#endif 场景管理器游戏程序是一个巨大的死循环，也是一个巨大的状态机。不同的游戏场景代表着不同的状态，管理着这些状态的“状态机”，在游戏开发中有一个特殊的名字——场景管理器。","link":"/2024/06/23/CppGamingDEV-PVZ-BASE-EASYX/"},{"title":"DataVisualization_USE_R_ECHARTS","text":"📺 数据可视化基础（使用R和ECharts）本文会从 R 和 ECharts 两个方面来展开数据可视化技术的一些使用方法。 [TOC] 📊 1. 数据可视化概述 数据可视化概念 狭义概念：指的是数据用统计图表方式呈现 广义概念：是数据可视化、信息可视化以及科学可视化等等多个领域的统称 数据可视化分类 数据可视化 信息可视化 科学可视化 ⭐*** 数据可视化作用*** 数据表达：通过计算机图形图形技术来更加友好地展示数据信息，以方便人们理解和分析数据。 数据操作：以计算提供的界面、接口和协议等条件为基础完成人与数据的交互需求。 数据分析：通过计算机获得多维、多源、异构和海量数据所隐含信息的核心手段，它是数据存储、数据转换、数据计算和数据可视化的综合应用。 ⭐ 数据可视化的工具 R、ECharts、D3 … Python、pyecharts、Excel … 第三方商业工具：FineBI、FineReport … 学习数据可视化需要了解（具备）什么？ 对色彩的感知：了解原色、间色、复色等，并且了解色彩的视觉感受（心理感受），当然也就是了解即可，色彩这种东西天赋更重要一些，知道三原色是啥就行。 原色：红、黄、蓝。 间色：可存在多个间色，当原色之间 1:1 匹配时则为：橙、绿、紫。 复色：可存在多个复色（一定包含三原色），两个间色或一种原色和其对应的间色（黄 + 紫、蓝 + 橙）相混合得到复色。 色彩感受：轻重感、冷暖感、前后感、收缩感 … 数据可视化的基本流程 数据采集 数据处理 可视化映射 用户感知 数据可视化的一些设计技巧 设计原则：注重用户的视觉体验 设计技巧： ① （颜色）建立视觉层次，用醒目的颜色突出数据，淡化其他元素 ② （内容）高亮显示重点内容 ③ （跨度）提升不同区域的色阶跨度 ④ （场景）借助场景来表现数据指标 ⑤ （转换）将抽象的不易理解的数字转换为容易被人感知的图表 ⑥ （简洁）尽量让图表简洁 数据可视化的图表类型 在统计图表中每一种类型的图表中都可包含不同的数据可视化图形，如：柱状图、饼图、气泡图、热力图、趋势图、直方图、雷达图、色块图、漏斗图、和弦图、仪表盘、面积图、折线图、密度图 以及 K线图等。 🎫 2. R - R语言快速入门参考文献：数据科学中的 R 语言 (bookdown.org) 2.1 R 语言概述R语言是为数学研究工作者设计的一种数学编程语言，主要用于统计分析、绘图、数据挖掘数据可视化。 目前主流的数据分析语言有 R、Python、Matlab 等。 R Python Matlab 语言学习难易程度 入门难度低 入门难度一般 入门难度一般 使用场景 数据分析、数据挖掘、机器学习、数据可视化等。 数据分析、机器学习、矩阵运算、科学数据可视化、数字图像处理、Web应用、网络爬虫、系统运维等。 矩阵计算、数值分析、科学数据可视化、机器学习、符号计算、数字图像处理、数字信号处理、仿真模拟等。 第三方支持 拥有大量的 Packages，能够调用 C、C++、Fortran、Java等其他程序语言。 拥有大量的第三方库，能够简便地调用C、C++、Fortran、Java等其他程序语言。 拥有大量专业的工具箱，在新版本中加入了对C、C++，Java的支持。 流行领域 工业界 ≈ 学术界 工业界 ＞ 学术界 工业界 ≤ 学术界 软件成本 开源免费 开源免费 商业收费 2.2 R 语言中包的概念和使用 软件包和软件库 软件包：R 中的包是 R 函数、数据、预编译代码以一种定义完善的格式组成的集合。 软件库：软件库指的是一个包含了若干个包的目录。你可以拥有一个系统级别的软件库，也可以为每个用户单独设立一个软件库。 R 自带了一系列默认包（包括 base、datasets、utils、grDevices、graphics、stats以及methods），它们提供了种类繁多的默认函数和数据集。 base：包含基础的 R 函数 database：自带的数据包，里面的数据结构包括矩阵、向量、数据框等 utils：工具函数包 grDevices：基础绘图工具包，提供调整图形颜色和字体的功能，可用于配色 graphics：基础绘图工具包 stats：统计函数的扩展包 methods：R 对象和其他编程工具定义方法和类比的扩展包 包的相关操作 查看包的安装路径：.libPaths() 12&gt; .libPaths()[1] &quot;D:/R/R-4.3.3/library&quot; 查看已安装的包：library() 查看编译环境下已载入的包的列表：search() 123&gt; search() [1] &quot;.GlobalEnv&quot; &quot;tools:rstudio&quot; &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; [6] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; 下载包： 通过 CRAN (The Comprehensive R Archive Network) 下载。CRAN 是 R 语言的综合档案网络。用户可以从 CRAN 上下载并安装各种第三方包来扩展 R 语言的功能。 ① 指定包名进行安装：install.packages(&quot;PackageName&quot;) ② 使用图形界面安装：install.packages() 手动安装：https://cran.r-project.org/web/packages/XML/index.html 使用 RStudio 安装 包的卸载：remove.packages(&quot;PackageName&quot;) 包的载入：library(&quot;PackageName&quot;) 12345&gt; library(&quot;XML&quot;)&gt; search() # 可以看到 &quot;package:XML&quot; 已经被加载进来了 [1] &quot;.GlobalEnv&quot; &quot;package:XML&quot; &quot;tools:rstudio&quot; &quot;package:stats&quot; &quot;package:graphics&quot; [6] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; [11] &quot;package:base&quot; 包的取消载入：detach(&quot;package:PackageName&quot;, unload = True) 1234&gt; detach(&quot;package:XML&quot;, unload = TRUE)&gt; search() [1] &quot;.GlobalEnv&quot; &quot;tools:rstudio&quot; &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; [6] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; 2.3 R 语言的工作空间的概念和使用 概念：工作空间(Work Space) 就是当前 R 的工作环境，它储存着所有用户定义的对象（向量、矩阵、函数、数据框、列表）。在一个 R 会话结束时，用户可以将当前工作空间保存到一个镜像中，并在下次启动 R 时自动载入它。 设置工作目录：setwd() 或使用 RStudio 可视化窗口进行设置（Session → Set Working Directory） 获取工作目录：getwd() 2.4 R 语言的六大原子数据类型 数字型(Numeric) 逻辑型(Logical) 整型(Integer) 字符型(Character) 复合型(Complex) 原子型(Raw) 对于不同原子数据类型可以按照如下方式定义： 1234567891011121314151617181920212223242526272829&gt; # 数值型(Numeric)&gt; varNumeric = 12.5&gt; varNumeric[1] 12.5&gt; # 逻辑型(Logical)&gt; varLogical = TRUE&gt; varLogical[1] TRUE&gt; # 整型(Integer)&gt; varInteger = 25L&gt; varInteger[1] 25&gt; # 字符型(Character)&gt; varCharacter = &quot;Hello World&quot;&gt; varCharacter[1] &quot;Hello World&quot;&gt; # 复合型(Complex)&gt; varComplex = 2 + 3i&gt; varComplex[1] 2+3i&gt; # 原子型(Raw)&gt; varRaw = charToRaw('Hello')&gt; varRaw[1] 48 65 6c 6c 6f 2.5 R 语言变量使用 定义变量并赋值：使用 &lt;-、=、-&gt; 123456789101112131415&gt; var1 = 12.5 # 定义 var1, 并赋值为 12.5&gt; print(var1) # 输出 var1[1] 12.5&gt; var2 &lt;- 13 # 定义 var2, 并赋值为 13(由右向左)&gt; print(var2) # 输出 var2[1] 13&gt; 14 -&gt; var3 # 定义 var3, 并赋值为 14(由左向右)&gt; print(var3) # 输出 var3[1] 14&gt; var2 -&gt; var3&gt; print(var3)[1] 13 变量的打印：print() 或 cat() print() 和 cat() 都可以向控制台输出文字，区别在于 print() 有返回值，其会返回输出的内容，无法使用转义字符，当使用转义字符时，会直接输出转义字符；而 cat() 没有返回值，可以使用转义字符。 1234567891011121314151617&gt; varX = &quot;Hello World&quot;&gt; varY = &quot;Hello World\\n&quot;&gt; printX &lt;- print(varX)[1] &quot;Hello World&quot;&gt; catX &lt;- cat(varX, &quot;\\n&quot;) # 如果不添加转义字符会无法换行Hello World &gt; printY &lt;- print(varY)[1] &quot;Hello World\\n&quot; # 使用 print() 直接将转义字符进行了输出&gt; catY &lt;- cat(varY)Hello World # 使用 cat() 没有将转义字符直接输出, 而是转化成了响应的格式, 如&quot;换行&quot;&gt; print(printX) # print() 有返回值[1] &quot;Hello World&quot;&gt; print(catX) # cat() 没有返回值NULL 变量的查看：ls() 1234567891011121314151617&gt; ls() # 输出所有变量 [1] &quot;catX&quot; &quot;catY&quot; &quot;cyl.f&quot; &quot;printX&quot; &quot;printY&quot; &quot;score&quot; &quot;tempdens&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot;[11] &quot;varX&quot; &quot;varY&quot; &gt; ls(pattern = &quot;var&quot;) # 模式匹配部分变量, 包含 &quot;var&quot; 的[1] &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; &quot;varX&quot; &quot;varY&quot;&gt; ls(pattern = &quot;*&quot;) # 模式匹配部分变量, 使用通配符 [1] &quot;catX&quot; &quot;catY&quot; &quot;cyl.f&quot; &quot;printX&quot; &quot;printY&quot; &quot;score&quot; &quot;tempdens&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; [11] &quot;varX&quot; &quot;varY&quot; &gt; ls(pattern = &quot;2&quot;) # 模式匹配部分变量, 包含 &quot;2&quot; 的[1] &quot;var2&quot;&gt; ls(all.names = TRUE) # [1] &quot;.Random.seed&quot; &quot;catX&quot; &quot;catY&quot; &quot;cyl.f&quot; &quot;printX&quot; &quot;printY&quot; &quot;score&quot; [8] &quot;tempdens&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; &quot;varX&quot; &quot;varY&quot; 变量的删除：rm() 12345678&gt; rm(&quot;var1&quot;, &quot;var2&quot;) # 删除 &quot;var1&quot; 和 &quot;var2&quot;&gt; rm(list = ls(pattern = &quot;var&quot;)) # 删除 匹配到 &quot;var&quot; 的变量, list 是一个可以指定的参数&gt; rmlist = ls(pattern = &quot;X&quot;)&gt; rmlist[1] &quot;catX&quot; &quot;printX&quot;&gt; rm(list = rmlist) [⭐] 变量的数据类型判别和转换 判别：is.XXX()，返回 True/False 123456789101112131415161718192021222324252627282930313233&gt; # 数值型(Numeric)&gt; varNumeric = 12.5&gt; varNumeric[1] 12.5&gt; is.numeric(varNumeric)[1] TRUE&gt; is.logical(varNumeric)[1] FALSE&gt; is.integer(varNumeric)[1] FALSE&gt; is.character(varNumeric)[1] FALSE&gt; is.complex(varNumeric)[1] FALSE&gt; is.raw(varNumeric)[1] FALSE&gt; # 逻辑型(Logical)&gt; varLogical = TRUE&gt; varLogical[1] TRUE&gt; is.numeric(varLogical)[1] FALSE&gt; is.logical(varLogical)[1] TRUE&gt; is.integer(varLogical)[1] FALSE&gt; is.character(varLogical)[1] FALSE&gt; is.complex(varLogical)[1] FALSE&gt; is.raw(varLogical)[1] FALSE 转换：as.XXX() 12345678910111213141516&gt; # 变量类型转换&gt; varNumToChar &lt;- as.character(varNumeric)&gt; varNumToChar[1] &quot;12.5&quot;&gt; is.numeric(varNumToChar)[1] FALSE&gt; is.logical(varNumToChar)[1] FALSE&gt; is.integer(varNumToChar)[1] FALSE&gt; is.character(varNumToChar)[1] TRUE &lt;-------------------- 可以看到已经转换成了字符型&gt; is.complex(varNumToChar)[1] FALSE&gt; is.raw(varNumToChar)[1] FALSE [⭐] 变量的数据类型查看 mode()：查看数据元素类型 typeof()：查看数据元素类型，基本等同于 mode() ，比 mode() 更为详细 class()：查看数据结构，vector、matrix、array、dataframe、list 123456789101112131415161718192021222324252627282930313233343536373839404142&gt; # 查看变量类型&gt; mode(varNumeric)[1] &quot;numeric&quot;&gt; typeof(varNumeric)[1] &quot;double&quot;&gt; class(varNumeric)[1] &quot;numeric&quot;&gt; mode(varLogical)[1] &quot;logical&quot;&gt; typeof(varLogical)[1] &quot;logical&quot;&gt; class(varLogical)[1] &quot;logical&quot;&gt; mode(varInteger)[1] &quot;numeric&quot;&gt; typeof(varInteger)[1] &quot;integer&quot;&gt; class(varInteger)[1] &quot;integer&quot;&gt; mode(varCharacter)[1] &quot;character&quot;&gt; typeof(varCharacter)[1] &quot;character&quot;&gt; class(varCharacter)[1] &quot;character&quot;&gt; mode(varComplex)[1] &quot;complex&quot;&gt; typeof(varComplex)[1] &quot;complex&quot;&gt; class(varComplex)[1] &quot;complex&quot;&gt; mode(varRaw)[1] &quot;raw&quot;&gt; typeof(varRaw)[1] &quot;raw&quot;&gt; class(varRaw)[1] &quot;raw&quot; [⭐] 字符串的应用 R语言中的文本，或者原子数据类型中 Character。在 R 语言中的单引号' ' 或 双引号&quot; &quot; 中写入的任何值都被视为字符串。在字符串构造中应用的规则： 在字符串的开头和结尾的引号应该是两个双引号或两个单引号。它们不能被混合。 双引号可以插入到以单引号开头和结尾的字符串中 单引号可以插入以双引号开头和结尾的字符串。 双引号不能插入以双引号开头和结尾的字符串。 单引号不能插入以单引号开头和结尾的字符串。 2.7 R 语言的运算符的概念和使用 运算符的概念 算数运算符 +加、-减、*乘、/除 %% 整除取余 %/% 整除 1234567891011&gt; # 相除&gt; 6 / 4[1] 1.5&gt; # %% 整除取余&gt; 6 %% 4[1] 2&gt; # %/% 整除&gt; 6 %/% 4[1] 1 关系运算符 &lt; 小于、&gt; 大于、= 等于 &lt;=小于等于、&gt;= 大于等于 != 不等于 逻辑运算符 &amp; | &amp;&amp; || &amp;&amp;和||为值逻辑，&amp;和|为位逻辑说人话就是，&amp;&amp; 和 ||是讲两个操作目的值做逻辑运算，无论操作对象是向量还是标量，返回值都是一个逻辑值，（NOW）&amp;&amp; 和 || 运算符只接受长度为 1 的逻辑值作为参数；而 &amp; 和 | 是讲两个对象按位比较，其返回值的长度与对象是标量还是向量有关。 12345678910111213141516171819&gt; # 逻辑运算符&gt; v &lt;- c(3, 1, TRUE, 2+3i)&gt; t &lt;- c(4, 1, FALSE, 2+3i)&gt; print(v&amp;t)[1] TRUE TRUE FALSE TRUE&gt; v &lt;- c(3, 0, TRUE, 2+2i)&gt; t &lt;- c(1, 3, TRUE, 2+3i)&gt; print(v &amp;&amp; t)Error in v &amp;&amp; t : 'length = 4' in coercion to 'logical(1)'&gt; TRUE &amp;&amp; TRUE[1] TRUE&gt; TRUE &amp;&amp; FALSE[1] FALSE&gt; FALSE &amp;&amp; TRUE[1] FALSE&gt; FALSE &amp;&amp; FALSE[1] FALSE 赋值运算符 &lt;- = -&gt; &lt;&lt;- -&gt;&gt; &lt;− 或 = 或 &lt;&lt;− 运算符： 称为左赋值。 1234567891011121314&gt; # 赋值运算符&gt; # 左赋值&gt; v1 &lt;- c(3,1,TRUE,2+3i)&gt; v2 &lt;&lt;- c(3,1,TRUE,2+3i)&gt; v3 = c(3,1,TRUE,2+3i)&gt; print(v1)[1] 3+0i 1+0i 1+0i 2+3i&gt; print(v2)[1] 3+0i 1+0i 1+0i 2+3i&gt; print(v3)[1] 3+0i 1+0i 1+0i 2+3i -&gt; 或 -&gt;&gt; 运算符：称为右赋值。 123456789&gt; # 右赋值&gt; c(3,1,TRUE,2+3i) -&gt; v1&gt; c(3,1,TRUE,2+3i) -&gt;&gt; v2 &gt; print(v1)[1] 3+0i 1+0i 1+0i 2+3i&gt; print(v2)[1] 3+0i 1+0i 1+0i 2+3i 其他运算符 : 为向量创建数字序列。 123&gt; v &lt;- 2:8&gt; print(v)[1] 2 3 4 5 6 7 8 %in% 用于确定元素是否属于向量。 1234567&gt; v1 &lt;- 8&gt; v2 &lt;- 12&gt; t &lt;- 1:10&gt; print(v1 %in% t) # 8 是否再 1~10 中[1] TRUE&gt; print(v2 %in% t) # 12 是否再 1~10 中[1] FALSE %*% 用于求两个向量的内积，也称为点乘 1234567891011121314151617181920212223242526272829&gt; M = matrix( c(2,6,5,1,10,4), nrow = 2, ncol = 3, byrow = TRUE)# M2, 6, 51, 10, 4# t(M)2, 16, 105, 4# M %*% t(M)2*2 + 6*6 + 5*5, 1*2 + 10* 6 + 4*52*1 + 10*6 + 4*5, 1*1 + 10*10 + 4*4&gt; t = M %*% t(M)&gt; print(t) [,1] [,2][1,] 65 82[2,] 82 117&gt; #===================================================================#&gt; M = matrix(c(2, 6, 5, 1, 10, 4), nrow = 2,ncol = 3, byrow = TRUE)&gt; N = matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2, byrow = TRUE)&gt; t = M %*% N&gt; print(t) [,1] [,2][1,] 45 58[2,] 51 66 2.8 R 语言的六大数据对象R 语言有六种基本的数据结构（数据对象）：向量vector、列表list、矩阵matrix、数组array、数据框data.frame 和 因子factor 类型。 向量 vector 概念：向量是最基本的 R 语言数据对象，向量的元素支持六种原子数据类型，即逻辑，整数，双精度，复合，字符和原型。 特征：一个向量的所有元素都必须属于相同的类型。如果不是，R将强制执行类型转换。 创建向量：c()、seq()、rep() c() 这里的c就是 combine 或 concatenate 的意思，它要求元素之间用英文的逗号分隔，且元素的数据类型是统一的，比如都是数值。c() 函数把一组数据聚合到了一起，就构成了一个向量。 12345678910111213141516171819&gt; # 创建向量&gt; # 使用 c()&gt; low &lt;- c(1, 2, 3)&gt; high &lt;- c(4, 5, 6)&gt; sequence &lt;- c(low, high)&gt; sequence[1] 1 2 3 4 5 6&gt; # 给变量命名&gt; x &lt;- c('a' = 5, 'b' = 6, 'c' = 7, 'd' = 8)&gt; xa b c d 5 6 7 8 &gt; x &lt;- c(5, 6, 7, 8)&gt; names(x) &lt;- c('a', 'b', 'c', 'd')&gt; xa b c d 5 6 7 8 如果向量元素很多，用手工一个个去输入，那就成了体力活，不现实。在特定情况下，有几种偷懒方法: seq() 函数可以生成等差数列，from 参数指定数列的起始值，to 参数指定数列的终止值，by 参数指定数值的间距。 1234&gt; # 使用 seq()&gt; s1 &lt;- seq(from = 0, to = 10, by = 0.5)&gt; s1 [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 rep() 是 repeat（重复）的意思，可以用于产生重复出现的数字序列：x 用于重复的向量，times 参数可以指定要生成的个数，each 参数可以指定每个元素重复的次数。 1234567891011&gt; s2 &lt;- rep(x = c(0, 1), times = 3)&gt; s2[1] 0 1 0 1 0 1&gt; s3 &lt;- rep(x = c(0, 1), each = 3)&gt; s3[1] 0 0 0 1 1 1&gt; s4 &lt;- rep(x = c(0, 1), time = 3, each = 3)&gt; s4[1] 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 向量元素访问 12345&gt; # 定义一个向量&gt; t &lt;- seq(from = 10, to = 20, by = 0.4)&gt; t [1] 10.0 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8[24] 19.2 19.6 20.0 t[index]：使用这种方式进行访问，index 默认从 1 开始。 12345&gt; # 方式 1&gt; t[1][1] 10&gt; t[2][1] 10.4 t[Logic Index]： TRUE 表示读取，FALSE 为不读取。 1234567&gt; # 方式 2&gt; t[c(TRUE, TRUE, FALSE, TRUE)] [1] 10.0 10.4 11.2 11.6 12.0 12.8 13.2 13.6 14.4 14.8 15.2 16.0 16.4 16.8 17.6 18.0 18.4 19.2 19.6 20.0&gt; t[c(1, 0, 2, 0, 3, 0, 1)][1] 10.0 10.4 10.8 10.0&gt; t[c(6, 0, 0, 0, 1, 0, 1)][1] 12 10 10 t[name]：通过 name 进行访问。 12345678910&gt; # 方式 3&gt; names(t) &lt;- c(&quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;, &quot;v4&quot;, &quot;v5&quot;, &quot;v6&quot;, &quot;v7&quot;, &quot;v8&quot;, &quot;v9&quot;, &quot;v10&quot;)&gt; t v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.0 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 19.2 19.6 20.0 &gt; t[&quot;v1&quot;]v1 10 t[-index]：索引为负数，则会删除该位置的元素。 1234567891011121314&gt; # 方式 4&gt; t v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.0 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 19.2 19.6 20.0 &gt; t[-1] v2 v3 v4 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.4 10.8 11.2 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 19.2 &lt;NA&gt; &lt;NA&gt; 19.6 20.0 &gt; t[c(-2: -4)] v1 v5 v6 v7 v8 v9 v10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10.0 11.6 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0 16.4 16.8 17.2 17.6 18.0 18.4 18.8 19.2 19.6 20.0 向量元素运算 长度相同的向量 1234567891011121314151617&gt; # 长度不同的向量&gt; v3 = c(10, 20)&gt; v4 = c(10, 20, 10, 20, 10, 20)&gt; add.result = v1 + v3&gt; add.result = v1 + v4&gt; # 向量的运算&gt; # 长度相同的向量&gt; v1 = c(1, 2, 3, 4, 5, 6)&gt; v2 = c(6, 7, 8, 9, 10, 11)&gt; add.result = v1 + v2&gt; sub.result = v1 - v2&gt; add.result[1] 7 9 11 13 15 17&gt; sub.result[1] -5 -5 -5 -5 -5 -5 长度不同的向量 如果长度不同，较短的向量会循环补充到与较长的向量长度相同，在进行运算。但是前提是：长的向量的长度必须为短的向量的整数倍。 123456789&gt; # 长度不同的向量&gt; v3 = c(10, 20)&gt; v4 = c(10, 20, 10, 20, 10, 20)&gt; add.result = v1 + v3&gt; add.result[1] 11 22 13 24 15 26&gt; add.result = v1 + v4&gt; add.result[1] 11 22 13 24 15 26 列表 list 概念：列表是 R 语言的对象集合，可以用来保存不同类型的数据，可以是数字、字符串、向量、另一个列表等，当然还可以包含矩阵和函数。R语言创建列表使用 list() 函数。 列表的创建 1234567891011121314151617181920&gt; # 列表的创建&gt; list_data = list(&quot;Hello&quot;, &quot;World&quot;, c(1, 10), TRUE, 51.23, 119L)&gt; list_data[[1]][1] &quot;Hello&quot;[[2]][1] &quot;World&quot;[[3]][1] 1 10[[4]][1] TRUE[[5]][1] 51.23[[6]][1] 119 列表元素的重命名(names 函数) 12345678910111213141516171819202122232425262728293031323334&gt; # 列表的重命名&gt; list_data &lt;- list(c(&quot;Hello&quot;, &quot;World&quot;), matrix(c(1, 2, 3, 4, 5, 6), nrow=2), list(&quot;Happy&quot;, 1314L))&gt; list_data[[1]][1] &quot;Hello&quot; &quot;World&quot;[[2]] [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6[[3]][[3]][[1]][1] &quot;Happy&quot;[[3]][[2]][1] 1314&gt; names(list_data) &lt;- c(&quot;HelloString&quot;, &quot;MatrixElem&quot;, &quot;ListElem&quot;)&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314 列表元素的访问——增删改查 查 1234567891011121314151617181920212223242526272829303132333435363738394041&gt; # (查)&gt; # 根据索引访问&gt; list_data[0]named list()&gt; list_data[3]$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314&gt; # 根据元素名称访问&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;&gt; list_data$ListElem[[1]][1] &quot;Happy&quot;[[2]][1] 1314&gt; list_data[4]&lt;-&quot;Hello&quot;&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314[[4]][1] &quot;Hello&quot; 增 123456789101112131415161718192021&gt; # (增)&gt; list_data[4]&lt;-&quot;Hello&quot;&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314[[4]][1] &quot;Hello&quot; 改 1234567891011121314151617181920&gt; # (改)&gt; list_data[4] &lt;- 12&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314[[4]][1] 12 删 1234567891011121314151617&gt; # (删)&gt; list_data[4] &lt;- NULL&gt; list_data$HelloString[1] &quot;Hello&quot; &quot;World&quot;$MatrixElem [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6$ListElem$ListElem[[1]][1] &quot;Happy&quot;$ListElem[[2]][1] 1314 列表的合并：使用 c 可以将多个列表合并为一个列表 12345678910111213141516171819202122232425262728&gt; # 合并列表&gt; list1 &lt;- list(1, 2, 3)&gt; list2 &lt;- list(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)&gt; class(list1)[1] &quot;list&quot;&gt; class(list2)[1] &quot;list&quot;&gt; merged.list &lt;- c(list1, list2)&gt; merged.list[[1]][1] 1[[2]][1] 2[[3]][1] 3[[4]][1] &quot;A&quot;[[5]][1] &quot;B&quot;[[6]][1] &quot;C&quot;&gt; class(merged.list)[1] &quot;list&quot; 列表和向量之间的转换 1234567891011121314151617181920212223242526272829303132333435&gt; # 列表转换为向量&gt; list1 &lt;- list(1 : 5)&gt; list2 &lt;- list(10: 14)&gt; list3 &lt;- list(&quot;hello&quot;, matrix(c(1, 2, 3, 4), nrow=2), 3)&gt; list1[[1]][1] 1 2 3 4 5&gt; list2[[1]][1] 10 11 12 13 14&gt; list3[[1]][1] &quot;hello&quot;[[2]] [,1] [,2][1,] 1 3[2,] 2 4[[3]][1] 3&gt; v1 &lt;- unlist(list1)&gt; v2 &lt;- unlist(list2)&gt; v3 &lt;- unlist(list3)&gt; class(v1)[1] &quot;integer&quot;&gt; class(v2)[1] &quot;integer&quot;&gt; class(v3)[1] &quot;character&quot;&gt; v3[1] &quot;hello&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;3&quot; 矩阵 matrix 概念：R 语言为线性代数的研究提供了矩阵类型，这种数据结构很类似于其它语言中的二维数组。 矩阵的创建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&gt; # 矩阵的创建&gt; m1 = matrix(data=c(3: 14))&gt; m1 [,1] [1,] 3 [2,] 4 [3,] 5 [4,] 6 [5,] 7 [6,] 8 [7,] 9 [8,] 10 [9,] 11[10,] 12[11,] 13[12,] 14&gt; m2 = matrix(data=c(3: 14), nrow=4)&gt; m2 [,1] [,2] [,3][1,] 3 7 11[2,] 4 8 12[3,] 5 9 13[4,] 6 10 14&gt; m2 = matrix(data=c(3: 14), ncol=4)&gt; m2 [,1] [,2] [,3] [,4][1,] 3 6 9 12[2,] 4 7 10 13[3,] 5 8 11 14&gt; m3 &lt;- matrix(data=c(3, 14), nrow=4, byrow=TRUE) # 默认为 TRUE&gt; m3 [,1][1,] 3[2,] 14[3,] 3[4,] 14&gt; rownames &lt;- c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;, &quot;row4&quot;) &gt; colnames &lt;- c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;)&gt; m4 &lt;- matrix(data=c(3: 14), nrow=4, byrow=TRUE, dimnames=list(rownames, colnames))&gt; m4 col1 col2 col3row1 3 4 5row2 6 7 8row3 9 10 11row4 12 13 14 矩阵转置 1234567&gt; # 矩阵转置 t()&gt; # R 语言矩阵提供了 t() 函数, 可以实现矩阵的行列转换&gt; t(m4) row1 row2 row3 row4col1 3 6 9 12col2 4 7 10 13col3 5 8 11 14 矩阵元素访问 1234&gt; m4[c(&quot;row1&quot;, &quot;row4&quot;), c(&quot;col1&quot;, &quot;col3&quot;)] col1 col3row1 3 5row4 12 14 矩阵相关操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt; # 矩阵元素操作&gt; # 矩阵计算&gt; m1 &lt;- matrix(data=c(3: 14), nrow=4)&gt; m1 [,1] [,2] [,3][1,] 3 7 11[2,] 4 8 12[3,] 5 9 13[4,] 6 10 14&gt; m2 &lt;- matrix(data=c(1: 12), nrow=4)&gt; m2 [,1] [,2] [,3][1,] 1 5 9[2,] 2 6 10[3,] 3 7 11[4,] 4 8 12&gt; m1 + m2 [,1] [,2] [,3][1,] 4 12 20[2,] 6 14 22[3,] 8 16 24[4,] 10 18 26&gt; # 矩阵转换 &gt; # 矩阵 -&gt; 向量&gt; is.matrix(m1)[1] TRUE&gt; class(m1)[1] &quot;matrix&quot; &quot;array&quot; &gt; v1 &lt;- as.vector(m1)&gt; v1 [1] 3 4 5 6 7 8 9 10 11 12 13 14&gt; class(v1)[1] &quot;integer&quot;&gt; # 矩阵组合&gt; # cbind() 把其自变量横向拼成一个大矩阵, 横向组合, 行数一致&gt; # rbind() 把其自变量纵向拼成一个大矩阵, 纵向组合, 列数一致&gt; v1 = c(1, 2, 3, 4, 5)&gt; v2 = c(6, 7, 8, 9,10)&gt; y1 = cbind(v1, v2)&gt; y2 = rbind(v1, v2)&gt; y1 v1 v2[1,] 1 6[2,] 2 7[3,] 3 8[4,] 4 9[5,] 5 10&gt; y2 [,1] [,2] [,3] [,4] [,5]v1 1 2 3 4 5v2 6 7 8 9 10&gt; 数组 array 概念：数组也是 R 语言的对象，R 语言可以创建一维或多维数组。 数组的创建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt; # 创建数组&gt; # 一维数组&gt; v1 &lt;- c(5, 9, 1)&gt; a1 &lt;- array(v1)&gt; a1[1] 5 9 1&gt; is.array(a1)[1] TRUE&gt; a &lt;- array(10: 15, dim=9)&gt; a[1] 10 11 12 13 14 15 10 11 12&gt; a &lt;- array(10: 15, dim=c(2))&gt; a[1] 10 11&gt; &gt; # 二维数组&gt; a &lt;- array(10: 15, dim=c(2, 3)) # 10, 11, 12, 13, 14, 15&gt; a [,1] [,2] [,3][1,] 10 12 14[2,] 11 13 15&gt; is.array(a)[1] TRUE&gt; &gt; # 三维数组&gt; a &lt;- array(10: 15, dim=c(2, 3, 2))&gt; a, , 1 [,1] [,2] [,3][1,] 10 12 14[2,] 11 13 15, , 2 [,1] [,2] [,3][1,] 10 12 14[2,] 11 13 15&gt; column.names &lt;- c(&quot;COL1&quot;, &quot;COL2&quot;, &quot;COL3&quot;)&gt; row.names &lt;- c(&quot;ROW1&quot;, &quot;ROW2&quot;, &quot;ROW3&quot;)&gt; matrix.names &lt;- c(&quot;Matrix1&quot;, &quot;Matrix2&quot;)&gt; &gt; vector1 &lt;- c(5, 9, 3)&gt; vector2 &lt;- c(10, 11, 12, 13, 14, 15)&gt; a &lt;- array(c(vector1, vector2), dim=c(3, 3, 2), dimnames = list(row.names, column.names, matrix.names))&gt; a, , Matrix1 COL1 COL2 COL3ROW1 5 10 13ROW2 9 11 14ROW3 3 12 15, , Matrix2 COL1 COL2 COL3ROW1 5 10 13ROW2 9 11 14ROW3 3 12 15 数组元素访问 123456789101112131415161718192021&gt; # 数组访问&gt; a[1] # 一维数组的访问[1] 5&gt; a[1: 6] # 一维数组的访问——向量访问[1] 5 9 3 10 11 12&gt; &gt; dim2_a &lt;- array(c(vector1, vector2), dim=c(3, 3))&gt; dim2_a [,1] [,2] [,3][1,] 5 10 13[2,] 9 11 14[3,] 3 12 15&gt; dim2_a[1: 1] # 二维数组的访问, 访问的必须是二维数组[1] 5&gt; dim2_a[1: 2, 1: 2] # 二维数组的访问——矩阵访问 [,1] [,2][1,] 5 10[2,] 9 11&gt; &gt; a[3, 3, 1] # 三维数组[1] 15 数据框 data.frame 概念：数据框 DataFrame 可以理解成我们常说的表格，数据框是 R 语言的数据结构，是特殊的二维列表。数据框每一列都有一个唯一的列名，长度都是相等的，同一列的数据类型需要一致，不同列的数据类型可以不一样。数据框的特点： 列名称应为非空。 行名称应该是唯一的。 存储在数据框中的数据可以是数字，字符型等。 每个列应包含相同数量的数据项。 数据框的创建 12345678910111213&gt; # 数据框&gt; # 创建数据框对象&gt; df1 &lt;- data.frame(+ # tag = value+ 姓名 = c(&quot;张三&quot;, &quot;李四&quot;, &quot;王五&quot;),+ 工号 = c(&quot;001&quot;, &quot;002&quot;, &quot;003&quot;),+ 月薪 = c(1000, 2000, 3000)+ )&gt; df1 姓名 工号 月薪1 张三 001 10002 李四 002 20003 王五 003 3000 查看数据框结构 1234567891011121314&gt; # 查看数据框结构&gt; str(df1)'data.frame': 3 obs. of 3 variables: $ 姓名: chr &quot;张三&quot; &quot;李四&quot; &quot;王五&quot; $ 工号: chr &quot;001&quot; &quot;002&quot; &quot;003&quot; $ 月薪: num 1000 2000 3000&gt; summary(df1) 姓名 工号 月薪 Length:3 Length:3 Min. :1000 Class :character Class :character 1st Qu.:1500 Mode :character Mode :character Median :2000 Mean :2000 3rd Qu.:2500 Max. :3000 数据框访问、新增列 123456789101112131415161718192021222324252627282930313233343536373839&gt; # 提取数据框中的列&gt; df1$姓名[1] &quot;张三&quot; &quot;李四&quot; &quot;王五&quot;&gt; df2 &lt;- data.frame(df1$姓名, df1$月薪)&gt; df2 df1.姓名 df1.月薪1 张三 10002 李四 20003 王五 3000&gt; &gt; # 提取数据框中的行&gt; # 提取前两行&gt; result &lt;- df1[1:2, ]&gt; result 姓名 工号 月薪1 张三 001 10002 李四 002 2000&gt; &gt; # 提取前两行, 前两列&gt; result &lt;- df1[1:2, 1:3]&gt; result 姓名 工号 月薪1 张三 001 10002 李四 002 2000&gt; &gt; # 提取第1、3行, 第1、2列的数据&gt; result &lt;- df1[c(1, 3), c(1, 2)]&gt; result 姓名 工号1 张三 0013 王五 003&gt; &gt; # 新增列&gt; df1$部门 &lt;- c(&quot;运营&quot;, &quot;技术&quot;, &quot;运营&quot;)&gt; df1 姓名 工号 月薪 部门1 张三 001 1000 运营2 李四 002 2000 技术3 王五 003 3000 运营 数据框合并 12# 数据框合并# cbind()/rbind() 数据框筛选 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt; # 数据框筛选&gt; student &lt;- data.frame(+ sno = c(1, 2, 3),+ sname = c(&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;),+ sage = c(10, 12, 14)+ )&gt; student sno sname sage1 1 zhangsan 102 2 lisi 123 3 wangwu 14&gt; &gt; student[1] # 第一个字段 sno1 12 23 3&gt; student[1, 2] # 第一行, 第二列[1] &quot;zhangsan&quot;&gt; student$sage&gt;12 # FALSE FALSE TRUE[1] FALSE FALSE TRUE&gt; &gt; # [...] 里面一个参数取字段, 两个参数取行列&gt; student[student$sage&gt;11] # 执行判断, 得到结果 c(FALSE, TRUE, TRUE) sname sage1 zhangsan 102 lisi 123 wangwu 14&gt; student[c(FALSE, TRUE, TRUE)] # 使用 c(FALSE, TRUE, TRUE) 进行运算 sname sage1 zhangsan 102 lisi 123 wangwu 14&gt; student[student$sage&gt;11, ] # 选择年龄大于 11 的, 并取所有字段 sno sname sage2 2 lisi 123 3 wangwu 14&gt; student[c(FALSE, TRUE, TRUE), ] # 选择年龄大于 11 的行, 并取所有字段 sno sname sage2 2 lisi 123 3 wangwu 14&gt; student[student$sage&gt;11, 2] # 选择年龄大于 11 的, 并取第 2 个字段[1] &quot;lisi&quot; &quot;wangwu&quot;&gt; student[student$sage&gt;11, c(1, 2)] # 选择年龄大于 11 的, 并取第 1、2 个字段 sno sname2 2 lisi3 3 wangwu&gt; &gt; subset(student, select = c(&quot;sname&quot;, &quot;sage&quot;)) # 可以理解为: SELECT sname, sage FROM student; sname sage1 zhangsan 102 lisi 123 wangwu 14&gt; subset(student, select = c(&quot;sname&quot;, &quot;sage&quot;), sage&gt;11 &amp; sno == 2) # 可以理解问: SELECT sname, sage FROM student WHERE sage&gt;11 AND sno=2; sname sage2 lisi 12 因子 factor 概念：因子用于存储不同类别的数据类型，例如：人的性别有 男 和 女 两个类别，年龄来分可以有 未成年人 和 成年人 。 因子创建 123456789101112&gt; # 因子创建&gt; # 例 1:男 2:女&gt; x &lt;- c(1, 2, 1, 2, 2, 1, 3)&gt; factor(x)[1] 1 2 1 2 2 1 3Levels: 1 2 3&gt; factor(x, levels = c(1, 2))[1] 1 2 1 2 2 1 &lt;NA&gt;Levels: 1 2&gt; factor(x, levels = c(1, 2), labels = c(&quot;男&quot;, &quot;女&quot;))[1] 男 女 男 女 女 男 &lt;NA&gt;Levels: 男 女 🎫 3. ECharts - ECharts的简单使用3.1 ECharts 简介 ECharts 概念：一个基于 JavaScript 的开源可视化图表库（别人写好了 .js，我直接使用），可以流畅的运行在 PC 和移动设备上，兼容当前绝大部分浏览器（IE 9/10/11，Chrome，Firefox，Safari 等），底层依赖矢量图形库 *ZRender*，提供直观，交互丰富，可高度个性化定制的数据可视化图表。 官网：ECharts 特性：ECharts Feature 获取 Apache ECharts 通过 GitHub 获取 Apache/ECharts 项目的 release 页面可以找到各个版本的链接。点击下载页面下方 Assets 中的 Source code，解压后即为包含完整 ECharts 功能的文件。 echarts-5.4.0/echarts-5.4.0/dist/echarts.common.js 与 echarts-5.4.0/echarts-5.4.0/dist/echarts.common.min.js：体积适中，常用版，支持常见的图表和组件 echarts-5.4.0/echarts-5.4.0/dist/echarts.js 与 echarts-5.4.0/echarts-5.4.0/dist/echarts.min.js：体积最大，完整版，包含所有支持的图表和组件。 echarts-5.4.0/echarts-5.4.0/dist/echarts.simple.js 与 echarts-5.4.0/echarts-5.4.0/dist/echarts.simple.min.js：体积较小，精简版，包含最常用的图表和组件。 从 npm 获取 npm install echarts 从 CDN 获取 &lt;script src=&quot;https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; 在线定制 3.2 第一个 ECharts 程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;第一个 ECharts 程序&lt;/title&gt; &lt;!-- 1. 引入 ECharts.js 类库文件 --&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; window.onload=function() { // 3. 实例化 ECharts 对象 var dom = document.getElementById(&quot;main&quot;); var myChart = echarts.init(dom); // 4. 指定图表的配置项和数据 var option = { title: { text: &quot;ECharts 入门案例&quot; }, tooltip: { }, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '帽子', '高跟鞋', '袜子'] }, yAxis: { }, series: [{ name: '销量', type: 'bar', data: [5, 20, 26, 10, 20, 8] }] }; // 5. 使用刚指定的配置项和数据显示图表 myChart.setOption(option); } &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- 2. 准备一个放图表的容器 --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 900px; height: 400px&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 3.3 ECharts 绘图流程 新建一个 HTML 文件，并在网页头部 &lt;head&gt; ... &lt;/head&gt; 部分引入在线的 ECharts 类库或者本地已经下载好的类库。 123456&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;!-- 引入在线的 ECharts 类库或者本地已经下载好的类库 --&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;js/echarts.js&quot;&gt;&lt;/script&gt; --&gt; &lt;script src=&quot;http://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt; 在网页创建一个容器，为 ECharts 准备一个具备大小（宽高）的 DOM （Document Object Model, 文件对象模型 ) 。 1234&lt;body&gt; &lt;!-- 为 ECharts 准备一个具备大小(宽高)的 DOM --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px; height: 400px&quot;&gt;&lt;/div&gt;&lt;/body&gt; 获取 DOM，并初始化 ECharts 对象 1234&lt;script&gt; var dom = document.getElementById(&quot;main&quot;); var myChart = echarts.init(dom);&lt;/script&gt; 指定配置项信息 12345678910111213141516171819&lt;script&gt; // 省略上一步的操作... var options = { title: { text: &quot;ECharts 入门案例&quot; }, tooltip: { }, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: { }, series: [ {name:'销量', type: 'bar', data: [5, 20, 36, 10, 10, 20]} ] }&lt;/script&gt; 用刚指定的配置项和数据显示图表 1234&lt;script&gt; // 省略上一步的操作... myChart.setOption(option);&lt;/script&gt; 3.4 ECharts 常用配置项详解完成简单柱状图的绘制需要包含的组件包括： xAxis：X 轴组件 yAxis：Y 轴组件 series：系列组件（我更喜欢称其序列组件） 完善图形补充组件： title：标题组件 legend：图例组件 tooltip：提示框组件 toolbox：工具栏配置项 datazoom：数据区域缩放 3.5 ECharts 样式设置","link":"/2024/06/22/DataVisualization-USE-R-ECHARTS/"},{"title":"HBaseExamReview","text":"HBase 考察知识点复习","link":"/2024/06/22/HBaseExamReview/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"软件源","slug":"软件源","link":"/tags/%E8%BD%AF%E4%BB%B6%E6%BA%90/"},{"name":"开发工具","slug":"开发工具","link":"/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"Axure RP 9","slug":"Axure-RP-9","link":"/tags/Axure-RP-9/"},{"name":"闲聊","slug":"闲聊","link":"/tags/%E9%97%B2%E8%81%8A/"},{"name":"计划","slug":"计划","link":"/tags/%E8%AE%A1%E5%88%92/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"VSCode","slug":"VSCode","link":"/tags/VSCode/"},{"name":"大数据技术","slug":"大数据技术","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Kaggle","slug":"Kaggle","link":"/tags/Kaggle/"},{"name":"ECharts","slug":"ECharts","link":"/tags/ECharts/"},{"name":"Tomcat","slug":"Tomcat","link":"/tags/Tomcat/"},{"name":"CppDev","slug":"CppDev","link":"/tags/CppDev/"},{"name":"QT6","slug":"QT6","link":"/tags/QT6/"},{"name":"项目","slug":"项目","link":"/tags/%E9%A1%B9%E7%9B%AE/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"EasyX","slug":"EasyX","link":"/tags/EasyX/"},{"name":"GameDev","slug":"GameDev","link":"/tags/GameDev/"},{"name":"R","slug":"R","link":"/tags/R/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"闲聊","slug":"闲聊","link":"/categories/%E9%97%B2%E8%81%8A/"},{"name":"软件源","slug":"Linux/软件源","link":"/categories/Linux/%E8%BD%AF%E4%BB%B6%E6%BA%90/"},{"name":"Axure RP 9","slug":"开发工具/Axure-RP-9","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Axure-RP-9/"},{"name":"计划","slug":"闲聊/计划","link":"/categories/%E9%97%B2%E8%81%8A/%E8%AE%A1%E5%88%92/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Git","slug":"开发工具/Git","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/Git/"},{"name":"VSCode","slug":"开发工具/VSCode","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/VSCode/"},{"name":"PyTorch","slug":"深度学习/PyTorch","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/"},{"name":"大数据技术","slug":"大数据技术","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"},{"name":"HBase","slug":"大数据技术/HBase","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/HBase/"},{"name":"数据结构&#x2F;算法","slug":"数据结构-算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/"},{"name":"哈希(Hash)","slug":"数据结构-算法/哈希-Hash","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E5%93%88%E5%B8%8C-Hash/"},{"name":"栈(Stack)","slug":"数据结构-算法/栈-Stack","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%88-Stack/"},{"name":"树(Tree)","slug":"数据结构-算法/树-Tree","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95/%E6%A0%91-Tree/"},{"name":"Kaggle","slug":"深度学习/Kaggle","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Kaggle/"},{"name":"ECharts","slug":"大数据技术/ECharts","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/ECharts/"},{"name":"CppDev","slug":"CppDev","link":"/categories/CppDev/"},{"name":"QT6","slug":"CppDev/QT6","link":"/categories/CppDev/QT6/"},{"name":"项目","slug":"大数据技术/项目","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/"},{"name":"Scala","slug":"大数据技术/Scala","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Scala/"},{"name":"Spark","slug":"大数据技术/Spark","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/Spark/"},{"name":"Gaming","slug":"CppDev/Gaming","link":"/categories/CppDev/Gaming/"},{"name":"数据可视化","slug":"大数据技术/数据可视化","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"pages":[]}